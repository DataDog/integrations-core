init_config:

instances:
  - host: localhost
    port: 5432
#    username: my_username
#    password: my_password
#    dbname: db_name
#    ssl: False
#    use_psycopg2: False # Force using psycogp2 instead of pg8000 to connect. WARNING: psycopg2 doesn't support ssl mode.
#    tags:
#      - optional_tag1
#      - optional_tag2

# Connect using a UNIX socket (host must begin with a /)
# If `use_psycopg2` is enabled, use the directory containing the UNIX socket (ex: `/run/postgresql/`)
# otherwise, use the full path to the socket file (ex: `/run/postgresql/.s.PGSQL.5433`).
#  - host: /run/postgresql/.s.PGSQL.5433
#    dbname: db_name

# Track per-relation (table) metrics
# The list of relations/tables must be specified here.
# Each relation generates many metrics (10 + 10 per index)
#
#    relations:
#      - my_table
#      - my_other_table
#
# By default all schemas are included. To track relations from specific schemas only,
# use the following syntax:
#
#    relations:
#      - relation_name: another_table
#        schemas:
#          - public
#          - prod
#

# Custom metrics
# Providing custom queries is also supported. Each query must have 3 fields:
#
# 1. metric_prefix - This is what each metric will start with.
# 2. query - This is the SQL to execute. It can be a simple statement or a
#            multi-line script. Only the first row of the result is read.
# 3. columns - This is a list representing each column, ordered sequentially
#              from left to right. The number of columns must equal the number
#              of columns returned in the query.
#              There are 2 required pieces of data:
#                a. name - This is the suffix to append to the metric_prefix
#                          in order to form the full metric name. If `type` is
#                          `tag`, this column will instead be considered a tag
#                          and will be applied to every metric collected by
#                          this particular query.
#                b. type - This is the submission method (gauge, count, rate, etc.).
#                          This can also be set to 'tag' to tag each metric in the row
#                          with the name and value of the item in this column.
# 4. tags (optional) - A list of tags to apply to each metric.
#
#    custom_queries:
#      - metric_prefix: postgresql
#        query: |  # Use the pipe if you require a multi-line script.
#          SELECT consumer_name,
#                 GREATEST(0, EXTRACT(EPOCH FROM lag)) as lag,
#                 GREATEST(0, EXTRACT(EPOCH FROM lag)) as last_seen,
#                 pending_events
#          FROM pgq.get_consumer_info()
#          WHERE consumer_name !~ 'watermark$';
#        columns:
#          # Columns without a name are ignored, put this for any column you wish to skip:
#          # - {}
#          - name: consumer_name
#            type: tag
#          - name: londiste_lag
#            type: gauge
#          - name: londiste_last_seen
#            type: gauge
#          - name: londiste_pending_events
#            type: gauge
#        tags:
#          - tester:postgresql

#    /!\ DEPRECATION: Please use custom_queries instead of the now deprecated custom_metrics.
#    Please visit our help article for a guide on collecting custom metrics from PostgreSQL:
#    https://help.datadoghq.com/hc/en-us/articles/208385813-Postgres-custom-metric-collection-explained
#
#    custom_metrics:
#    - # Londiste 3 replication lag
#      descriptors:
#        - [consumer_name, consumer_name]
#      metrics:
#         GREATEST(0, EXTRACT(EPOCH FROM lag)) as lag: [postgresql.londiste_lag, GAUGE]
#         GREATEST(0, EXTRACT(EPOCH FROM lag)) as last_seen: [postgresql.londiste_last_seen, GAUGE]
#         pending_events: [postgresql.londiste_pending_events, GAUGE]
#      query: SELECT consumer_name, %s from pgq.get_consumer_info() where consumer_name !~ 'watermark$';
#      relation: false
#

#    Collect metrics regarding PL/pgSQL functions from pg_stat_user_functions
#    collect_function_metrics: False
#

#    Collect count metrics, default value is True for backward compatibility but they might be slow,
#    suggested value is False.
#    collect_count_metrics: False
#

#    Collect database size metrics. Default value is True but they might be slow with large databases
#    collect_database_size_metrics: False
#

#    Include statistics from the default database 'postgres' in the check metrics, default to false
#    collect_default_database: False
#

## log Section (Available for Agent >=6.0)
#logs:

    # - type : (mandatory) type of log input source (tcp / udp / file)
    #   port / path : (mandatory) Set port if type is tcp or udp. Set path if type is file
    #   service : (mandatory) name of the service owning the log
    #   source : (mandatory) attribute that defines which integration is sending the logs
    #   sourcecategory : (optional) Multiple value attribute. Can be used to refine the source attribtue
    #   tags: (optional) add tags to each logs collected

#  - type: file
#    path: /path/to/postgres.log
#    source: postgresql
#    sourcecategory: database
#    service: myapp
#    #To handle multi line that starts with yyyy-mm-dd or yyyy-dd-mm use the following pattern
#    log_processing_rules:
#      - type: multi_line
#        pattern: \d{4}\-(0?[1-9]|1[012])\-(0?[1-9]|[12][0-9]|3[01])
#        name: new_log_start_with_date
