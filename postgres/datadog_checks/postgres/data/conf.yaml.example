## All options defined here are available to all instances.
#
init_config:

    ## @param service - string - optional
    ## Attach the tag `service:<SERVICE>` to every metric, event, and service check emitted by this integration.
    ##
    ## Additionally, this sets the default `service` for every log source.
    #
    # service: <SERVICE>

## Every instance is scheduled independently of the others.
#
instances:

    ## @param host - string - required
    ## The hostname to connect to.
    ## NOTE: Even if the server name is `localhost`, the Agent connects to PostgreSQL using TCP/IP unless you also
    ## provide a value for the sock key.
    #
  - host: localhost

    ## @param port - integer - optional - default: 5432
    ## The port to use when connecting to PostgreSQL.
    #
    # port: 5432

    ## @param username - string - required
    ## The Datadog username created to connect to PostgreSQL.
    #
    username: datadog

    ## @param password - string - optional
    ## The password associated with the Datadog user.
    #
    # password: <PASSWORD>

    ## @param dbname - string - optional - default: postgres
    ## The name of the PostgresSQL database to monitor.
    ## Note: If omitted, the default system Postgres database is queried.
    #
    # dbname: <DBNAME>

    ## @param reported_hostname - string - optional
    ## Set the reported hostname for this instance. This value overrides the hostname detected by the Agent
    ## and can be useful to set a custom hostname when connecting to a remote database through a proxy.
    #
    # reported_hostname: <REPORTED_HOSTNAME>

    ## @param dbstrict - boolean - optional - default: false
    ## Whether to restrict the scope of the check to just the database in question.
    ## Set to `true` if you only want to gather metrics from the database provided in the dbname parameter.
    #
    # dbstrict: false

    ## @param ignore_databases - list of strings - optional
    ## A list of database to ignore. No metrics or statement samples will be collected for these databases.
    ## Each value can be a plain string or a Postgres pattern.
    ## For more information on how patterns work, see https://www.postgresql.org/docs/12/functions-matching.html
    #
    # ignore_databases:
    #   - template%
    #   - rdsadmin
    #   - azure_maintenance
    #   - cloudsqladmin

    ## @param ssl - string - optional - default: allow
    ## This option determines whether or not and with what priority a secure SSL TCP/IP connection
    ## is negotiated with the server. There are six modes:
    ## - `disable`: Only tries a non-SSL connection.
    ## - `allow`: First tries a non-SSL connection; if if fails, tries an SSL connection.
    ## - `prefer`: First tries an SSL connection; if it fails, tries a non-SSL connection.
    ## - `require`: Only tries an SSL connection. If a root CA file is present, verifies the certificate in
    ##              the same way as if verify-ca was specified.
    ## - `verify-ca`: Only tries an SSL connection, and verifies that the server certificate is issued by a
    ##                trusted certificate authority (CA).
    ## - `verify-full`: Only tries an SSL connection and verifies that the server certificate is issued by a
    ##                  trusted CA and that the requested server host name matches the one in the certificate.
    ##
    ## For a detailed description of how these options work see https://www.postgresql.org/docs/current/libpq-ssl.html
    #
    # ssl: allow

    ## @param ssl_root_cert - string - optional
    ## The path to the ssl root certificate.
    ##
    ## For a detailed description of how this option works see https://www.postgresql.org/docs/current/libpq-ssl.html
    #
    # ssl_root_cert: <SSL_ROOT_CERT>

    ## @param ssl_cert - string - optional
    ## The path to the ssl certificate.
    ##
    ## For a detailed description of how this option works see https://www.postgresql.org/docs/current/libpq-ssl.html
    #
    # ssl_cert: <SSL_CERT>

    ## @param ssl_key - string - optional
    ## The path to the ssl client key.
    ##
    ## For a detailed description of how this option works see https://www.postgresql.org/docs/current/libpq-ssl.html
    #
    # ssl_key: <SSL_KEY>

    ## @param ssl_password - string - optional
    ## The password for the secret key specified in ssl_key, allowing client certificate private keys to be stored
    ## in encrypted form on disk.
    ##
    ## For a detailed description of how this option works see https://www.postgresql.org/docs/current/libpq-ssl.html
    #
    # ssl_password: <SSL_PASSWORD>

    ## @param query_timeout - integer - optional - default: 5000
    ## Adds a statement_timeout https://www.postgresql.org/docs/current/runtime-config-client.html#GUC-STATEMENT-TIMEOUT
    ## to all metric collection queries. Aborts any statement that takes more than the specified number of milliseconds,
    ## starting from the time the command arrives at the server from the client. A value of zero turns this off.
    ## Cancelled queries won't log any metrics.
    #
    # query_timeout: 5000

    ## @param relations - (list of string or mapping) - optional
    ## The list of relations/tables must be specified here to track per-relation (table, index , view, etc.) metrics.
    ## If enabled, `dbname` should be specified to collect database-specific relations metrics.
    ## You can either specify a single relation by its exact name in 'relation_name' or use a regex to track metrics
    ## from all matching relations (useful in cases where relation names are dynamically generated, e.g. TimescaleDB).
    ## Each relation generates many metrics (10 + 10 per index).
    ##
    ## By default all schemas are included. To track relations from specific schemas only,
    ## you can specify the `schemas` attribute and provide a list of schemas to use for filtering.
    ##
    ## Size metrics are collected only for ordinary tables. Index metrics are collected only for user indexes. Lock
    ## metrics are collected for all relation types (table, index , view, etc.). The rest of the metrics are
    ## collected only for user tables.
    ## To track lock metrics for relations of a specific kind only, specify the `relkind` attribute
    ## as a list of the options:
    ## * r = ordinary table
    ## * i = index
    ## * S = sequence
    ## * t = TOAST table
    ## * m = materialized view
    ## * c = composite type
    ## * f = foreign table
    ## * p = partitioned table
    ##
    ## Note: For compatibility reasons you can also use the following syntax to track relations metrics by specifying
    ## the list of table names. All schemas are included and regex are not supported.
    ## relations:
    #
    # relations:
    #   - relation_name: <TABLE_NAME>
    #     schemas:
    #     - <SCHEMA_NAME1>
    #   - relation_regex: <TABLE_PATTERN>
    #     relkind:
    #     - r
    #     - p

    ## @param max_relations - integer - optional - default: 300
    ## Determines the maximum number of relations to fetch.
    #
    # max_relations: 300

    ## @param collect_function_metrics - boolean - optional - default: false
    ## If set to true, collects metrics regarding PL/pgSQL functions from pg_stat_user_functions.
    #
    # collect_function_metrics: false

    ## @param collect_count_metrics - boolean - optional - default: true
    ## Collect count of user tables up to max_relations size from pg_stat_user_tables.
    #
    # collect_count_metrics: true

    ## @param collect_activity_metrics - boolean - optional - default: false
    ## Collect metrics regarding transactions from pg_stat_activity. Please make sure the user
    ## has sufficient privileges to read from pg_stat_activity before enabling this option.
    #
    # collect_activity_metrics: false

    ## @param activity_metrics_excluded_aggregations - list of strings - optional
    ## A list of columns to remove from the pg_stat_activity aggregation.
    ## By default, datname, usename and application_name will be used.
    ## If applications with different application_name are creating a lot of short-lived queries,
    ## removing application_name from the aggregation can help generate more stable metrics.
    ## Note that datname is a required aggregation on activity metrics and can't be excluded.
    #
    # activity_metrics_excluded_aggregations:
    #   - application_name

    ## @param collect_database_size_metrics - boolean - optional - default: true
    ## Collect database size metrics.
    #
    # collect_database_size_metrics: true

    ## @param collect_default_database - boolean - optional - default: true
    ## Include statistics from the default database 'postgres' in the check metrics.
    #
    # collect_default_database: true

    ## @param collect_bloat_metrics - boolean - optional - default: false
    ## Collect metrics about table bloat. Only available when `relation` metrics are enabled.
    #
    collect_bloat_metrics: false

    ## @param collect_wal_metrics - boolean - optional - default: false
    ## Collect metrics about WAL file age.
    ## NOTE: You must be running the check local to your database if you want to enable this option.
    ## Starting PG10, wal metrics are collected by default using `pg_ls_waldir` and don't need local access anymore.
    #
    # collect_wal_metrics: false

    ## @param data_directory - string - optional - default: /usr/local/pgsql/data
    ## The data directory of your postgres installation
    ## Required when collecting WAL metrics.
    #
    # data_directory: /usr/local/pgsql/data

    ## @param tag_replication_role - boolean - optional - default: false
    ## Tag metrics and checks with `replication_role:<master|standby>`.
    #
    # tag_replication_role: false

    ## @param table_count_limit - integer - optional - default: 200
    ## The maximum number of tables to collect metrics from.
    #
    # table_count_limit: 200

    ## @param custom_queries - list of mappings - optional
    ## Define custom queries to collect custom metrics from your PostgreSQL
    ## See Datadog FAQ article for a guide on collecting custom metrics from PostgreSQL:
    ## https://docs.datadoghq.com/integrations/faq/postgres-custom-metric-collection-explained/
    #
    # custom_queries:
    #   - metric_prefix: postgresql
    #     query: <QUERY>
    #     columns:
    #     - name: <COLUNMS_1_NAME>
    #       type: <COLUMNS_1_TYPE>
    #     - name: <COLUNMS_2_NAME>
    #       type: <COLUMNS_2_TYPE>
    #     tags:
    #     - <TAG_KEY>:<TAG_VALUE>

    ## Define the configuration for database autodiscovery.
    ## Complete this section if you want to auto-discover databases on this host
    ## instead of specifying each using dbname. 
    #
    # database_autodiscovery:

        ## @param enabled - boolean - optional - default: false
        ## Enable database autodiscovery.
        #
        # enabled: false

        ## @param max_databases - integer - optional - default: 100
        ## The maximum number of databases this host should monitor.
        #
        # max_databases: 100

        ## @param include - list of strings - optional - default: ['.*']
        ## Regular expression for database names to include as part of 
        ## database autodiscovery.
        ## Will report metrics for databases that are found in this instance, 
        ## ignores databases listed but not found.
        ## Character casing is ignored. The regular expressions start matching from 
        ## the beginning, so to match anywhere, prepend `.*`. For exact matches append `$`.
        ## Defaults to `.*` to include everything.
        #
        # include:
        #   - master$
        #   - AdventureWorks.*

        ## @param exclude - list of strings - optional - default: ['cloudsqladmin']
        ## Regular expression for database names to exclude as part of `database_autodiscovery`.
        ## Character casing is ignored. The regular expressions start matching from the beginning,
        ## so to match anywhere, prepend `.*`. For exact matches append `$`.
        ## In case of conflicts, database exclusion via `exclude` takes precedence over
        ## those found via `include`
        #
        # exclude:
        #   - model
        #   - msdb
        #   - cloudsqladmin

        ## @param refresh - integer - optional - default: 600
        ## Frequency in seconds of scans for new databases. Defaults to 10 minutes.
        #
        # refresh: 600

    ## @param application_name - string - optional - default: datadog-agent
    ## The application_name can be any string of less than NAMEDATALEN characters (64 characters in a standard build).
    ## It is typically set by an application upon connection to the server.
    ## The name is displayed in the pg_stat_activity view and included in CSV log entries.
    #
    # application_name: datadog-agent

    ## @param dbm - boolean - optional - default: false
    ## Set to `true` to enable Database Monitoring.
    #
    # dbm: false

    ## @param pg_stat_statements_view - string - optional - default: show_pg_stat_statements()
    ## Set this value if you want to define a custom view or function to allow the datadog user to query the
    ## `pg_stat_statements` table, which is useful for restricting the permissions given to the datadog agent.
    ## Please note this is an ALPHA feature and is subject to change or deprecation without notice.
    #
    # pg_stat_statements_view: show_pg_stat_statements()

    ## Configure collection of query metrics
    #
    # query_metrics:

        ## @param enabled - boolean - optional - default: true
        ## Enable collection of query metrics. Requires `dbm: true`.
        #
        # enabled: true

        ## @param collection_interval - number - optional - default: 10
        ## Set the query metric collection interval (in seconds). Each collection involves a single query to
        ## `pg_stat_statements`. If a non-default value is chosen then that exact same value must be used for *every*
        ## check instance. Running different instances with different collection intervals is not supported.
        #
        # collection_interval: 10

    ## Configure collection of query samples
    #
    # query_samples:

        ## @param enabled - boolean - optional - default: true
        ## Enable collection of query samples. Requires `dbm: true`.
        #
        # enabled: true

        ## @param collection_interval - number - optional - default: 1
        ## Set the query sample collection interval (in seconds). Each collection involves a single query to
        ## `pg_stat_activity` followed by at most one `EXPLAIN` query per unique normalized query seen.
        #
        # collection_interval: 1

        ## @param explain_function - string - optional - default: datadog.explain_statement
        ## Override the default function used to collect execution plans for queries.
        #
        # explain_function: datadog.explain_statement

        ## @param explained_queries_per_hour_per_query - integer - optional - default: 60
        ## Set the rate limit for how many execution plans will be collected per hour per normalized query.
        #
        # explained_queries_per_hour_per_query: 60

        ## @param samples_per_hour_per_query - integer - optional - default: 15
        ## Set the rate limit for how many query sample events will be ingested per hour per normalized execution
        ## plan.
        #
        # samples_per_hour_per_query: 15

        ## @param explained_queries_cache_maxsize - integer - optional - default: 5000
        ## Set the max size of the cache used for the explained_queries_per_hour_per_query rate limit. This should
        ## be increased for databases with a very large number unique normalized queries which exceed the cache's
        ## limit.
        #
        # explained_queries_cache_maxsize: 5000

        ## @param seen_samples_cache_maxsize - integer - optional - default: 10000
        ## Set the max size of the cache used for the samples_per_hour_per_query rate limit. This should be increased
        ## for databases with a very large number of unique normalized execution plans which exceed the cache's limit.
        #
        # seen_samples_cache_maxsize: 10000

        ## @param explain_parameterized_queries - boolean - optional - default: true
        ## This option will enable the ability to explain parameterized queries.
        ## This is useful if your SQL clients are using the extended query protocol or prepared statements.
        #
        # explain_parameterized_queries: true

    ## Configure collection of query activity
    #
    # query_activity:

        ## @param enabled - boolean - optional - default: true
        ## Enable collection of query activity. Requires `dbm: true`.
        #
        # enabled: true

        ## @param collection_interval - number - optional - default: 10
        ## Set the query activity collection interval (in seconds). This number cannot be smaller than
        ## query_samples configured collection_interval.
        #
        # collection_interval: 10

        ## @param payload_row_limit - number - optional - default: 3500
        ## Set the query activity maximum number of pg_stat_activity rows you want to report. If the table is larger
        ## than the maximum rows set, then the top N longest running transactions will be reported.
        #
        # payload_row_limit: 3500

    ## Configure collection of pg_settings. This is an alpha feature.
    #
    # collect_settings:

        ## @param enabled - boolean - optional - default: false
        ## Enable collection of pg_settings. Requires `dbm: true`.
        #
        # enabled: false

        ## @param collection_interval - number - optional - default: 600
        ## Set the database settings collection interval (in seconds). Each collection involves a single query to
        ## `pg_settings`.
        #
        # collection_interval: 600

    ## Enable collection of database schemas. In order to collect schemas from all user databases, 
    ## enable `database_autodiscovery`. To collect from a single database, set `dbname` to collect 
    ## the schema for that database.
    ## Relation metrics must be enabled for schema collection.
    #
    # collect_schemas:

        ## @param enabled - boolean - optional - default: false
        ## Enable collection of database schemas. Requires `dbm: true` and relation metrics must be enabled.
        #
        # enabled: false

        ## @param max_tables - number - optional - default: 1000
        ## Maximum amount of tables the Agent collects from the instance.
        #
        # max_tables: 1000

        ## @param max_columns - number - optional - default: 50
        ## Maximum amount of columns the Agent collects per table.
        #
        # max_columns: 50

        ## @param collection_interval - number - optional - default: 600
        ## The database schema collection interval (in seconds).
        #
        # collection_interval: 600

    ## This block defines the configuration for AWS RDS and Aurora instances. 
    ##
    ## Complete this section if you have installed the Datadog AWS Integration 
    ## (https://docs.datadoghq.com/integrations/amazon_web_services) to enrich instances 
    ## with Postgres integration telemetry.
    ##
    ## These values are only applied when `dbm: true` option is set.
    #
    # aws:

        ## @param instance_endpoint - string - optional - default: mydb.cfxgae8cilcf.us-east-1.rds.amazonaws.com
        ## Equal to the Endpoint.Address of the instance the agent is connecting to. 
        ## This value is optional if the value of `host` is already configured to the instance endpoint.
        ##
        ## For more information on instance endpoints, 
        ## see the AWS docs https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_Endpoint.html
        #
        # instance_endpoint: mydb.cfxgae8cilcf.us-east-1.rds.amazonaws.com

        ## @param region - string - optional - default: us-east-1
        ## Equal to the region of the instance the agent is connecting to. 
        ## This value is used to configure IAM authentication.
        #
        # region: us-east-1

    ## This block defines the configuration for Google Cloud SQL instances.
    ##
    ## Complete this section if you have installed the Datadog GCP Integration 
    ## (https://docs.datadoghq.com/integrations/google_cloud_platform) to enrich instances 
    ## with Postgres integration telemetry.
    ##
    ## These values are only applied when `dbm: true` option is set.
    #
    # gcp:

        ## @param project_id - string - optional - default: foo-project
        ## Equal to the GCP resource's project ID.
        ##
        ## For more information on project IDs,
        ## See the GCP docs https://cloud.google.com/resource-manager/docs/creating-managing-projects
        #
        # project_id: foo-project

        ## @param instance_id - string - optional - default: foo-database
        ## Equal to the GCP resource's instance ID.
        ##
        ## For more information on instance IDs,
        ## See the GCP docs https://cloud.google.com/sql/docs/postgres/instance-settings#instance-id-2ndgen
        #
        # instance_id: foo-database

    ## This block defines the configuration for Azure Database for PostgreSQL.
    ##
    ## Complete this section if you have installed the Datadog Azure Integration 
    ## (https://docs.datadoghq.com/integrations/azure) to enrich instances 
    ## with Postgres integration telemetry.
    ## These values are only applied when `dbm: true` option is set.
    #
    # azure:

        ## @param deployment_type - string - optional - default: flexible_server
        ## Equal to the deployment type for the managed database. 
        ##
        ## Acceptable values are:
        ##   - `flexible_server` 
        ##   - `single_server`
        ##   - `virtual_machine`
        ##
        ## For more information on deployment types, 
        ## see the Azure docs https://docs.microsoft.com/en-us/azure/postgresql/overview-postgres-choose-server-options
        #
        # deployment_type: flexible_server

        ## @param fully_qualified_domain_name - string - optional - default: my-postgres-database.database.windows.net
        ## Equal to the full qualified domain name of the Azure PostgreSQL database.
        ##
        ## This value is optional if the value of `host` is already configured to the fully qualified domain name.
        #
        # fully_qualified_domain_name: my-postgres-database.database.windows.net

    ## Configuration section used for Azure AD Authentication.
    ##
    ## This supports using System or User assigned managed identities.
    ## If this section is set, then the `username` and `password` fields will be ignored.
    ##
    ## For more information on Managed Identities, see the Azure docs
    ## https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview
    #
    # managed_identity:

        ## @param client_id - string - optional
        ## Client ID of the Managed Identity.
        #
        # client_id: <CLIENT_ID>

        ## @param identity_scope - string - optional - default: https://ossrdbms-aad.database.windows.net/.default
        ## The permission scope from where to access the identity token. This value is optional if using the default
        ## identity scope for Azure managed databases.
        ##
        ## For more information on scopes, see the Azure docs
        ## https://learn.microsoft.com/en-us/azure/active-directory/develop/scopes-oidc
        #
        # identity_scope: https://ossrdbms-aad.database.windows.net/.default

    ## Configure how the SQL obfuscator behaves.
    ## Note: This option only applies when `dbm` is enabled.
    #
    # obfuscator_options:

        ## @param replace_digits - boolean - optional - default: false
        ## Set to `true` to replace digits in identifiers and table names with question marks in your SQL statements.
        ## Note: This option also applies to extracted tables using `collect_tables`.
        #
        # replace_digits: false

        ## @param collect_metadata - boolean - optional - default: true
        ## Set to `false` to disable the collection of metadata in your SQL statements.
        ## Metadata includes things such as tables, commands, and comments.
        #
        # collect_metadata: true

        ## @param collect_tables - boolean - optional - default: true
        ## Set to `false` to disable the collection of tables in your SQL statements.
        ## Requires `collect_metadata: true`.
        #
        # collect_tables: true

        ## @param collect_commands - boolean - optional - default: true
        ## Set to `false` to disable the collection of commands in your SQL statements.
        ## Requires `collect_metadata: true`.
        ##
        ## Examples: SELECT, UPDATE, DELETE, etc.
        #
        # collect_commands: true

        ## @param collect_comments - boolean - optional - default: true
        ## Set to `false` to disable the collection of comments in your SQL statements.
        ## Requires `collect_metadata: true`.
        #
        # collect_comments: true

        ## @param keep_sql_alias - boolean - optional - default: true
        ## Set to `true` to keep sql aliases in obfuscated SQL statements. Examples of aliases are
        ## `with select 1 as alias`, `select column as other_name`, or `select * from table t`.
        ## When `true` these aliases will not be removed.
        #
        # keep_sql_alias: true

        ## @param keep_dollar_quoted_func - boolean - optional - default: true
        ## Set to `true` to prevent dollar quoted function strings (e.g. `$func$`) from being removed.
        ## When not removed, the sql content of dollar quoted func strings will be obfuscated.
        ## Only strings with the tag `$func$` are supported.
        #
        # keep_dollar_quoted_func: true

    ## @param tags - list of strings - optional
    ## A list of tags to attach to every metric and service check emitted by this instance.
    ##
    ## Learn more about tagging at https://docs.datadoghq.com/tagging
    #
    # tags:
    #   - <KEY_1>:<VALUE_1>
    #   - <KEY_2>:<VALUE_2>

    ## @param service - string - optional
    ## Attach the tag `service:<SERVICE>` to every metric, event, and service check emitted by this integration.
    ##
    ## Overrides any `service` defined in the `init_config` section.
    #
    # service: <SERVICE>

    ## @param min_collection_interval - number - optional - default: 15
    ## This changes the collection interval of the check. For more information, see:
    ## https://docs.datadoghq.com/developers/write_agent_check/#collection-interval
    #
    # min_collection_interval: 15

    ## @param empty_default_hostname - boolean - optional - default: false
    ## This forces the check to send metrics with no hostname.
    ##
    ## This is useful for cluster-level checks.
    #
    # empty_default_hostname: false

    ## @param disable_generic_tags - boolean - optional - default: false
    ## The integration will stop sending server tag as is reduntant with host tag
    #
    disable_generic_tags: true

    ## @param metric_patterns - mapping - optional
    ## A mapping of metrics to include or exclude, with each entry being a regular expression.
    ##
    ## Metrics defined in `exclude` will take precedence in case of overlap.
    #
    # metric_patterns:
    #   include:
    #   - <INCLUDE_REGEX>
    #   exclude:
    #   - <EXCLUDE_REGEX>

## Log Section
##
## type - required - Type of log input source (tcp / udp / file / windows_event).
## port / path / channel_path - required - Set port if type is tcp or udp.
##                                         Set path if type is file.
##                                         Set channel_path if type is windows_event.
## source  - required - Attribute that defines which integration sent the logs.
## encoding - optional - For file specifies the file encoding. Default is utf-8. Other
##                       possible values are utf-16-le and utf-16-be.
## service - optional - The name of the service that generates the log.
##                      Overrides any `service` defined in the `init_config` section.
## tags - optional - Add tags to the collected logs.
##
## Discover Datadog log collection: https://docs.datadoghq.com/logs/log_collection/
#
# logs:
#   - type: file
#     path: /path/to/postgres.log
#     source: postgresql
#     service: <SERVICE>
#     log_processing_rules:
#     - type: multi_line
#       pattern: \d{4}\-(0?[1-9]|1[012])\-(0?[1-9]|[12][0-9]|3[01])
#       name: new_log_start_with_date
