## All options defined here are available to all instances.
#
init_config:

    ## @param global_custom_queries - list of mappings - optional
    ## See `custom_queries` defined below.
    ## Global custom queries can be applied to all instances using the
    ## `use_global_custom_queries` setting at the instance level.
    #
    # global_custom_queries:
    #   - metric_prefix: snowflake
    #     query: <QUERY>
    #     columns: <COLUMNS>
    #     tags: <TAGS>

    ## @param service - string - optional
    ## Attach the tag `service:<SERVICE>` to every metric, event, and service check emitted by this integration.
    ##
    ## Additionally, this sets the default `service` for every log source.
    #
    # service: <SERVICE>

## Every instance is scheduled independent of the others.
#
instances:

    ## @param account - string - required
    ## Name of your account (provided by Snowflake), including the platform and region if applicable.
    #
  - account: <ACCOUNT>

    ## @param user - string - required
    ## Login name for the user.
    #
    user: <USER>

    ## @param password - string - required
    ## Password for the user
    #
    password: <PASSWORD>

    ## @param database - string - optional - default: SNOWFLAKE
    ## Name of the default database to use.
    #
    # database: SNOWFLAKE

    ## @param schema - string - optional - default: ACCOUNT_USAGE
    ## Name of the default schema to use for the database.
    #
    # schema: ACCOUNT_USAGE

    ## @param role - string - optional - default: ACCOUNTADMIN
    ## Name of the default role to use.
    #
    # role: ACCOUNTADMIN

    ## @param warehouse - string - optional
    ## Name of the default warehouse to use.
    #
    # warehouse: <WAREHOUSE>

    ## @param passcode_in_password - boolean - optional - default: false
    ## Enable if the Multi-Factor Authentication passcode is embedded in the login password.
    #
    # passcode_in_password: false

    ## @param passcode - string - optional
    ## The passcode provided by Duo when using MFA for login.
    #
    # passcode: <PASSCODE>

    ## @param client_prefetch_threads - integer - optional - default: 4
    ## The number of threads used to download the results sets. Increasing the value improves fetch
    ## performance but requires more memory.
    #
    # client_prefetch_threads: 4

    ## @param login_timeout - integer - optional - default: 60
    ## Timeout in seconds for login. The login request gives up after the timeout length if the HTTP response succeeds.
    #
    # login_timeout: 60

    ## @param ocsp_response_cache_filename - string - optional
    ## URI for the OCSP response cache file. By default, the OCSP response cache file is created in the cache directory:
    ##   Linux   - ~/.cache/snowflake/ocsp_response_cache
    ##   macOS   - ~/Library/Caches/Snowflake/ocsp_response_cache
    ##   Windows - %USERPROFILE%\AppData\Local\Snowflake\Caches\ocsp_response_cache
    #
    # ocsp_response_cache_filename: <OCSP_RESPONSE_CACHE_FILENAME>

    ## @param metric_groups - list of strings - required
    ## List Snowflake metric groups to collect. Metric groups are determined by the metric prefixes.
    ## - snowflake.query
    ## - snowflake.billing
    ## - snowflake.storage
    ## - snowflake.storage.database
    ## - snowflake.storage.table
    ## - snowflake.logins
    ## - snowflake.data_transfer
    ## - snowflake.auto_recluster
    ## - snowflake.pipe
    ## - snowflake.replication
    #
    metric_groups:
      - snowflake.query
      - snowflake.billing
      - snowflake.storage
      - snowflake.logins

    ## @param custom_queries - list of mappings - optional
    ## Each query must have 2 fields:
    ## 1. query - The SQL to execute. It can be a simple statement or a multi-line script.
    ##            Use the pipe `|` if you require a multi-line script.
    ## 2. columns - The list representing each column, ordered sequentially from left to right.
    ##              The number of columns must equal the number of columns returned in the query.
    ##              There are 2 required pieces of data:
    ##                a. name - The suffix to append to `clickhouse.` to form
    ##                          the full metric name. If `type` is `tag`, this column is
    ##                          considered a tag and applied to every
    ##                          metric collected by this particular query.
    ##                b. type - The submission method (gauge, monotonic_count, etc.).
    ##                          This can also be set to `tag` to tag each metric in the row
    ##                          with the name and value of the item in this column. You can
    ##                          use the `count` type to perform aggregation for queries that
    ##                          return multiple rows with the same or no tags.
    ##              Columns without a name are ignored, use this for any column you wish to skip:
    ##                - {}
    ## 3. tags (optional) - A list of tags to apply to each metric.
    #
    # custom_queries:
    #   - query: SELECT foo, COUNT(*) FROM table.events GROUP BY foo
    #     columns:
    #     - name: foo
    #       type: tag
    #     - name: event.total
    #       type: gauge
    #     tags:
    #     - test:snowflake

    ## @param tags - list of strings - optional
    ## A list of tags to attach to every metric and service check emitted by this instance.
    ##
    ## Learn more about tagging at https://docs.datadoghq.com/tagging
    #
    # tags:
    #   - <KEY_1>:<VALUE_1>
    #   - <KEY_2>:<VALUE_2>

    ## @param service - string - optional
    ## Attach the tag `service:<SERVICE>` to every metric, event, and service check emitted by this integration.
    ##
    ## Overrides any `service` defined in the `init_config` section.
    #
    # service: <SERVICE>

    ## @param min_collection_interval - number - optional - default: 3600
    ## This changes the collection interval of the check. For more information, see:
    ## https://docs.datadoghq.com/developers/write_agent_check/#collection-interval
    ##
    ## NOTE: The Snowflake ACCOUNT_USAGE schema views do not update in frequently.
    #
    # min_collection_interval: 3600

    ## @param empty_default_hostname - boolean - optional - default: false
    ## This forces the check to send metrics with no hostname.
    ##
    ## This is useful for cluster-level checks.
    #
    # empty_default_hostname: false
