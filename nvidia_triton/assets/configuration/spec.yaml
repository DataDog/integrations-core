name: Nvidia Triton
files:
- name: nvidia_triton.yaml
  options:
  - template: init_config
    options:
    - template: init_config/default
  - template: instances
    options:
    - template: instances/openmetrics
      overrides:
        openmetrics_endpoint.required: true
        openmetrics_endpoint.hidden: false
        openmetrics_endpoint.display_priority: 2
        openmetrics_endpoint.value.example: http://localhost:8002/metrics
        openmetrics_endpoint.description: |
          Endpoint exposing the Nvidia Triton's Prometheus metrics. For more information refer to:
          https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/metrics.html#metrics
    - name: collect_server_info
      display_priority: 1
      description: |
        Whether to collect server status and metadata the HTTP endpoint of Nvidia Triton API.
        https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/protocol/README.html
      value:
        display_default: true
        example: true
        type: boolean
    - name: server_port
      description: |
        The port exposing the HTTP Endpoint of the Nvidia Triton API.
        This will be ignored if `collect_server_info` is false.
      value:
        display_default: 8000
        type: integer
  - template: logs
    example:
    - type: docker
      source: nvidia_triton
      service: <SERVICE>
