metric_name,metric_type,interval,unit_name,per_unit_name,description,orientation,integration,short_name,curated_metric
nvidia_triton.cache.insertion.duration,gauge,,microsecond,,"Total cache insertion duration, in microseconds",0,nvidia_triton,,
nvidia_triton.cache.lookup.duration,gauge,,microsecond,,"Total cache lookup duration (hit and miss), in microseconds",0,nvidia_triton,,
nvidia_triton.cache.num.entries,gauge,,,,Number of responses stored in response cache,0,nvidia_triton,,
nvidia_triton.cache.num.evictions,gauge,,,,Number of cache evictions in response cache,0,nvidia_triton,,
nvidia_triton.cache.num.hits,gauge,,,,Number of cache hits in response cache,0,nvidia_triton,,
nvidia_triton.cache.num.lookups,gauge,,,,Number of cache lookups in response cache,0,nvidia_triton,,
nvidia_triton.cache.num.misses,gauge,,,,Number of cache misses in response cache,0,nvidia_triton,,
nvidia_triton.cache.util,gauge,,,,Cache utilization [0.0 - 1.0],0,nvidia_triton,,
nvidia_triton.cpu.memory.total_bytes,gauge,,byte,,"CPU total memory (RAM), in bytes",0,nvidia_triton,,
nvidia_triton.cpu.memory.used_bytes,gauge,,byte,,"CPU used memory (RAM), in bytes",0,nvidia_triton,,
nvidia_triton.cpu.utilization,gauge,,,,CPU utilization rate [0.0 - 1.0],0,nvidia_triton,,
nvidia_triton.energy.consumption.count,count,,,,GPU energy consumption in joules since the Triton Server started,0,nvidia_triton,,
nvidia_triton.gpu.memory.total_bytes,gauge,,byte,,"GPU total memory, in bytes",0,nvidia_triton,,
nvidia_triton.gpu.memory.used_bytes,gauge,,byte,,"GPU used memory, in bytes",0,nvidia_triton,,
nvidia_triton.gpu.power.limit,gauge,,watt,,GPU power management limit in watts,0,nvidia_triton,,
nvidia_triton.gpu.power.usage,gauge,,watt,,GPU power usage in watts,0,nvidia_triton,,
nvidia_triton.gpu.utilization,gauge,,,,GPU utilization rate [0.0 - 1.0),0,nvidia_triton,,
nvidia_triton.inference.compute.infer.duration_us.count,count,,microsecond,,Cumulative compute inference duration in microseconds (does not include cached requests),0,nvidia_triton,,
nvidia_triton.inference.compute.infer.summary_us.count,count,,microsecond,,Cumulative compute inference duration in microseconds (count) (does not include cached requests),0,nvidia_triton,,
nvidia_triton.inference.compute.infer.summary_us.quantile,gauge,,microsecond,,Cumulative compute inference duration in microseconds (quantile)(does not include cached requests),0,nvidia_triton,,
nvidia_triton.inference.compute.infer.summary_us.sum,count,,microsecond,,Cumulative compute inference duration in microseconds (sum) (does not include cached requests),0,nvidia_triton,,
nvidia_triton.inference.compute.input.duration_us.count,count,,microsecond,,Cumulative compute input duration in microseconds (does not include cached requests),0,nvidia_triton,,
nvidia_triton.inference.compute.input.summary_us.count,count,,microsecond,,Cumulative compute input duration in microseconds (sum) (does not include cached requests),0,nvidia_triton,,
nvidia_triton.inference.compute.input.summary_us.quantile,gauge,,microsecond,,Cumulative compute input duration in microseconds (quantile) (does not include cached requests),0,nvidia_triton,,
nvidia_triton.inference.compute.input.summary_us.sum,count,,microsecond,,Cumulative compute input duration in microseconds (count) (does not include cached requests),0,nvidia_triton,,
nvidia_triton.inference.compute.output.duration_us.count,count,,microsecond,,Cumulative inference compute output duration in microseconds (does not include cached requests),0,nvidia_triton,,
nvidia_triton.inference.compute.output.summary_us.count,count,,microsecond,,Cumulative inference compute output duration in microseconds (count) (does not include cached requests),0,nvidia_triton,,
nvidia_triton.inference.compute.output.summary_us.quantile,gauge,,microsecond,,Cumulative inference compute output duration in microseconds (quantile) (does not include cached requests),0,nvidia_triton,,
nvidia_triton.inference.compute.output.summary_us.sum,count,,microsecond,,Cumulative inference compute output duration in microseconds (sum) (does not include cached requests),0,nvidia_triton,,
nvidia_triton.inference.count.count,count,,,,Number of inferences performed (does not include cached requests),0,nvidia_triton,,
nvidia_triton.inference.exec.count.count,count,,,,Number of model executions performed (does not include cached requests),0,nvidia_triton,,
nvidia_triton.inference.pending.request.count,gauge,,,,Instantaneous number of pending requests awaiting execution per-model.,0,nvidia_triton,,
nvidia_triton.inference.queue.duration_us.count,count,,microsecond,,Cumulative inference queuing duration in microseconds (includes cached requests),0,nvidia_triton,,
nvidia_triton.inference.queue.summary_us.count,count,,microsecond,,Summary of inference queuing duration in microseconds (count) (includes cached requests),0,nvidia_triton,,
nvidia_triton.inference.queue.summary_us.quantile,gauge,,microsecond,,Summary of inference queuing duration in microseconds (quantile) (includes cached requests),0,nvidia_triton,,
nvidia_triton.inference.queue.summary_us.sum,count,,microsecond,,Summary of inference queuing duration in microseconds (sum) (includes cached requests),0,nvidia_triton,,
nvidia_triton.inference.request.duration_us.count,count,,microsecond,,Cumulative inference request duration in microseconds (includes cached requests),0,nvidia_triton,,
nvidia_triton.inference.request.summary_us.count,count,,microsecond,,Summary of inference request duration in microseconds (count) (includes cached requests),0,nvidia_triton,,
nvidia_triton.inference.request.summary_us.quantile,gauge,,microsecond,,Summary of inference request duration in microseconds (quantile) (includes cached requests),0,nvidia_triton,,
nvidia_triton.inference.request.summary_us.sum,count,,microsecond,,Summary of inference request duration in microseconds (sum) (includes cached requests),0,nvidia_triton,,
nvidia_triton.inference.request_failure.count,count,,,,"Number of failed inference requests, all batch sizes",0,nvidia_triton,,
nvidia_triton.inference.request_success.count,count,,,,"Number of successful inference requests, all batch sizes",0,nvidia_triton,,
