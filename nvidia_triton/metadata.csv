metric_name,metric_type,interval,unit_name,per_unit_name,description,orientation,integration,short_name,curated_metric
nvidia_triton.nv.cache.insertion.duration,gauge,,,,"Total cache insertion duration, in microseconds",0,nvidia_triton,,
nvidia_triton.nv.cache.lookup.duration,gauge,,,,"Total cache lookup duration (hit and miss), in microseconds",0,nvidia_triton,,
nvidia_triton.nv.cache.num.entries,gauge,,,,Number of responses stored in response cache,0,nvidia_triton,,
nvidia_triton.nv.cache.num.evictions,gauge,,,,Number of cache evictions in response cache,0,nvidia_triton,,
nvidia_triton.nv.cache.num.hits,gauge,,,,Number of cache hits in response cache,0,nvidia_triton,,
nvidia_triton.nv.cache.num.lookups,gauge,,,,Number of cache lookups in response cache,0,nvidia_triton,,
nvidia_triton.nv.cache.num.misses,gauge,,,,Number of cache misses in response cache,0,nvidia_triton,,
nvidia_triton.nv.cache.util,gauge,,,,Cache utilization [0.0 - 1.0],0,nvidia_triton,,
nvidia_triton.nv.cpu.memory.total_bytes,gauge,,,,"CPU total memory (RAM), in bytes",0,nvidia_triton,,
nvidia_triton.nv.cpu.memory.used_bytes,gauge,,,,"CPU used memory (RAM), in bytes",0,nvidia_triton,,
nvidia_triton.nv.cpu.utilization,gauge,,,,CPU utilization rate [0.0 - 1.0],0,nvidia_triton,,
nvidia_triton.nv.energy.consumption,count,,,,GPU energy consumption in joules since the Triton Server started,0,nvidia_triton,,
nvidia_triton.nv.gpu.memory.total_bytes,gauge,,,,"GPU total memory, in bytes",0,nvidia_triton,,
nvidia_triton.nv.gpu.memory.used_bytes,gauge,,,,"GPU used memory, in bytes",0,nvidia_triton,,
nvidia_triton.nv.gpu.power.limit,gauge,,,,GPU power management limit in watts,0,nvidia_triton,,
nvidia_triton.nv.gpu.power.usage,gauge,,,,GPU power usage in watts,0,nvidia_triton,,
nvidia_triton.nv.gpu.utilization,gauge,,,,GPU utilization rate [0.0 - 1.0),0,nvidia_triton,,
nvidia_triton.nv.inference.compute.infer.duration_us,count,,,,Cumulative compute inference duration in microseconds (does not include cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.compute.infer.summary_us.quantile,gauge,,,,Cumulative compute inference duration in microseconds (quantile)(does not include cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.compute.infer.summary_us.count,count,,,,Cumulative compute inference duration in microseconds (count) (does not include cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.compute.infer.summary_us.sum,count,,,,Cumulative compute inference duration in microseconds (sum) (does not include cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.compute.input.duration_us,count,,,,Cumulative compute input duration in microseconds (does not include cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.compute.input.summary_us.quantile,gauge,,,,Cumulative compute input duration in microseconds (quantile) (does not include cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.compute.input.summary_us.count,count,,,,Cumulative compute input duration in microseconds (sum) (does not include cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.compute.input.summary_us.sum,count,,,,Cumulative compute input duration in microseconds (count) (does not include cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.compute.output.duration_us,count,,,,Cumulative inference compute output duration in microseconds (does not include cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.compute.output.summary_us.quantile,gauge,,,,Cumulative inference compute output duration in microseconds (quantile) (does not include cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.compute.output.summary_us.count,count,,,,Cumulative inference compute output duration in microseconds (count) (does not include cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.compute.output.summary_us.sum,count,,,,Cumulative inference compute output duration in microseconds (sum) (does not include cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.count,count,,,,Number of inferences performed (does not include cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.exec.count,count,,,,Number of model executions performed (does not include cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.pending.request.count,gauge,,,,Instantaneous number of pending requests awaiting execution per-model.,0,nvidia_triton,,
nvidia_triton.nv.inference.queue.duration_us,count,,,,Cumulative inference queuing duration in microseconds (includes cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.queue.summary_us.quantile,gauge,,,,Summary of inference queuing duration in microseconds (quantile) (includes cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.queue.summary_us.sum,count,,,,Summary of inference queuing duration in microseconds (sum) (includes cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.queue.summary_us.count,count,,,,Summary of inference queuing duration in microseconds (count) (includes cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.request.duration_us,count,,,,Cumulative inference request duration in microseconds (includes cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.request.summary_us.quantile,gauge,,,,Summary of inference request duration in microseconds (quantile) (includes cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.request.summary_us.sum,count,,,,Summary of inference request duration in microseconds (sum) (includes cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.request.summary_us.count,count,,,,Summary of inference request duration in microseconds (count) (includes cached requests),0,nvidia_triton,,
nvidia_triton.nv.inference.request_failure,count,,,,"Number of failed inference requests, all batch sizes",0,nvidia_triton,,
nvidia_triton.nv.inference.request_success,count,,,,"Number of successful inference requests, all batch sizes",0,nvidia_triton,,
