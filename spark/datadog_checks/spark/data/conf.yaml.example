## All options defined here are available to all instances.
#
init_config:

    ## @param proxy - mapping - optional
    ## Set HTTP or HTTPS proxies for all instances. Use the `no_proxy` list
    ## to specify hosts that must bypass proxies.
    ##
    ## The SOCKS protocol is also supported like so:
    ##
    ##   socks5://user:pass@host:port
    ##
    ## Using the scheme `socks5` causes the DNS resolution to happen on the
    ## client, rather than on the proxy server. This is in line with `curl`,
    ## which uses the scheme to decide whether to do the DNS resolution on
    ## the client or proxy. If you want to resolve the domains on the proxy
    ## server, use `socks5h` as the scheme.
    #
    # proxy:
    #   http: http://<PROXY_SERVER_FOR_HTTP>:<PORT>
    #   https: https://<PROXY_SERVER_FOR_HTTPS>:<PORT>
    #   no_proxy:
    #   - <HOSTNAME_1>
    #   - <HOSTNAME_2>

    ## @param skip_proxy - boolean - optional - default: false
    ## If set to `true`, this makes the check bypass any proxy
    ## settings enabled and attempt to reach services directly.
    #
    # skip_proxy: false

    ## @param timeout - number - optional - default: 10
    ## The timeout for connecting to services.
    #
    # timeout: 10

    ## @param service - string - optional
    ## Attach the tag `service:<SERVICE>` to every metric, event, and service check emitted by this integration.
    ##
    ## Additionally, this sets the default `service` for every log source.
    #
    # service: <SERVICE>

## Every instance is scheduled independently of the others.
#
instances:

    ## @param spark_url - string - required
    ## The Spark check can retrieve metrics from Standalone Spark, YARN and Mesos.
    ##
    ## For Spark Standalone, `spark_url` must be set to the Spark master's web UI.
    ## This is "http://localhost:8080" by default.
    ##
    ## For YARN, `spark_url` must be set to YARN's resource manager address. The
    ## ResourceManager host name can be found in the yarn-site.xml conf file
    ## under the `property yarn.resourcemanager.address` The ResourceManager port
    ## can be found in the yarn-site.xml conf file under the property
    ## `yarn.resourcemanager.webapp.address`. This is "http://localhost:8088"
    ## by default.
    ##
    ## For Mesos, `spark_url` must be set to the Mesos master's web UI. This is
    ## "http://<master_ip>:5050" by default, where `<master_ip>` is the IP
    ## address or resolvable host name for the Mesos master.
    ##
    ## For Kubernetes or a standalone spark driver, `spark_url` must be set to the spark application driver IP.
    ## "http://<driver_ip>:4040" by default, where `<driver_ip>` is the IP
    ## address or resolvable spark driver service name.
    #
  - spark_url: http://localhost:8080

    ## @param cluster_name - string - required
    ## A friendly name for the cluster.
    #
    cluster_name: <CLUSTER_NAME>

    ## @param spark_cluster_mode - string - optional - default: spark_yarn_mode
    ## To enable monitoring of a Standalone Spark cluster, the spark cluster
    ## mode must be set. Choose the cluster mode between :
    ##  * `spark_yarn_mode`
    ##  * `spark_standalone_mode`
    ##  * `spark_mesos_mode`
    ##  * `spark_driver_mode`
    #
    # spark_cluster_mode: spark_yarn_mode

    ## @param spark_ui_ports - list of integers - optional
    ## If you're using multiple frameworks with Mesos, you can specify which
    ## port(s) the Spark Web UI runs on and the check filters out the frameworks
    ## whose port don't match. If the check attempts to connect to a framework that
    ## requires authentication it will fail; this is to prevent that.
    #
    # spark_ui_ports:
    #   - <PORT_1>
    #   - <PORT_2>

    ## @param spark_pre_20_mode - boolean - optional - default: false
    ## To use an older (versions prior to 2.0) Standalone Spark cluster,
    ## the 'spark_pre_20_mode' must be set to true
    #
    # spark_pre_20_mode: false

    ## @param spark_proxy_enabled - boolean - optional - default: false
    ## If you have enabled the spark UI proxy, set this to `true`
    #
    # spark_proxy_enabled: false

    ## @param streaming_metrics - boolean - optional - default: true
    ## Enable collection of streaming statistics.
    ## Both Spark Streaming (DStream) and Structured Streaming (DataFrame) can be monitored but the latter requires
    ## additional configuration. To collect Structured Streaming metrics you need to:
    ##   1. The default MetricsServlet sink has not been disabled.
    ##   2. Set 'spark.sql.streaming.metricsEnabled=true' in the configuration.
    ## For more details refer to Spark official documentation:
    ##   1. https://spark.apache.org/docs/latest/monitoring.html#metrics
    ##   2. https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#reporting-metrics-using-dropwizard 
    ## Note that if the MetricsServlet is not serving the JSON metrics payload on the default path (/metrics/json)
    ## you also need to set the `metricsservlet_path` option below.
    #
    # streaming_metrics: true

    ## @param metricsservlet_path - string - optional - default: /metrics/json
    ## Some Spark metrics are only available via a DropWizard sink, notably the Structured Streaming ones. The
    ## integration fetches a JSON metrics payload from each application by leveraging the default MetricsServlet
    ## sink which exposes metrics on the /metrics/json path by default for all applications. If you configured
    ## the MetricsServlet to use a non-default path, you also need to update the following field to tell
    ## the integration about it.
    #
    # metricsservlet_path: /metrics/json

    ## @param executor_level_metrics - boolean - optional - default: false
    ## Enable collection of more granular executor metrics.
    #
    # executor_level_metrics: false

    ## @param disable_legacy_cluster_tag - boolean - optional - default: false
    ## Enable to stop submitting the tag `cluster_name`, which has been renamed to `spark_cluster`.
    #
    disable_legacy_cluster_tag: true

    ## @param enable_query_name_tag - boolean - optional - default: false
    ## Enable to add a `query_name` tag for Structured Streaming metrics.
    ## This option should ONLY be enabled if the stream had a `.queryName` param supplied on `writeStream`,
    ## otherwise the stream is given a default UUID.
    ## See: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#starting-streaming-queries
    #
    # enable_query_name_tag: true

    ## @param proxy - mapping - optional
    ## This overrides the `proxy` setting in `init_config`.
    ##
    ## Set HTTP or HTTPS proxies for this instance. Use the `no_proxy` list
    ## to specify hosts that must bypass proxies.
    ##
    ## The SOCKS protocol is also supported, for example:
    ##
    ##   socks5://user:pass@host:port
    ##
    ## Using the scheme `socks5` causes the DNS resolution to happen on the
    ## client, rather than on the proxy server. This is in line with `curl`,
    ## which uses the scheme to decide whether to do the DNS resolution on
    ## the client or proxy. If you want to resolve the domains on the proxy
    ## server, use `socks5h` as the scheme.
    #
    # proxy:
    #   http: http://<PROXY_SERVER_FOR_HTTP>:<PORT>
    #   https: https://<PROXY_SERVER_FOR_HTTPS>:<PORT>
    #   no_proxy:
    #   - <HOSTNAME_1>
    #   - <HOSTNAME_2>

    ## @param skip_proxy - boolean - optional - default: false
    ## This overrides the `skip_proxy` setting in `init_config`.
    ##
    ## If set to `true`, this makes the check bypass any proxy
    ## settings enabled and attempt to reach services directly.
    #
    # skip_proxy: false

    ## @param auth_type - string - optional - default: basic
    ## The type of authentication to use. The available types (and related options) are:
    ##
    ##   - basic
    ##     |__ username
    ##     |__ password
    ##     |__ use_legacy_auth_encoding
    ##   - digest
    ##     |__ username
    ##     |__ password
    ##   - ntlm
    ##     |__ ntlm_domain
    ##     |__ password
    ##   - kerberos
    ##     |__ kerberos_auth
    ##     |__ kerberos_cache
    ##     |__ kerberos_delegate
    ##     |__ kerberos_force_initiate
    ##     |__ kerberos_hostname
    ##     |__ kerberos_keytab
    ##     |__ kerberos_principal
    ##   - aws
    ##     |__ aws_region
    ##     |__ aws_host
    ##     |__ aws_service
    ##
    ## The `aws` auth type relies on boto3 to automatically gather AWS credentials, for example: from `.aws/credentials`.
    ## Details: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#configuring-credentials
    #
    # auth_type: basic

    ## @param use_legacy_auth_encoding - boolean - optional - default: true
    ## When `auth_type` is set to `basic`, this determines whether to encode as `latin1` rather than `utf-8`.
    #
    # use_legacy_auth_encoding: true

    ## @param username - string - optional
    ## The username to use if services are behind basic or digest auth.
    #
    # username: <USERNAME>

    ## @param password - string - optional
    ## The password to use if services are behind basic or NTLM auth.
    #
    # password: <PASSWORD>

    ## @param ntlm_domain - string - optional
    ## If your services use NTLM authentication, specify
    ## the domain used in the check. For NTLM Auth, append
    ## the username to domain, not as the `username` parameter.
    #
    # ntlm_domain: <NTLM_DOMAIN>\<USERNAME>

    ## @param kerberos_auth - string - optional - default: disabled
    ## If your services use Kerberos authentication, you can specify the Kerberos
    ## strategy to use between:
    ##
    ##   - required
    ##   - optional
    ##   - disabled
    ##
    ## See https://github.com/requests/requests-kerberos#mutual-authentication
    #
    # kerberos_auth: disabled

    ## @param kerberos_cache - string - optional
    ## Sets the KRB5CCNAME environment variable.
    ## It should point to a credential cache with a valid TGT.
    #
    # kerberos_cache: <KERBEROS_CACHE>

    ## @param kerberos_delegate - boolean - optional - default: false
    ## Set to `true` to enable Kerberos delegation of credentials to a server that requests delegation.
    ##
    ## See https://github.com/requests/requests-kerberos#delegation
    #
    # kerberos_delegate: false

    ## @param kerberos_force_initiate - boolean - optional - default: false
    ## Set to `true` to preemptively initiate the Kerberos GSS exchange and
    ## present a Kerberos ticket on the initial request (and all subsequent).
    ##
    ## See https://github.com/requests/requests-kerberos#preemptive-authentication
    #
    # kerberos_force_initiate: false

    ## @param kerberos_hostname - string - optional
    ## Override the hostname used for the Kerberos GSS exchange if its DNS name doesn't
    ## match its Kerberos hostname, for example: behind a content switch or load balancer.
    ##
    ## See https://github.com/requests/requests-kerberos#hostname-override
    #
    # kerberos_hostname: <KERBEROS_HOSTNAME>

    ## @param kerberos_principal - string - optional
    ## Set an explicit principal, to force Kerberos to look for a
    ## matching credential cache for the named user.
    ##
    ## See https://github.com/requests/requests-kerberos#explicit-principal
    #
    # kerberos_principal: <KERBEROS_PRINCIPAL>

    ## @param kerberos_keytab - string - optional
    ## Set the path to your Kerberos key tab file.
    #
    # kerberos_keytab: <KEYTAB_FILE_PATH>

    ## @param auth_token - mapping - optional
    ## This allows for the use of authentication information from dynamic sources.
    ## Both a reader and writer must be configured.
    ##
    ## The available readers are:
    ##
    ##   - type: file
    ##     path (required): The absolute path for the file to read from.
    ##     pattern: A regular expression pattern with a single capture group used to find the
    ##              token rather than using the entire file, for example: Your secret is (.+)
    ##
    ##   - type: dcos_auth
    ##     login_url (required): DC/OS login endpoint
    ##     service_account (required): The DC/OS service account to authenticate.
    ##     private_key_path (required): The absolute path for the DC/OS service account.
    ##     expiration: Token expiration in seconds, defaults to 300 (5 min).
    ##
    ## The available writers are:
    ##
    ##   - type: header
    ##     name (required): The name of the field, for example: Authorization
    ##     value: The template value, for example `Bearer <TOKEN>`. The default is: <TOKEN>
    ##     placeholder: The substring in `value` to replace by the token, defaults to: <TOKEN>
    #
    # auth_token:
    #   reader:
    #     type: <READER_TYPE>
    #     <OPTION_1>: <VALUE_1>
    #     <OPTION_2>: <VALUE_2>
    #   writer:
    #     type: <WRITER_TYPE>
    #     <OPTION_1>: <VALUE_1>
    #     <OPTION_2>: <VALUE_2>

    ## @param aws_region - string - optional
    ## If your services require AWS Signature Version 4 signing, set the region.
    ##
    ## See https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html
    #
    # aws_region: <AWS_REGION>

    ## @param aws_host - string - optional
    ## If your services require AWS Signature Version 4 signing, set the host.
    ## This only needs the hostname and does not require the protocol (HTTP, HTTPS, and more).
    ## For example, if connecting to https://us-east-1.amazonaws.com/, set `aws_host` to `us-east-1.amazonaws.com`.
    ##
    ## Note: This setting is not necessary for official integrations.
    ##
    ## See https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html
    #
    # aws_host: <AWS_HOST>

    ## @param aws_service - string - optional
    ## If your services require AWS Signature Version 4 signing, set the service code. For a list
    ## of available service codes, see https://docs.aws.amazon.com/general/latest/gr/rande.html
    ##
    ## Note: This setting is not necessary for official integrations.
    ##
    ## See https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html
    #
    # aws_service: <AWS_SERVICE>

    ## @param tls_verify - boolean - optional - default: true
    ## Instructs the check to validate the TLS certificate of services.
    #
    # tls_verify: true

    ## @param tls_use_host_header - boolean - optional - default: false
    ## If a `Host` header is set, this enables its use for SNI (matching against the TLS certificate CN or SAN).
    #
    # tls_use_host_header: false

    ## @param tls_ignore_warning - boolean - optional - default: false
    ## If `tls_verify` is disabled, security warnings are logged by the check.
    ## Disable those by setting `tls_ignore_warning` to true.
    #
    # tls_ignore_warning: false

    ## @param tls_cert - string - optional
    ## The path to a single file in PEM format containing a certificate as well as any
    ## number of CA certificates needed to establish the certificate's authenticity for
    ## use when connecting to services. It may also contain an unencrypted private key to use.
    #
    # tls_cert: <CERT_PATH>

    ## @param tls_private_key - string - optional
    ## The unencrypted private key to use for `tls_cert` when connecting to services. This is
    ## required if `tls_cert` is set and it does not already contain a private key.
    #
    # tls_private_key: <PRIVATE_KEY_PATH>

    ## @param tls_ca_cert - string - optional
    ## The path to a file of concatenated CA certificates in PEM format or a directory
    ## containing several CA certificates in PEM format. If a directory, the directory
    ## must have been processed using the c_rehash utility supplied with OpenSSL. See:
    ## https://www.openssl.org/docs/manmaster/man3/SSL_CTX_load_verify_locations.html
    #
    # tls_ca_cert: <CA_CERT_PATH>

    ## @param tls_protocols_allowed - list of strings - optional
    ## The expected versions of TLS/SSL when fetching intermediate certificates.
    ## Only `SSLv3`, `TLSv1.2`, `TLSv1.3` are allowed by default. The possible values are:
    ##   SSLv3
    ##   TLSv1
    ##   TLSv1.1
    ##   TLSv1.2
    ##   TLSv1.3
    #
    # tls_protocols_allowed:
    #   - SSLv3
    #   - TLSv1.2
    #   - TLSv1.3

    ## @param headers - mapping - optional
    ## The headers parameter allows you to send specific headers with every request.
    ## You can use it for explicitly specifying the host header or adding headers for
    ## authorization purposes.
    ##
    ## This overrides any default headers.
    #
    # headers:
    #   Host: <ALTERNATIVE_HOSTNAME>
    #   X-Auth-Token: <AUTH_TOKEN>

    ## @param extra_headers - mapping - optional
    ## Additional headers to send with every request.
    #
    # extra_headers:
    #   Host: <ALTERNATIVE_HOSTNAME>
    #   X-Auth-Token: <AUTH_TOKEN>

    ## @param timeout - number - optional - default: 10
    ## The timeout for accessing services.
    ##
    ## This overrides the `timeout` setting in `init_config`.
    #
    # timeout: 10

    ## @param connect_timeout - number - optional
    ## The connect timeout for accessing services. Defaults to `timeout`.
    #
    # connect_timeout: <CONNECT_TIMEOUT>

    ## @param read_timeout - number - optional
    ## The read timeout for accessing services. Defaults to `timeout`.
    #
    # read_timeout: <READ_TIMEOUT>

    ## @param request_size - number - optional - default: 16
    ## The number of kibibytes (KiB) to read from streaming HTTP responses at a time.
    #
    # request_size: 16

    ## @param log_requests - boolean - optional - default: false
    ## Whether or not to debug log the HTTP(S) requests made, including the method and URL.
    #
    # log_requests: false

    ## @param persist_connections - boolean - optional - default: false
    ## Whether or not to persist cookies and use connection pooling for improved performance.
    #
    # persist_connections: false

    ## @param allow_redirects - boolean - optional - default: true
    ## Whether or not to allow URL redirection.
    #
    # allow_redirects: true

    ## @param tags - list of strings - optional
    ## A list of tags to attach to every metric and service check emitted by this instance.
    ##
    ## Learn more about tagging at https://docs.datadoghq.com/tagging
    #
    # tags:
    #   - <KEY_1>:<VALUE_1>
    #   - <KEY_2>:<VALUE_2>

    ## @param service - string - optional
    ## Attach the tag `service:<SERVICE>` to every metric, event, and service check emitted by this integration.
    ##
    ## Overrides any `service` defined in the `init_config` section.
    #
    # service: <SERVICE>

    ## @param min_collection_interval - number - optional - default: 15
    ## This changes the collection interval of the check. For more information, see:
    ## https://docs.datadoghq.com/developers/write_agent_check/#collection-interval
    #
    # min_collection_interval: 15

    ## @param empty_default_hostname - boolean - optional - default: false
    ## This forces the check to send metrics with no hostname.
    ##
    ## This is useful for cluster-level checks.
    #
    # empty_default_hostname: false

    ## @param metric_patterns - mapping - optional
    ## A mapping of metrics to include or exclude, with each entry being a regular expression.
    ##
    ## Metrics defined in `exclude` will take precedence in case of overlap.
    #
    # metric_patterns:
    #   include:
    #   - <INCLUDE_REGEX>
    #   exclude:
    #   - <EXCLUDE_REGEX>

## Log Section
##
## type - required - Type of log input source (tcp / udp / file / windows_event).
## port / path / channel_path - required - Set port if type is tcp or udp.
##                                         Set path if type is file.
##                                         Set channel_path if type is windows_event.
## source  - required - Attribute that defines which integration sent the logs.
## encoding - optional - For file specifies the file encoding. Default is utf-8. Other
##                       possible values are utf-16-le and utf-16-be.
## service - optional - The name of the service that generates the log.
##                      Overrides any `service` defined in the `init_config` section.
## tags - optional - Add tags to the collected logs.
##
## Discover Datadog log collection: https://docs.datadoghq.com/logs/log_collection/
#
# logs:
#   - type: file
#     path: /var/log/spark/*.log
#     source: spark
#     service: <SERVICE_NAME>
