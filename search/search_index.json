{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Agent Integrations \u00b6 Welcome to the wonderful world of developing Agent Integrations for Datadog. Here we document how we do things, the processes for various tasks, coding conventions & best practices, the internals of our testing infrastructure, and so much more. If you are intrigued, continue reading. If not, continue all the same Getting started \u00b6 To work on any integration (a.k.a. Check ), you must setup your development environment. After that you may immediately begin testing or read through the best practices we strive to follow. Also, feel free to check out how ddev works and browse the API reference of the base package.","title":"Home"},{"location":"#agent-integrations","text":"Welcome to the wonderful world of developing Agent Integrations for Datadog. Here we document how we do things, the processes for various tasks, coding conventions & best practices, the internals of our testing infrastructure, and so much more. If you are intrigued, continue reading. If not, continue all the same","title":"Agent Integrations"},{"location":"#getting-started","text":"To work on any integration (a.k.a. Check ), you must setup your development environment. After that you may immediately begin testing or read through the best practices we strive to follow. Also, feel free to check out how ddev works and browse the API reference of the base package.","title":"Getting started"},{"location":"e2e/","text":"E2E \u00b6 Any integration that makes use of our pytest plugin in its test suite supports end-to-end testing on a live Datadog Agent . The entrypoint for E2E management is the command group ddev env . Discovery \u00b6 Use the ls command to see what environments are available, for example: $ ddev env ls envoy envoy: py27 py38 You'll notice that only environments that actually run tests are available. Running simply ddev env ls with no arguments will display the active environments. Creation \u00b6 To start an environment run ddev env start <INTEGRATION> <ENVIRONMENT> , for example: $ ddev env start envoy py38 Setting up environment `py38`... success! Updating `datadog/agent-dev:master`... success! Detecting the major version... Agent 7 detected Writing configuration for `py38`... success! Starting the Agent... success! Config file (copied to your clipboard): C:\\Users\\ofek\\AppData\\Local\\dd-checks-dev\\envs\\envoy\\py38\\config\\envoy.yaml To run this check, do: ddev env check envoy py38 To stop this check, do: ddev env stop envoy py38 This sets up the selected environment and an instance of the Agent running in a Docker container. The default configuration is defined by each environment's test suite and is saved to a file, which is then mounted to the Agent container so you may freely modify it. Let's see what we have running: $ docker ps --format \"table {{.Image}}\\t{{.Status}}\\t{{.Ports}}\\t{{.Names}}\" IMAGE STATUS PORTS NAMES datadog/agent-dev:master-py3 Up 4 seconds (health: starting) dd_envoy_py38 default_service2 Up 5 seconds 80/tcp, 10000/tcp default_service2_1 envoyproxy/envoy:latest Up 5 seconds 0.0.0.0:8001->8001/tcp, 10000/tcp, 0.0.0.0:8000->80/tcp default_front-envoy_1 default_xds Up 5 seconds 8080/tcp default_xds_1 default_service1 Up 5 seconds 80/tcp, 10000/tcp default_service1_1 Agent version \u00b6 You can select a particular build of the Agent to use with the --agent / -a option. Any Docker image is valid e.g. datadog/agent:7.17.0 . A custom nightly build will be used by default, which is re-built on every commit to the Datadog Agent repository . Integration version \u00b6 By default the version of the integration used will be the one shipped with the chosen Agent version, as if you had passed in the --prod flag. If you wish to modify an integration and test changes in real time, use the --dev flag. Doing so will mount and install the integration in the Agent container. All modifications to the integration's directory will be propagated to the Agent, whether it be a code change or switching to a different Git branch. If you modify the base package then you will need to mount that with the --base flag, which implicitly activates --dev . Testing \u00b6 To run tests against the live Agent, use the ddev env test command. It is similar to the test command except it is capable of running tests marked as E2E , and only runs such tests. Automation \u00b6 You can use the --new-env / -ne flag to automate environment management. For example running: ddev env test apache:py38 vault:py38 -ne will start the py38 environment for Apache, run E2E tests, tear down the environment, and then do the same for Vault. Tip Since running tests implies code changes are being introduced, --new-env enables --dev by default. Execution \u00b6 Similar to the Agent's check command, you can perform manual check runs using ddev env check <INTEGRATION> <ENVIRONMENT> , for example: $ ddev env check envoy py38 --log-level debug ... ========= Collector ========= Running Checks ============== envoy (1.12.0) -------------- Instance ID: envoy:c705bd922a3c275c [OK] Configuration Source: file:/etc/datadog-agent/conf.d/envoy.d/envoy.yaml Total Runs: 1 Metric Samples: Last Run: 546, Total: 546 Events: Last Run: 0, Total: 0 Service Checks: Last Run: 1, Total: 1 Average Execution Time : 25ms Last Execution Date : 2020-02-17 00:58:05.000000 UTC Last Successful Execution Date : 2020-02-17 00:58:05.000000 UTC Debugging \u00b6 You may start an interactive debugging session using the --breakpoint / -b option. The option accepts an integer representing the line number at which to break. For convenience, 0 and -1 are shortcuts to the first and last line of the integration's check method, respectively. $ ddev env check envoy py38 -b 0 > /opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/envoy/envoy.py(34)check() -> custom_tags = instance.get('tags', []) (Pdb) list 29 self.blacklisted_metrics = set() 30 31 self.caching_metrics = None 32 33 def check(self, instance): 34 B-> custom_tags = instance.get('tags', []) 35 36 try: 37 stats_url = instance['stats_url'] 38 except KeyError: 39 msg = 'Envoy configuration setting `stats_url` is required' (Pdb) print(instance) {'stats_url': 'http://localhost:8001/stats'} Caveat The line number must be within the integration's check method. Refreshing state \u00b6 Testing and manual check runs always reflect the current state of code and configuration however, if you want to see the result of changes in-app , you will need to refresh the environment by running ddev env reload <INTEGRATION> <ENVIRONMENT> . Removal \u00b6 To stop an environment run ddev env stop <INTEGRATION> <ENVIRONMENT> . Any environments that haven't been explicitly stopped will show as active in the output of ddev env ls , even persisting through system restarts. If you are confident that environments are no longer active, you can run ddev env prune to remove all accumulated environment state.","title":"E2E"},{"location":"e2e/#e2e","text":"Any integration that makes use of our pytest plugin in its test suite supports end-to-end testing on a live Datadog Agent . The entrypoint for E2E management is the command group ddev env .","title":"E2E"},{"location":"e2e/#discovery","text":"Use the ls command to see what environments are available, for example: $ ddev env ls envoy envoy: py27 py38 You'll notice that only environments that actually run tests are available. Running simply ddev env ls with no arguments will display the active environments.","title":"Discovery"},{"location":"e2e/#creation","text":"To start an environment run ddev env start <INTEGRATION> <ENVIRONMENT> , for example: $ ddev env start envoy py38 Setting up environment `py38`... success! Updating `datadog/agent-dev:master`... success! Detecting the major version... Agent 7 detected Writing configuration for `py38`... success! Starting the Agent... success! Config file (copied to your clipboard): C:\\Users\\ofek\\AppData\\Local\\dd-checks-dev\\envs\\envoy\\py38\\config\\envoy.yaml To run this check, do: ddev env check envoy py38 To stop this check, do: ddev env stop envoy py38 This sets up the selected environment and an instance of the Agent running in a Docker container. The default configuration is defined by each environment's test suite and is saved to a file, which is then mounted to the Agent container so you may freely modify it. Let's see what we have running: $ docker ps --format \"table {{.Image}}\\t{{.Status}}\\t{{.Ports}}\\t{{.Names}}\" IMAGE STATUS PORTS NAMES datadog/agent-dev:master-py3 Up 4 seconds (health: starting) dd_envoy_py38 default_service2 Up 5 seconds 80/tcp, 10000/tcp default_service2_1 envoyproxy/envoy:latest Up 5 seconds 0.0.0.0:8001->8001/tcp, 10000/tcp, 0.0.0.0:8000->80/tcp default_front-envoy_1 default_xds Up 5 seconds 8080/tcp default_xds_1 default_service1 Up 5 seconds 80/tcp, 10000/tcp default_service1_1","title":"Creation"},{"location":"e2e/#agent-version","text":"You can select a particular build of the Agent to use with the --agent / -a option. Any Docker image is valid e.g. datadog/agent:7.17.0 . A custom nightly build will be used by default, which is re-built on every commit to the Datadog Agent repository .","title":"Agent version"},{"location":"e2e/#integration-version","text":"By default the version of the integration used will be the one shipped with the chosen Agent version, as if you had passed in the --prod flag. If you wish to modify an integration and test changes in real time, use the --dev flag. Doing so will mount and install the integration in the Agent container. All modifications to the integration's directory will be propagated to the Agent, whether it be a code change or switching to a different Git branch. If you modify the base package then you will need to mount that with the --base flag, which implicitly activates --dev .","title":"Integration version"},{"location":"e2e/#testing","text":"To run tests against the live Agent, use the ddev env test command. It is similar to the test command except it is capable of running tests marked as E2E , and only runs such tests.","title":"Testing"},{"location":"e2e/#automation","text":"You can use the --new-env / -ne flag to automate environment management. For example running: ddev env test apache:py38 vault:py38 -ne will start the py38 environment for Apache, run E2E tests, tear down the environment, and then do the same for Vault. Tip Since running tests implies code changes are being introduced, --new-env enables --dev by default.","title":"Automation"},{"location":"e2e/#execution","text":"Similar to the Agent's check command, you can perform manual check runs using ddev env check <INTEGRATION> <ENVIRONMENT> , for example: $ ddev env check envoy py38 --log-level debug ... ========= Collector ========= Running Checks ============== envoy (1.12.0) -------------- Instance ID: envoy:c705bd922a3c275c [OK] Configuration Source: file:/etc/datadog-agent/conf.d/envoy.d/envoy.yaml Total Runs: 1 Metric Samples: Last Run: 546, Total: 546 Events: Last Run: 0, Total: 0 Service Checks: Last Run: 1, Total: 1 Average Execution Time : 25ms Last Execution Date : 2020-02-17 00:58:05.000000 UTC Last Successful Execution Date : 2020-02-17 00:58:05.000000 UTC","title":"Execution"},{"location":"e2e/#debugging","text":"You may start an interactive debugging session using the --breakpoint / -b option. The option accepts an integer representing the line number at which to break. For convenience, 0 and -1 are shortcuts to the first and last line of the integration's check method, respectively. $ ddev env check envoy py38 -b 0 > /opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/envoy/envoy.py(34)check() -> custom_tags = instance.get('tags', []) (Pdb) list 29 self.blacklisted_metrics = set() 30 31 self.caching_metrics = None 32 33 def check(self, instance): 34 B-> custom_tags = instance.get('tags', []) 35 36 try: 37 stats_url = instance['stats_url'] 38 except KeyError: 39 msg = 'Envoy configuration setting `stats_url` is required' (Pdb) print(instance) {'stats_url': 'http://localhost:8001/stats'} Caveat The line number must be within the integration's check method.","title":"Debugging"},{"location":"e2e/#refreshing-state","text":"Testing and manual check runs always reflect the current state of code and configuration however, if you want to see the result of changes in-app , you will need to refresh the environment by running ddev env reload <INTEGRATION> <ENVIRONMENT> .","title":"Refreshing state"},{"location":"e2e/#removal","text":"To stop an environment run ddev env stop <INTEGRATION> <ENVIRONMENT> . Any environments that haven't been explicitly stopped will show as active in the output of ddev env ls , even persisting through system restarts. If you are confident that environments are no longer active, you can run ddev env prune to remove all accumulated environment state.","title":"Removal"},{"location":"setup/","text":"Setup \u00b6 This will be relatively painless, we promise! Integrations \u00b6 You will need to clone integrations-core and/or integrations-extras depending on which integrations you intend to work on. Python \u00b6 To work on any integration you must install Python 3.8+. After installation, restart your terminal and ensure that your newly installed Python comes first in your PATH . macOS We recommend using Homebrew . First update the formulae and Homebrew itself: brew update then either install Python: brew install python or upgrade it: brew upgrade python After it completes, check the output to see if it asked you to run any extra commands and if so, execute them. Verify successful PATH modification: which -a python Windows Windows users have it the easiest. Simply download the latest x86-64 executable installer from https://www.python.org/downloads/windows and run it. When prompted, be sure to select the option to add to your PATH . Also, it is recommended that you choose the per-user installation method. Verify successful PATH modification: where python Linux Ah, you enjoy difficult things. Are you using Gentoo? We recommend using either Miniconda or pyenv . Whatever you do, never modify the system Python. Verify successful PATH modification: which -a python ddev \u00b6 Installation \u00b6 You have 2 options to install the CLI provided by the package datadog-checks-dev . Warning For either option, if you are on macOS/Linux do not use sudo ! Doing so will result in a broken installation. Development \u00b6 If you cloned integrations-core and want to always use the version based on the current branch, run: python -m pip install -e \"path/to/datadog_checks_dev[cli]\" Note Be aware that this method does not keep track of dependencies so you will need to re-run the command if/when the required dependencies are changed. Stable \u00b6 The latest released version may be installed from PyPI : python -m pip install --upgrade \"datadog-checks-dev[cli]\" Configuration \u00b6 Upon the first invocation, ddev will create its config file if it does not yet exist. You will need to set the location of each cloned repository: ddev config set <REPO> /path/to/integrations-<REPO> The <REPO> may be either core or extras . By default, the repo core will be the target of all commands. If you want to switch to integrations-extras , run: ddev config set repo extras Docker \u00b6 Docker is used in nearly every integration's test suite therefore we simply require it to avoid confusion. macOS Install Docker Desktop for Mac . Right-click the Docker taskbar item and update Preferences > File Sharing with any locations you need to open. Windows Install Docker Desktop for Windows . Right-click the Docker taskbar item and update Settings > Shared Drives with any locations you need to open e.g. C:\\ . Linux Install Docker Engine for your distribution: Ubuntu Docker CE for Ubuntu Debian Docker CE for Debian Fedora Docker CE for Fedora CentOS Docker CE for CentOS Add your user to the docker group: sudo usermod -aG docker $USER Sign out and then back in again so your changes take effect. After installation, restart your terminal one last time.","title":"Setup"},{"location":"setup/#setup","text":"This will be relatively painless, we promise!","title":"Setup"},{"location":"setup/#integrations","text":"You will need to clone integrations-core and/or integrations-extras depending on which integrations you intend to work on.","title":"Integrations"},{"location":"setup/#python","text":"To work on any integration you must install Python 3.8+. After installation, restart your terminal and ensure that your newly installed Python comes first in your PATH . macOS We recommend using Homebrew . First update the formulae and Homebrew itself: brew update then either install Python: brew install python or upgrade it: brew upgrade python After it completes, check the output to see if it asked you to run any extra commands and if so, execute them. Verify successful PATH modification: which -a python Windows Windows users have it the easiest. Simply download the latest x86-64 executable installer from https://www.python.org/downloads/windows and run it. When prompted, be sure to select the option to add to your PATH . Also, it is recommended that you choose the per-user installation method. Verify successful PATH modification: where python Linux Ah, you enjoy difficult things. Are you using Gentoo? We recommend using either Miniconda or pyenv . Whatever you do, never modify the system Python. Verify successful PATH modification: which -a python","title":"Python"},{"location":"setup/#ddev","text":"","title":"ddev"},{"location":"setup/#installation","text":"You have 2 options to install the CLI provided by the package datadog-checks-dev . Warning For either option, if you are on macOS/Linux do not use sudo ! Doing so will result in a broken installation.","title":"Installation"},{"location":"setup/#development","text":"If you cloned integrations-core and want to always use the version based on the current branch, run: python -m pip install -e \"path/to/datadog_checks_dev[cli]\" Note Be aware that this method does not keep track of dependencies so you will need to re-run the command if/when the required dependencies are changed.","title":"Development"},{"location":"setup/#stable","text":"The latest released version may be installed from PyPI : python -m pip install --upgrade \"datadog-checks-dev[cli]\"","title":"Stable"},{"location":"setup/#configuration","text":"Upon the first invocation, ddev will create its config file if it does not yet exist. You will need to set the location of each cloned repository: ddev config set <REPO> /path/to/integrations-<REPO> The <REPO> may be either core or extras . By default, the repo core will be the target of all commands. If you want to switch to integrations-extras , run: ddev config set repo extras","title":"Configuration"},{"location":"setup/#docker","text":"Docker is used in nearly every integration's test suite therefore we simply require it to avoid confusion. macOS Install Docker Desktop for Mac . Right-click the Docker taskbar item and update Preferences > File Sharing with any locations you need to open. Windows Install Docker Desktop for Windows . Right-click the Docker taskbar item and update Settings > Shared Drives with any locations you need to open e.g. C:\\ . Linux Install Docker Engine for your distribution: Ubuntu Docker CE for Ubuntu Debian Docker CE for Debian Fedora Docker CE for Fedora CentOS Docker CE for CentOS Add your user to the docker group: sudo usermod -aG docker $USER Sign out and then back in again so your changes take effect. After installation, restart your terminal one last time.","title":"Docker"},{"location":"testing/","text":"Testing \u00b6 The entrypoint for testing any integration is the command ddev test , which accepts an arbitrary number of integrations as arguments. Under the hood, we use tox for environment management and pytest as our test framework. Discovery \u00b6 Use the --list / -l flag to see what environments are available, for example: $ ddev test postgres envoy -l postgres: py27-10 py27-11 py27-93 py27-94 py27-95 py27-96 py38-10 py38-11 py38-93 py38-94 py38-95 py38-96 format_style style envoy: py27 py38 bench format_style style You'll notice that all environments for running tests are prefixed with pyXY , indicating the Python version to use. If you don't have a particular version installed (for example Python 2.7), such environments will be skipped. The second part of a test environment's name corresponds to the version of the product. For example, the 11 in py38-11 implies tests will run against version 11.x of PostgreSQL. If there is no version suffix, it means that either: the version is pinned, usually set to pull the latest release, or there is no concept of a product, such as the disk check Usage \u00b6 Explicit \u00b6 Passing just the integration name will run every test environment e.g. executing ddev test envoy will run the environments py27 , py38 , and style . You may select a subset of environments to run by appending a : followed by a comma-separated list of environments. For example, executing: ddev test postgres:py38-11,style envoy:py38 will run, in order, the environments py38-11 and style for the PostgreSQL check and the environment py38 for the Envoy check. Detection \u00b6 If no integrations are specified then only integrations that were changed will be tested, based on a diff between the latest commit to the current and master branches. The criteria for an integration to be considered changed is based on the file extension of paths in the diff. So for example if only Markdown files were modified then nothing will be tested. The integrations will be tested in lexicographical order. Coverage \u00b6 To measure code coverage, use the --cov / -c flag. Doing so will display a summary of coverage statistics after successful execution of integrations' tests. $ ddev test tls -c ... ---------- Coverage report ---------- Name Stmts Miss Branch BrPart Cover ------------------------------------------------------------------- datadog_checks\\tls\\__about__.py 1 0 0 0 100% datadog_checks\\tls\\__init__.py 3 0 0 0 100% datadog_checks\\tls\\tls.py 185 4 50 2 97% datadog_checks\\tls\\utils.py 43 0 16 0 100% tests\\__init__.py 0 0 0 0 100% tests\\conftest.py 105 0 0 0 100% tests\\test_config.py 47 0 0 0 100% tests\\test_local.py 113 0 0 0 100% tests\\test_remote.py 189 0 2 0 100% tests\\test_utils.py 15 0 0 0 100% tests\\utils.py 36 0 2 0 100% ------------------------------------------------------------------- TOTAL 737 4 70 2 99% To also show any line numbers that were not hit, use the --cov-missing / -cm flag instead. $ ddev test tls -cm ... ---------- Coverage report ---------- Name Stmts Miss Branch BrPart Cover Missing ----------------------------------------------------------------------------- datadog_checks\\tls\\__about__.py 1 0 0 0 100% datadog_checks\\tls\\__init__.py 3 0 0 0 100% datadog_checks\\tls\\tls.py 185 4 50 2 97% 160-167, 288->275, 297->300, 300 datadog_checks\\tls\\utils.py 43 0 16 0 100% tests\\__init__.py 0 0 0 0 100% tests\\conftest.py 105 0 0 0 100% tests\\test_config.py 47 0 0 0 100% tests\\test_local.py 113 0 0 0 100% tests\\test_remote.py 189 0 2 0 100% tests\\test_utils.py 15 0 0 0 100% tests\\utils.py 36 0 2 0 100% ----------------------------------------------------------------------------- TOTAL 737 4 70 2 99% Style \u00b6 To run only the style checking environments, use the --style / -s shortcut flag. You may also only run the formatter environment using the --format-style / -fs shortcut flag. The formatter will automatically resolve the most common errors caught by the style checker. Advanced \u00b6 There are a number of shortcut options available that correspond to pytest options . --marker / -m ( pytest : -m ) - Only run tests matching a given marker expression e.g. ddev test elastic:py38-7.2 -m unit --filter / -k ( pytest : -k ) - Only run tests matching a given substring expression e.g. ddev test redisdb -k replication --debug / -d ( pytest : --log-level=debug -s ) - Set the log level to debug --pdb ( pytest : --pdb -x ) - Drop to PDB on first failure, then end test session --verbose / -v ( pytest : -v --tb=auto ) - Increase verbosity (can be used additively) and disables shortened tracebacks You may also pass arguments directly to pytest using the --pytest-args / -pa option. For example, you could re-write -d as -pa \"--log-level=debug -s\" .","title":"Testing"},{"location":"testing/#testing","text":"The entrypoint for testing any integration is the command ddev test , which accepts an arbitrary number of integrations as arguments. Under the hood, we use tox for environment management and pytest as our test framework.","title":"Testing"},{"location":"testing/#discovery","text":"Use the --list / -l flag to see what environments are available, for example: $ ddev test postgres envoy -l postgres: py27-10 py27-11 py27-93 py27-94 py27-95 py27-96 py38-10 py38-11 py38-93 py38-94 py38-95 py38-96 format_style style envoy: py27 py38 bench format_style style You'll notice that all environments for running tests are prefixed with pyXY , indicating the Python version to use. If you don't have a particular version installed (for example Python 2.7), such environments will be skipped. The second part of a test environment's name corresponds to the version of the product. For example, the 11 in py38-11 implies tests will run against version 11.x of PostgreSQL. If there is no version suffix, it means that either: the version is pinned, usually set to pull the latest release, or there is no concept of a product, such as the disk check","title":"Discovery"},{"location":"testing/#usage","text":"","title":"Usage"},{"location":"testing/#explicit","text":"Passing just the integration name will run every test environment e.g. executing ddev test envoy will run the environments py27 , py38 , and style . You may select a subset of environments to run by appending a : followed by a comma-separated list of environments. For example, executing: ddev test postgres:py38-11,style envoy:py38 will run, in order, the environments py38-11 and style for the PostgreSQL check and the environment py38 for the Envoy check.","title":"Explicit"},{"location":"testing/#detection","text":"If no integrations are specified then only integrations that were changed will be tested, based on a diff between the latest commit to the current and master branches. The criteria for an integration to be considered changed is based on the file extension of paths in the diff. So for example if only Markdown files were modified then nothing will be tested. The integrations will be tested in lexicographical order.","title":"Detection"},{"location":"testing/#coverage","text":"To measure code coverage, use the --cov / -c flag. Doing so will display a summary of coverage statistics after successful execution of integrations' tests. $ ddev test tls -c ... ---------- Coverage report ---------- Name Stmts Miss Branch BrPart Cover ------------------------------------------------------------------- datadog_checks\\tls\\__about__.py 1 0 0 0 100% datadog_checks\\tls\\__init__.py 3 0 0 0 100% datadog_checks\\tls\\tls.py 185 4 50 2 97% datadog_checks\\tls\\utils.py 43 0 16 0 100% tests\\__init__.py 0 0 0 0 100% tests\\conftest.py 105 0 0 0 100% tests\\test_config.py 47 0 0 0 100% tests\\test_local.py 113 0 0 0 100% tests\\test_remote.py 189 0 2 0 100% tests\\test_utils.py 15 0 0 0 100% tests\\utils.py 36 0 2 0 100% ------------------------------------------------------------------- TOTAL 737 4 70 2 99% To also show any line numbers that were not hit, use the --cov-missing / -cm flag instead. $ ddev test tls -cm ... ---------- Coverage report ---------- Name Stmts Miss Branch BrPart Cover Missing ----------------------------------------------------------------------------- datadog_checks\\tls\\__about__.py 1 0 0 0 100% datadog_checks\\tls\\__init__.py 3 0 0 0 100% datadog_checks\\tls\\tls.py 185 4 50 2 97% 160-167, 288->275, 297->300, 300 datadog_checks\\tls\\utils.py 43 0 16 0 100% tests\\__init__.py 0 0 0 0 100% tests\\conftest.py 105 0 0 0 100% tests\\test_config.py 47 0 0 0 100% tests\\test_local.py 113 0 0 0 100% tests\\test_remote.py 189 0 2 0 100% tests\\test_utils.py 15 0 0 0 100% tests\\utils.py 36 0 2 0 100% ----------------------------------------------------------------------------- TOTAL 737 4 70 2 99%","title":"Coverage"},{"location":"testing/#style","text":"To run only the style checking environments, use the --style / -s shortcut flag. You may also only run the formatter environment using the --format-style / -fs shortcut flag. The formatter will automatically resolve the most common errors caught by the style checker.","title":"Style"},{"location":"testing/#advanced","text":"There are a number of shortcut options available that correspond to pytest options . --marker / -m ( pytest : -m ) - Only run tests matching a given marker expression e.g. ddev test elastic:py38-7.2 -m unit --filter / -k ( pytest : -k ) - Only run tests matching a given substring expression e.g. ddev test redisdb -k replication --debug / -d ( pytest : --log-level=debug -s ) - Set the log level to debug --pdb ( pytest : --pdb -x ) - Drop to PDB on first failure, then end test session --verbose / -v ( pytest : -v --tb=auto ) - Increase verbosity (can be used additively) and disables shortened tracebacks You may also pass arguments directly to pytest using the --pytest-args / -pa option. For example, you could re-write -d as -pa \"--log-level=debug -s\" .","title":"Advanced"},{"location":"base/about/","text":"About \u00b6 The package datadog-checks-base provides all the functionality and utilities necessary for writing Agent Integrations. Most importantly it provides the AgentCheck base class from which every Check must be inherited. You would use it like so: from datadog_checks.base import AgentCheck class AwesomeCheck ( AgentCheck ): __NAMESPACE__ = 'awesome' def check ( self , instance ): self . gauge ( 'test' , 1.23 , tags = [ 'foo:bar' ]) The check method is what the Datadog Agent will execute. In this example we created a Check and gave it a namespace of awesome . This means that by default, every submission's name will be prefixed with awesome. . We submitted a gauge metric named awesome.test with a value of 1.23 tagged by foo:bar . The magic hidden by the usability of the API is that this actually calls a C binding which communicates with the Agent (written in Go).","title":"About"},{"location":"base/about/#about","text":"The package datadog-checks-base provides all the functionality and utilities necessary for writing Agent Integrations. Most importantly it provides the AgentCheck base class from which every Check must be inherited. You would use it like so: from datadog_checks.base import AgentCheck class AwesomeCheck ( AgentCheck ): __NAMESPACE__ = 'awesome' def check ( self , instance ): self . gauge ( 'test' , 1.23 , tags = [ 'foo:bar' ]) The check method is what the Datadog Agent will execute. In this example we created a Check and gave it a namespace of awesome . This means that by default, every submission's name will be prefixed with awesome. . We submitted a gauge metric named awesome.test with a value of 1.23 tagged by foo:bar . The magic hidden by the usability of the API is that this actually calls a C binding which communicates with the Agent (written in Go).","title":"About"},{"location":"base/api/","text":"API \u00b6 AgentCheck \u00b6 class datadog_checks.base. AgentCheck ( *args , **kwargs ) The base class for any Agent based integration. In general, you don't need to and you should not override anything from the base class except the check method but sometimes it might be useful for a Check to have its own constructor. When overriding __init__ you have to remember that, depending on the configuration, the Agent might create several different Check instances and the method would be called as many times. Agent 6,7 signature: AgentCheck(name, init_config, instances) # instances contain only 1 instance AgentCheck.check(instance) Agent 8 signature: AgentCheck(name, init_config, instance) # one instance AgentCheck.check() # no more instance argument for check method Note when loading a Custom check, the Agent will inspect the module searching for a subclass of AgentCheck . If such a class exists but has been derived in turn, it'll be ignored - you should never derive from an existing Check . name ( str ) - the name of the check init_config ( dict ) - the init_config section of the configuration. instance ( List[dict] ) - a one-element list containing the instance options from the configuration file (a list is used to keep backward compatibility with older versions of the Agent). gauge ( self , name , value , tags=None , hostname=None , device_name=None , raw=False ) Sample a gauge metric. Parameters: name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix count ( self , name , value , tags=None , hostname=None , device_name=None , raw=False ) Sample a raw count metric. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix monotonic_count ( self , name , value , tags=None , hostname=None , device_name=None , raw=False ) Sample an increasing counter metric. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix rate ( self , name , value , tags=None , hostname=None , device_name=None , raw=False ) Sample a point, with the rate calculated at the end of the check. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix histogram ( self , name , value , tags=None , hostname=None , device_name=None , raw=False ) Sample a histogram metric. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix historate ( self , name , value , tags=None , hostname=None , device_name=None , raw=False ) Sample a histogram based on rate metrics. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix service_check ( self , name , status , tags=None , hostname=None , message=None , raw=False ) Send the status of a service. name ( str ) - the name of the service check status ( int ) - a constant describing the service status. tags ( List[str]) ) - a list of tags to associate with this service check message ( str ) - additional information or a description of why this status occurred. raw ( bool ) - whether to ignore any defined namespace prefix event ( self , event ) Send an event. An event is a dictionary with the following keys and data types: { \"timestamp\" : int , # the epoch timestamp for the event \"event_type\" : str , # the event name \"api_key\" : str , # the api key for your account \"msg_title\" : str , # the title of the event \"msg_text\" : str , # the text body of the event \"aggregation_key\" : str , # a key to use for aggregating events \"alert_type\" : str , # (optional) one of ('error', 'warning', 'success', 'info'), defaults to 'info' \"source_type_name\" : str , # (optional) the source type name \"host\" : str , # (optional) the name of the host \"tags\" : list , # (optional) a list of tags to associate with this event \"priority\" : str , # (optional) specifies the priority of the event (\"normal\" or \"low\") } event ( dict ) - the event to be sent Stubs \u00b6 Aggregator \u00b6 class datadog_checks.base.stubs.aggregator. AggregatorStub ( ) Mainly used for unit testing checks, this stub makes possible to execute a check without a running Agent. assert_metric ( self , name , value=None , tags=None , count=None , at_least=1 , hostname=None , metric_type=None , device=None ) Assert a metric was processed by this stub assert_metric_has_tag ( self , metric_name , tag , count=None , at_least=1 ) Assert a metric is tagged with tag assert_metric_has_tag_prefix ( self , metric_name , tag_prefix , count=None , at_least=1 ) assert_service_check ( self , name , status=None , tags=None , count=None , at_least=1 , hostname=None , message=None ) Assert a service check was processed by this stub assert_event ( self , msg_text , count=None , at_least=1 , exact_match=True , tags=None , **kwargs ) assert_all_metrics_covered ( self ) assert_no_duplicate_metrics ( self ) Assert no duplicate metrics have been submitted. Metrics are considered duplicate when all following fields match: - metric name - type (gauge, rate, etc) - tags - hostname assert_no_duplicate_service_checks ( self ) Assert no duplicate service checks have been submitted. Service checks are considered duplicate when all following fields match: - metric name - status - tags - hostname assert_no_duplicate_all ( self ) Assert no duplicate metrics and service checks have been submitted. reset ( self ) Set the stub to its initial state Datadog Agent \u00b6 class datadog_checks.base.stubs.datadog_agent. DatadogAgentStub ( ) assert_metadata ( self , check_id , data ) assert_metadata_count ( self , count ) get_config ( self , *args , **kwargs ) get_hostname ( self ) get_version ( self ) log ( self , *args , **kwargs ) reset ( self ) set_check_metadata ( self , check_id , name , value ) set_external_tags ( self , *args , **kwargs ) tracemalloc_enabled ( self , *args , **kwargs )","title":"API"},{"location":"base/api/#api","text":"","title":"API"},{"location":"base/api/#agentcheck","text":"class datadog_checks.base. AgentCheck ( *args , **kwargs ) The base class for any Agent based integration. In general, you don't need to and you should not override anything from the base class except the check method but sometimes it might be useful for a Check to have its own constructor. When overriding __init__ you have to remember that, depending on the configuration, the Agent might create several different Check instances and the method would be called as many times. Agent 6,7 signature: AgentCheck(name, init_config, instances) # instances contain only 1 instance AgentCheck.check(instance) Agent 8 signature: AgentCheck(name, init_config, instance) # one instance AgentCheck.check() # no more instance argument for check method Note when loading a Custom check, the Agent will inspect the module searching for a subclass of AgentCheck . If such a class exists but has been derived in turn, it'll be ignored - you should never derive from an existing Check . name ( str ) - the name of the check init_config ( dict ) - the init_config section of the configuration. instance ( List[dict] ) - a one-element list containing the instance options from the configuration file (a list is used to keep backward compatibility with older versions of the Agent). gauge ( self , name , value , tags=None , hostname=None , device_name=None , raw=False ) Sample a gauge metric. Parameters: name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix count ( self , name , value , tags=None , hostname=None , device_name=None , raw=False ) Sample a raw count metric. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix monotonic_count ( self , name , value , tags=None , hostname=None , device_name=None , raw=False ) Sample an increasing counter metric. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix rate ( self , name , value , tags=None , hostname=None , device_name=None , raw=False ) Sample a point, with the rate calculated at the end of the check. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix histogram ( self , name , value , tags=None , hostname=None , device_name=None , raw=False ) Sample a histogram metric. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix historate ( self , name , value , tags=None , hostname=None , device_name=None , raw=False ) Sample a histogram based on rate metrics. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix service_check ( self , name , status , tags=None , hostname=None , message=None , raw=False ) Send the status of a service. name ( str ) - the name of the service check status ( int ) - a constant describing the service status. tags ( List[str]) ) - a list of tags to associate with this service check message ( str ) - additional information or a description of why this status occurred. raw ( bool ) - whether to ignore any defined namespace prefix event ( self , event ) Send an event. An event is a dictionary with the following keys and data types: { \"timestamp\" : int , # the epoch timestamp for the event \"event_type\" : str , # the event name \"api_key\" : str , # the api key for your account \"msg_title\" : str , # the title of the event \"msg_text\" : str , # the text body of the event \"aggregation_key\" : str , # a key to use for aggregating events \"alert_type\" : str , # (optional) one of ('error', 'warning', 'success', 'info'), defaults to 'info' \"source_type_name\" : str , # (optional) the source type name \"host\" : str , # (optional) the name of the host \"tags\" : list , # (optional) a list of tags to associate with this event \"priority\" : str , # (optional) specifies the priority of the event (\"normal\" or \"low\") } event ( dict ) - the event to be sent","title":"AgentCheck"},{"location":"base/api/#stubs","text":"","title":"Stubs"},{"location":"base/api/#aggregator","text":"class datadog_checks.base.stubs.aggregator. AggregatorStub ( ) Mainly used for unit testing checks, this stub makes possible to execute a check without a running Agent. assert_metric ( self , name , value=None , tags=None , count=None , at_least=1 , hostname=None , metric_type=None , device=None ) Assert a metric was processed by this stub assert_metric_has_tag ( self , metric_name , tag , count=None , at_least=1 ) Assert a metric is tagged with tag assert_metric_has_tag_prefix ( self , metric_name , tag_prefix , count=None , at_least=1 ) assert_service_check ( self , name , status=None , tags=None , count=None , at_least=1 , hostname=None , message=None ) Assert a service check was processed by this stub assert_event ( self , msg_text , count=None , at_least=1 , exact_match=True , tags=None , **kwargs ) assert_all_metrics_covered ( self ) assert_no_duplicate_metrics ( self ) Assert no duplicate metrics have been submitted. Metrics are considered duplicate when all following fields match: - metric name - type (gauge, rate, etc) - tags - hostname assert_no_duplicate_service_checks ( self ) Assert no duplicate service checks have been submitted. Service checks are considered duplicate when all following fields match: - metric name - status - tags - hostname assert_no_duplicate_all ( self ) Assert no duplicate metrics and service checks have been submitted. reset ( self ) Set the stub to its initial state","title":"Aggregator"},{"location":"base/api/#datadog-agent","text":"class datadog_checks.base.stubs.datadog_agent. DatadogAgentStub ( ) assert_metadata ( self , check_id , data ) assert_metadata_count ( self , count ) get_config ( self , *args , **kwargs ) get_hostname ( self ) get_version ( self ) log ( self , *args , **kwargs ) reset ( self ) set_check_metadata ( self , check_id , name , value ) set_external_tags ( self , *args , **kwargs ) tracemalloc_enabled ( self , *args , **kwargs )","title":"Datadog Agent"},{"location":"base/databases/","text":"Databases \u00b6 What is a database, you may wonder. Well, the answer to that question is fascinating ! No matter the database you wish to monitor, the base package provides a standard way to define and collect data from arbitrary queries. The core premise is that you define a function that accepts a query (usually a str ) and it returns a sequence of equal length results. Interface \u00b6 All the functionality is exposed by the Query and QueryManager classes. Query \u00b6 class datadog_checks.base.utils.db. Query ( query_data ) This class accepts a single dict argument which is the necessary data to run the query. The representation is based on our custom_queries format originally designed and implemented in !1528 . It is now part of all our database integrations and other products have since adopted this format. compile ( self , column_transformers , extra_transformers ) This idempotent method will be called by QueryManager.compile_queries so you should never need to call it directly. QueryManager \u00b6 class datadog_checks.base.utils.db. QueryManager ( check , executor , queries=None , tags=None , error_handler=None ) This class is in charge of running any number of Query instances for a single Check instance. You will most often see it created during Check initialization like this: self . _query_manager = QueryManager ( self , self . execute_query , queries = [ queries . SomeQuery1 , queries . SomeQuery2 , queries . SomeQuery3 , queries . SomeQuery4 , queries . SomeQuery5 , ], tags = self . instance . get ( 'tags' , []), error_handler = self . _error_sanitizer , ) self . check_initializations . append ( self . _query_manager . compile_queries ) check ( AgentCheck ) - an instance of a Check executor ( callable ) - a callable accepting a str query as its sole argument and returning a sequence representing either the full result set or an iterator over the result set queries ( List[Query]) ) - a list of Query instances tags ( List[str]) ) - a list of tags to associate with every submission error_handler ( callable ) - a callable accepting a str error as its sole argument and returning a sanitized string, useful for scrubbing potentially sensitive information libraries emit compile_queries ( self ) This method compiles every Query object. execute ( self ) This method is what you call every check run. execute_query ( self , query ) Called by execute , this triggers query execution to check for errors immediately in a way that is compatible with any library. If there are no errors, this is guaranteed to return an iterator over the result set. Transformers \u00b6 Column \u00b6 class datadog_checks.base.utils.db.transform. ColumnTransformers ( ) match This is used for querying unstructured data. For example, say you want to collect the fields named foo and bar . Typically, they would be stored like: | foo | bar | | --- | --- | | 4 | 2 | and would be queried like: SELECT foo , bar FROM ... Often, you will instead find data stored in the following format: | metric | value | | ------ | ----- | | foo | 4 | | bar | 2 | and would be queried like: SELECT metric , value FROM ... In this case, the metric column stores the name with which to match on and its value is stored in a separate column. The required items modifier is a mapping of matched names to column data values. Consider the values to be exactly the same as the entries in the columns top level field. You must also define a source modifier either for this transformer itself or in the values of items (which will take precedence). The source will be treated as the value of the match. Say this is your configuration: query : SELECT source1, source2, metric FROM TABLE columns : - name : value1 type : source - name : value2 type : source - name : metric_name type : match source : value1 items : foo : name : test.foo type : gauge source : value2 bar : name : test.bar type : monotonic_gauge and the result set is: | source1 | source2 | metric | | ------- | ------- | ------ | | 1 | 2 | foo | | 3 | 4 | baz | | 5 | 6 | bar | Here's what would be submitted: foo - test.foo as a gauge with a value of 2 bar - test.bar.total as a gauge and test.bar.count as a monotonic_count , both with a value of 5 baz - nothing since it was not defined as a match item monotonic_gauge Send the result as both a gauge suffixed by .total and a monotonic_count suffixed by .count . service_check Submit a service check. The required modifier status_map is a mapping of values to statuses. Valid statuses include: OK WARNING CRITICAL UNKNOWN Any encountered values that are not defined will be sent as UNKNOWN . tag Convert a column to a tag that will be used in every subsequent submission. For example, if you named the column env and the column returned the value prod1 , all submissions from that row will be tagged by env:prod1 . This also accepts an optional modifier called boolean that when set to true will transform the result to the string true or false . So for example if you named the column alive and the result was the number 0 the tag will be alive:false . temporal_percent Send the result as percentage of time since the last check run as a rate . For example, say the result is a forever increasing counter representing the total time spent pausing for garbage collection since start up. That number by itself is quite useless, but as a percentage of time spent pausing since the previous collection interval it becomes a useful metric. There is one required parameter called scale that indicates what unit of time the result should be considered. Valid values are: second millisecond microsecond nanosecond You may also define the unit as an integer number of parts compared to seconds e.g. millisecond is equivalent to 1000 . time_elapsed Send the number of seconds elapsed from a time in the past as a gauge . For example, if the result is an instance of datetime.datetime representing 5 seconds ago, then this would submit with a value of 5 . The optional modifier format indicates what format the result is in. By default it is native , assuming the underlying library provides timestamps as datetime objects. If it does not and passes them through directly as strings, you must provide the expected timestamp format using the supported codes . Note The code %z (lower case) is not supported on Windows. Extra \u00b6 Every column transformer (except tag ) is supported at this level, the only difference being one must set a source to retrieve the desired value. So for example here: columns : - name : foo.bar type : rate extras : - name : foo.current type : gauge source : foo.bar the metric foo.current will be sent as a gauge will the value of foo.bar . class datadog_checks.base.utils.db.transform. ExtraTransformers ( ) expression This allows the evaluation of a limited subset of Python syntax and built-in functions. columns : - name : disk.total type : gauge - name : disk.used type : gauge extras : - name : disk.free expression : disk.total - disk.used submit_type : gauge For brevity, if the expression attribute exists and type does not then it is assumed the type is expression . The submit_type can be any transformer and any extra options are passed down to it. The result of every expression is stored, so in lieu of a submit_type the above example could also be written as: columns : - name : disk.total type : gauge - name : disk.used type : gauge extras : - name : free expression : disk.total - disk.used - name : disk.free type : gauge source : free The order matters though, so for example the following will fail: columns : - name : disk.total type : gauge - name : disk.used type : gauge extras : - name : disk.free type : gauge source : free - name : free expression : disk.total - disk.used since the source free does not yet exist. percent Send a percentage based on 2 sources as a gauge . The required modifiers are part and total . For example, if you have this configuration: columns : - name : disk.total type : gauge - name : disk.used type : gauge extras : - name : disk.utilized type : percent part : disk.used total : disk.total then the extra metric disk.utilized would be sent as a gauge calculated as disk.used / disk.total * 100 . If the source of total is 0 , then the submitted value will always be sent as 0 too.","title":"Databases"},{"location":"base/databases/#databases","text":"What is a database, you may wonder. Well, the answer to that question is fascinating ! No matter the database you wish to monitor, the base package provides a standard way to define and collect data from arbitrary queries. The core premise is that you define a function that accepts a query (usually a str ) and it returns a sequence of equal length results.","title":"Databases"},{"location":"base/databases/#interface","text":"All the functionality is exposed by the Query and QueryManager classes.","title":"Interface"},{"location":"base/databases/#query","text":"class datadog_checks.base.utils.db. Query ( query_data ) This class accepts a single dict argument which is the necessary data to run the query. The representation is based on our custom_queries format originally designed and implemented in !1528 . It is now part of all our database integrations and other products have since adopted this format. compile ( self , column_transformers , extra_transformers ) This idempotent method will be called by QueryManager.compile_queries so you should never need to call it directly.","title":"Query"},{"location":"base/databases/#querymanager","text":"class datadog_checks.base.utils.db. QueryManager ( check , executor , queries=None , tags=None , error_handler=None ) This class is in charge of running any number of Query instances for a single Check instance. You will most often see it created during Check initialization like this: self . _query_manager = QueryManager ( self , self . execute_query , queries = [ queries . SomeQuery1 , queries . SomeQuery2 , queries . SomeQuery3 , queries . SomeQuery4 , queries . SomeQuery5 , ], tags = self . instance . get ( 'tags' , []), error_handler = self . _error_sanitizer , ) self . check_initializations . append ( self . _query_manager . compile_queries ) check ( AgentCheck ) - an instance of a Check executor ( callable ) - a callable accepting a str query as its sole argument and returning a sequence representing either the full result set or an iterator over the result set queries ( List[Query]) ) - a list of Query instances tags ( List[str]) ) - a list of tags to associate with every submission error_handler ( callable ) - a callable accepting a str error as its sole argument and returning a sanitized string, useful for scrubbing potentially sensitive information libraries emit compile_queries ( self ) This method compiles every Query object. execute ( self ) This method is what you call every check run. execute_query ( self , query ) Called by execute , this triggers query execution to check for errors immediately in a way that is compatible with any library. If there are no errors, this is guaranteed to return an iterator over the result set.","title":"QueryManager"},{"location":"base/databases/#transformers","text":"","title":"Transformers"},{"location":"base/databases/#column","text":"class datadog_checks.base.utils.db.transform. ColumnTransformers ( ) match This is used for querying unstructured data. For example, say you want to collect the fields named foo and bar . Typically, they would be stored like: | foo | bar | | --- | --- | | 4 | 2 | and would be queried like: SELECT foo , bar FROM ... Often, you will instead find data stored in the following format: | metric | value | | ------ | ----- | | foo | 4 | | bar | 2 | and would be queried like: SELECT metric , value FROM ... In this case, the metric column stores the name with which to match on and its value is stored in a separate column. The required items modifier is a mapping of matched names to column data values. Consider the values to be exactly the same as the entries in the columns top level field. You must also define a source modifier either for this transformer itself or in the values of items (which will take precedence). The source will be treated as the value of the match. Say this is your configuration: query : SELECT source1, source2, metric FROM TABLE columns : - name : value1 type : source - name : value2 type : source - name : metric_name type : match source : value1 items : foo : name : test.foo type : gauge source : value2 bar : name : test.bar type : monotonic_gauge and the result set is: | source1 | source2 | metric | | ------- | ------- | ------ | | 1 | 2 | foo | | 3 | 4 | baz | | 5 | 6 | bar | Here's what would be submitted: foo - test.foo as a gauge with a value of 2 bar - test.bar.total as a gauge and test.bar.count as a monotonic_count , both with a value of 5 baz - nothing since it was not defined as a match item monotonic_gauge Send the result as both a gauge suffixed by .total and a monotonic_count suffixed by .count . service_check Submit a service check. The required modifier status_map is a mapping of values to statuses. Valid statuses include: OK WARNING CRITICAL UNKNOWN Any encountered values that are not defined will be sent as UNKNOWN . tag Convert a column to a tag that will be used in every subsequent submission. For example, if you named the column env and the column returned the value prod1 , all submissions from that row will be tagged by env:prod1 . This also accepts an optional modifier called boolean that when set to true will transform the result to the string true or false . So for example if you named the column alive and the result was the number 0 the tag will be alive:false . temporal_percent Send the result as percentage of time since the last check run as a rate . For example, say the result is a forever increasing counter representing the total time spent pausing for garbage collection since start up. That number by itself is quite useless, but as a percentage of time spent pausing since the previous collection interval it becomes a useful metric. There is one required parameter called scale that indicates what unit of time the result should be considered. Valid values are: second millisecond microsecond nanosecond You may also define the unit as an integer number of parts compared to seconds e.g. millisecond is equivalent to 1000 . time_elapsed Send the number of seconds elapsed from a time in the past as a gauge . For example, if the result is an instance of datetime.datetime representing 5 seconds ago, then this would submit with a value of 5 . The optional modifier format indicates what format the result is in. By default it is native , assuming the underlying library provides timestamps as datetime objects. If it does not and passes them through directly as strings, you must provide the expected timestamp format using the supported codes . Note The code %z (lower case) is not supported on Windows.","title":"Column"},{"location":"base/databases/#extra","text":"Every column transformer (except tag ) is supported at this level, the only difference being one must set a source to retrieve the desired value. So for example here: columns : - name : foo.bar type : rate extras : - name : foo.current type : gauge source : foo.bar the metric foo.current will be sent as a gauge will the value of foo.bar . class datadog_checks.base.utils.db.transform. ExtraTransformers ( ) expression This allows the evaluation of a limited subset of Python syntax and built-in functions. columns : - name : disk.total type : gauge - name : disk.used type : gauge extras : - name : disk.free expression : disk.total - disk.used submit_type : gauge For brevity, if the expression attribute exists and type does not then it is assumed the type is expression . The submit_type can be any transformer and any extra options are passed down to it. The result of every expression is stored, so in lieu of a submit_type the above example could also be written as: columns : - name : disk.total type : gauge - name : disk.used type : gauge extras : - name : free expression : disk.total - disk.used - name : disk.free type : gauge source : free The order matters though, so for example the following will fail: columns : - name : disk.total type : gauge - name : disk.used type : gauge extras : - name : disk.free type : gauge source : free - name : free expression : disk.total - disk.used since the source free does not yet exist. percent Send a percentage based on 2 sources as a gauge . The required modifiers are part and total . For example, if you have this configuration: columns : - name : disk.total type : gauge - name : disk.used type : gauge extras : - name : disk.utilized type : percent part : disk.used total : disk.total then the extra metric disk.utilized would be sent as a gauge calculated as disk.used / disk.total * 100 . If the source of total is 0 , then the submitted value will always be sent as 0 too.","title":"Extra"},{"location":"base/http/","text":"HTTP \u00b6 Whenever you need to make HTTP requests, the base class provides a convenience member that has the same interface as the popular requests library and ensures consistent behavior across all integrations. The wrapper automatically parses and uses configuration from the instance , init_config , and Agent config. Also, this is only done once during initialization and cached to reduce the overhead of every call. All you have to do is e.g.: response = self . http . get ( url ) and the wrapper will pass the right things to requests . All methods accept optional keyword arguments like stream , etc. Any method-level option will override configuration. So for example if tls_verify was set to false and you do self.http.get(url, verify=True) , then SSL certificates will be verified on that particular request. You can use the keyword argument persist to override persist_connections . There is also support for non-standard or legacy configurations with the HTTP_CONFIG_REMAPPER class attribute. For example: class MyCheck ( AgentCheck ): HTTP_CONFIG_REMAPPER = { 'disable_ssl_validation' : { 'name' : 'tls_verify' , 'default' : False , 'invert' : True , }, ... } ... Options \u00b6 Some options can be set globally in init_config (with instances taking precedence). For complete documentation of every option, see the associated configuration templates: instances init_config class datadog_checks.base.utils.http. StandardFields ( ) auth_type aws_host aws_region aws_service connect_timeout extra_headers headers kerberos_auth kerberos_cache kerberos_delegate kerberos_force_initiate kerberos_hostname kerberos_keytab kerberos_principal log_requests ntlm_domain password persist_connections proxy read_timeout skip_proxy timeout tls_ca_cert tls_cert tls_ignore_warning tls_private_key tls_verify username Future \u00b6 Support for UNIX sockets Support for configuring cookies! Since they can be set globally, per-domain, and even per-path, the configuration may be complex if not thought out adequately. We'll discuss options for what that might look like. Only our spark and cisco_aci checks currently set cookies, and that is based on code logic, not configuration.","title":"HTTP"},{"location":"base/http/#http","text":"Whenever you need to make HTTP requests, the base class provides a convenience member that has the same interface as the popular requests library and ensures consistent behavior across all integrations. The wrapper automatically parses and uses configuration from the instance , init_config , and Agent config. Also, this is only done once during initialization and cached to reduce the overhead of every call. All you have to do is e.g.: response = self . http . get ( url ) and the wrapper will pass the right things to requests . All methods accept optional keyword arguments like stream , etc. Any method-level option will override configuration. So for example if tls_verify was set to false and you do self.http.get(url, verify=True) , then SSL certificates will be verified on that particular request. You can use the keyword argument persist to override persist_connections . There is also support for non-standard or legacy configurations with the HTTP_CONFIG_REMAPPER class attribute. For example: class MyCheck ( AgentCheck ): HTTP_CONFIG_REMAPPER = { 'disable_ssl_validation' : { 'name' : 'tls_verify' , 'default' : False , 'invert' : True , }, ... } ...","title":"HTTP"},{"location":"base/http/#options","text":"Some options can be set globally in init_config (with instances taking precedence). For complete documentation of every option, see the associated configuration templates: instances init_config class datadog_checks.base.utils.http. StandardFields ( ) auth_type aws_host aws_region aws_service connect_timeout extra_headers headers kerberos_auth kerberos_cache kerberos_delegate kerberos_force_initiate kerberos_hostname kerberos_keytab kerberos_principal log_requests ntlm_domain password persist_connections proxy read_timeout skip_proxy timeout tls_ca_cert tls_cert tls_ignore_warning tls_private_key tls_verify username","title":"Options"},{"location":"base/http/#future","text":"Support for UNIX sockets Support for configuring cookies! Since they can be set globally, per-domain, and even per-path, the configuration may be complex if not thought out adequately. We'll discuss options for what that might look like. Only our spark and cisco_aci checks currently set cookies, and that is based on code logic, not configuration.","title":"Future"},{"location":"ddev/cli/","text":"CLI \u00b6 DDEV \u00b6 Usage: ddev [OPTIONS] COMMAND [ARGS]... Options: \u00b6 -c, --core Work on integrations-core . -e, --extras Work on integrations-extras . -a, --agent Work on datadog-agent . -x, --here Work on the current location. --color / --no-color Whether or not to display colored output (default true). -q, --quiet Silence output -d, --debug Include debug output --version Show the version and exit. -h, --help Show this message and exit. COMMANDS: \u00b6 ### agent A collection of tasks related to the Datadog Agent Options: \u00b6 -h, --help Show this message and exit. Commands: \u00b6 changelog Provide a list of updated checks on a given Datadog Agent \u00b6 version, in changelog form `Usage: ddev agent changelog [OPTIONS]` Generates a markdown file containing the list of checks that changed for a given Agent release. Agent version numbers are derived inspecting tags on `integrations-core` so running this tool might provide unexpected results if the repo is not up to date with the Agent release process. If neither `--since` or `--to` are passed (the most common use case), the tool will generate the whole changelog since Agent version 6.3.0 (before that point we don't have enough information to build the log). ci Collection of CI utilities clean Remove a project's build artifacts config Manage the config file create Create scaffolding for a new integration dep Manage dependencies docs Manage documentation env Manage environments meta Collection of useful utilities release Manage the release of checks run Run commands in the proper repo test Run tests validate Verify certain aspects of the repo","title":"CLI"},{"location":"ddev/cli/#cli","text":"","title":"CLI"},{"location":"ddev/cli/#ddev","text":"Usage: ddev [OPTIONS] COMMAND [ARGS]...","title":"DDEV"},{"location":"ddev/cli/#options","text":"-c, --core Work on integrations-core . -e, --extras Work on integrations-extras . -a, --agent Work on datadog-agent . -x, --here Work on the current location. --color / --no-color Whether or not to display colored output (default true). -q, --quiet Silence output -d, --debug Include debug output --version Show the version and exit. -h, --help Show this message and exit.","title":"Options:"},{"location":"ddev/cli/#commands","text":"### agent A collection of tasks related to the Datadog Agent","title":"COMMANDS:"},{"location":"ddev/cli/#options_1","text":"-h, --help Show this message and exit.","title":"Options:"},{"location":"ddev/cli/#commands_1","text":"","title":"Commands:"},{"location":"ddev/cli/#changelog-provide-a-list-of-updated-checks-on-a-given-datadog-agent","text":"version, in changelog form `Usage: ddev agent changelog [OPTIONS]` Generates a markdown file containing the list of checks that changed for a given Agent release. Agent version numbers are derived inspecting tags on `integrations-core` so running this tool might provide unexpected results if the repo is not up to date with the Agent release process. If neither `--since` or `--to` are passed (the most common use case), the tool will generate the whole changelog since Agent version 6.3.0 (before that point we don't have enough information to build the log). ci Collection of CI utilities clean Remove a project's build artifacts config Manage the config file create Create scaffolding for a new integration dep Manage dependencies docs Manage documentation env Manage environments meta Collection of useful utilities release Manage the release of checks run Run commands in the proper repo test Run tests validate Verify certain aspects of the repo","title":"changelog     Provide a list of updated checks on a given Datadog Agent"},{"location":"ddev/configuration/","text":"Configuration \u00b6 All configuration can be managed entirely by the ddev config command group. To locate the TOML config file, run: ddev config find Repository \u00b6 All CLI commands are aware of the current repository context, defined by the option repo . This option should be a reference to another key which is set to the path of a supported repository. For example, this configuration: core = \"/path/to/integrations-core\" extras = \"/path/to/integrations-extras\" repo = \"core\" would make it so running e.g. ddev test nginx will look for an integration named nginx in /path/to/integrations-core no matter what directory you are in. If the selected path does not exist, then the current directory will be used. By default, repo is set to core . Agent \u00b6 Organization \u00b6 GitHub \u00b6 To avoid GitHub's public API rate limits, you need to set github.user / github.token in your config file or use the DD_GITHUB_USER / DD_GITHUB_TOKEN environment variables. Run ddev config show to see if your GitHub user and token is set. If not: Run ddev config set github.user <YOUR_GITHUB_USERNAME> Create a personal access token with public_repo permissions Run ddev config set github.token then paste the token Enable single sign-on for the token Jira \u00b6 To participate as an Agent release manager , you need to set jira.user / jira.token in your config file. Run ddev config show to see if your Jira user and token is set. If not: Run ddev config set jira.user <YOUR_DATADOG_EMAIL> Create an API token Run ddev config set jira.token then paste the token","title":"Configuration"},{"location":"ddev/configuration/#configuration","text":"All configuration can be managed entirely by the ddev config command group. To locate the TOML config file, run: ddev config find","title":"Configuration"},{"location":"ddev/configuration/#repository","text":"All CLI commands are aware of the current repository context, defined by the option repo . This option should be a reference to another key which is set to the path of a supported repository. For example, this configuration: core = \"/path/to/integrations-core\" extras = \"/path/to/integrations-extras\" repo = \"core\" would make it so running e.g. ddev test nginx will look for an integration named nginx in /path/to/integrations-core no matter what directory you are in. If the selected path does not exist, then the current directory will be used. By default, repo is set to core .","title":"Repository"},{"location":"ddev/configuration/#agent","text":"","title":"Agent"},{"location":"ddev/configuration/#organization","text":"","title":"Organization"},{"location":"ddev/configuration/#github","text":"To avoid GitHub's public API rate limits, you need to set github.user / github.token in your config file or use the DD_GITHUB_USER / DD_GITHUB_TOKEN environment variables. Run ddev config show to see if your GitHub user and token is set. If not: Run ddev config set github.user <YOUR_GITHUB_USERNAME> Create a personal access token with public_repo permissions Run ddev config set github.token then paste the token Enable single sign-on for the token","title":"GitHub"},{"location":"ddev/configuration/#jira","text":"To participate as an Agent release manager , you need to set jira.user / jira.token in your config file. Run ddev config show to see if your Jira user and token is set. If not: Run ddev config set jira.user <YOUR_DATADOG_EMAIL> Create an API token Run ddev config set jira.token then paste the token","title":"Jira"},{"location":"ddev/layers/","text":"What's in the box? \u00b6 The Dev package, often referred to as its CLI entrypoint ddev , is fundamentally split into 2 parts. Test framework \u00b6 The test framework provides everything necessary to test integrations, such as: Dependencies like pytest , mock , requests , etc. Utilities for consistently handling complex logic or common operations An orchestrator for arbitrary E2E environments CLI \u00b6 The CLI provides the interface through which tests are invoked, E2E environments are managed, and general repository maintenance (such as dependency management) occurs. Separation \u00b6 As the dependencies of the test framework are a subset of what is required for the CLI, the CLI tooling may import from the test framework located at the root , but not vice versa.","title":"What's in the box?"},{"location":"ddev/layers/#whats-in-the-box","text":"The Dev package, often referred to as its CLI entrypoint ddev , is fundamentally split into 2 parts.","title":"What's in the box?"},{"location":"ddev/layers/#test-framework","text":"The test framework provides everything necessary to test integrations, such as: Dependencies like pytest , mock , requests , etc. Utilities for consistently handling complex logic or common operations An orchestrator for arbitrary E2E environments","title":"Test framework"},{"location":"ddev/layers/#cli","text":"The CLI provides the interface through which tests are invoked, E2E environments are managed, and general repository maintenance (such as dependency management) occurs.","title":"CLI"},{"location":"ddev/layers/#separation","text":"As the dependencies of the test framework are a subset of what is required for the CLI, the CLI tooling may import from the test framework located at the root , but not vice versa.","title":"Separation"},{"location":"ddev/plugins/","text":"Plugins \u00b6 tox \u00b6 pytest \u00b6 Environment manager \u00b6 Agent check runner \u00b6","title":"Plugins"},{"location":"ddev/plugins/#plugins","text":"","title":"Plugins"},{"location":"ddev/plugins/#tox","text":"","title":"tox"},{"location":"ddev/plugins/#pytest","text":"","title":"pytest"},{"location":"ddev/plugins/#environment-manager","text":"","title":"Environment manager"},{"location":"ddev/plugins/#agent-check-runner","text":"","title":"Agent check runner"},{"location":"ddev/test/","text":"Test framework \u00b6","title":"Test framework"},{"location":"ddev/test/#test-framework","text":"","title":"Test framework"},{"location":"faq/faq/","text":"FAQ \u00b6 Integration vs Check \u00b6 A Check is any integration whose execution is triggered directly in code by the Datadog Agent . Therefore, all Agent-based integrations written in Python or Go are considered Checks. Why test tests \u00b6 We track the coverage of tests in all cases. See pyca/pynacl !290 and !4280 .","title":"FAQ"},{"location":"faq/faq/#faq","text":"","title":"FAQ"},{"location":"faq/faq/#integration-vs-check","text":"A Check is any integration whose execution is triggered directly in code by the Datadog Agent . Therefore, all Agent-based integrations written in Python or Go are considered Checks.","title":"Integration vs Check"},{"location":"faq/faq/#why-test-tests","text":"We track the coverage of tests in all cases. See pyca/pynacl !290 and !4280 .","title":"Why test tests"},{"location":"guidelines/pr/","text":"Pull requests \u00b6","title":"Pull requests"},{"location":"guidelines/pr/#pull-requests","text":"","title":"Pull requests"},{"location":"guidelines/style/","text":"Style \u00b6 These are all the checkers used by our style enforcement. black \u00b6 An opinionated formatter, like JavaScript's prettier and Golang's gofmt . isort \u00b6 A tool to sort imports lexicographically, by section, and by type. We use the 5 standard sections: __future__ , stdlib, third party, first party, and local. datadog_checks is configured as a first party namespace. flake8 \u00b6 An easy-to-use wrapper around pycodestyle and pyflakes . We select everything it provides and only ignore a few things to give precedence to other tools. bugbear \u00b6 A flake8 plugin for finding likely bugs and design problems in programs. We enable: B001 : Do not use bare except: , it also catches unexpected events like memory errors, interrupts, system exit, and so on. Prefer except Exception: . B003 : Assigning to os.environ doesn't clear the environment. Subprocesses are going to see outdated variables, in disagreement with the current process. Use os.environ.clear() or the env= argument to Popen. B006 : Do not use mutable data structures for argument defaults. All calls reuse one instance of that data structure, persisting changes between them. B007 : Loop control variable not used within the loop body. If this is intended, start the name with an underscore. B301 : Python 3 does not include .iter* methods on dictionaries. The default behavior is to return iterables. Simply remove the iter prefix from the method. For Python 2 compatibility, also prefer the Python 3 equivalent if you expect that the size of the dict to be small and bounded. The performance regression on Python 2 will be negligible and the code is going to be the clearest. Alternatively, use six.iter* . B305 : .next() is not a thing on Python 3. Use the next() builtin. For Python 2 compatibility, use six.next() . B306 : BaseException.message has been deprecated as of Python 2.6 and is removed in Python 3. Use str(e) to access the user-readable message. Use e.args to access arguments passed to the exception. B902 : Invalid first argument used for method. Use self for instance methods, and cls for class methods. logging-format \u00b6 A flake8 plugin for ensuring a consistent logging format. We enable: G001 : Logging statements should not use string.format() for their first argument G002 : Logging statements should not use % formatting for their first argument G003 : Logging statements should not use + concatenation for their first argument G004 : Logging statements should not use f\"...\" for their first argument (only in Python 3.6+) G010 : Logging statements should not use warn (use warning instead) G100 : Logging statements should not use extra arguments unless whitelisted G201 : Logging statements should not use error(..., exc_info=True) (use exception(...) instead) G202 : Logging statements should not use redundant exc_info=True in exception Mypy \u00b6 A type checker allowing a mix of dynamic and static typing. This is optional for now.","title":"Style"},{"location":"guidelines/style/#style","text":"These are all the checkers used by our style enforcement.","title":"Style"},{"location":"guidelines/style/#black","text":"An opinionated formatter, like JavaScript's prettier and Golang's gofmt .","title":"black"},{"location":"guidelines/style/#isort","text":"A tool to sort imports lexicographically, by section, and by type. We use the 5 standard sections: __future__ , stdlib, third party, first party, and local. datadog_checks is configured as a first party namespace.","title":"isort"},{"location":"guidelines/style/#flake8","text":"An easy-to-use wrapper around pycodestyle and pyflakes . We select everything it provides and only ignore a few things to give precedence to other tools.","title":"flake8"},{"location":"guidelines/style/#bugbear","text":"A flake8 plugin for finding likely bugs and design problems in programs. We enable: B001 : Do not use bare except: , it also catches unexpected events like memory errors, interrupts, system exit, and so on. Prefer except Exception: . B003 : Assigning to os.environ doesn't clear the environment. Subprocesses are going to see outdated variables, in disagreement with the current process. Use os.environ.clear() or the env= argument to Popen. B006 : Do not use mutable data structures for argument defaults. All calls reuse one instance of that data structure, persisting changes between them. B007 : Loop control variable not used within the loop body. If this is intended, start the name with an underscore. B301 : Python 3 does not include .iter* methods on dictionaries. The default behavior is to return iterables. Simply remove the iter prefix from the method. For Python 2 compatibility, also prefer the Python 3 equivalent if you expect that the size of the dict to be small and bounded. The performance regression on Python 2 will be negligible and the code is going to be the clearest. Alternatively, use six.iter* . B305 : .next() is not a thing on Python 3. Use the next() builtin. For Python 2 compatibility, use six.next() . B306 : BaseException.message has been deprecated as of Python 2.6 and is removed in Python 3. Use str(e) to access the user-readable message. Use e.args to access arguments passed to the exception. B902 : Invalid first argument used for method. Use self for instance methods, and cls for class methods.","title":"bugbear"},{"location":"guidelines/style/#logging-format","text":"A flake8 plugin for ensuring a consistent logging format. We enable: G001 : Logging statements should not use string.format() for their first argument G002 : Logging statements should not use % formatting for their first argument G003 : Logging statements should not use + concatenation for their first argument G004 : Logging statements should not use f\"...\" for their first argument (only in Python 3.6+) G010 : Logging statements should not use warn (use warning instead) G100 : Logging statements should not use extra arguments unless whitelisted G201 : Logging statements should not use error(..., exc_info=True) (use exception(...) instead) G202 : Logging statements should not use redundant exc_info=True in exception","title":"logging-format"},{"location":"guidelines/style/#mypy","text":"A type checker allowing a mix of dynamic and static typing. This is optional for now.","title":"Mypy"},{"location":"meta/cd/","text":"CD \u00b6","title":"CD"},{"location":"meta/cd/#cd","text":"","title":"CD"},{"location":"meta/ci/","text":"CI \u00b6","title":"CI"},{"location":"meta/ci/#ci","text":"","title":"CI"},{"location":"meta/config_specs/","text":"Configuration \u00b6","title":"Config specs"},{"location":"meta/config_specs/#configuration","text":"","title":"Configuration"},{"location":"process/agent_release/","text":"Agent release \u00b6 A new minor version of the Agent is released every 6 weeks (approximately). Each release ships a snapshot of integrations-core . Setup \u00b6 Configure your GitHub and Jira auth. Freeze \u00b6 At midnight on the Friday before QA week we freeze, at which point the release manager will release all integrations with pending changes then branch off. Release \u00b6 Make a pull request to release any new integrations , then merge it and pull master Make a pull request to release all changed integrations , then merge it and pull master Branch \u00b6 Create a branch based on master named after the highest version of the Agent being released in the form <MAJOR>.<MINOR>.x Push the branch to GitHub Tag \u00b6 Run: git tag <MAJOR>.<MINOR>.0-rc.1 -m <MAJOR>.<MINOR>.0-rc.1 git push origin <MAJOR>.<MINOR>.0-rc.1 QA week \u00b6 We test all changes to integrations that were introduced since the last release. Create items \u00b6 Create an item for every change in our board using the ddev release testable command. For example: ddev release testable 7.17.1 7.18.0-rc.1 would select all commits that were merged between the Git references. The command will display each change and prompt you to assign a team or skip. Purely documentation changes are automatically skipped. You must assign each item to a team member after creation and ensure no one is assigned to a change that they authored. If you would like to automate this, then add a jira_users_$team table in your configuration , with keys being GitHub usernames and values being their corresponding Jira IDs (not names). You can find current team member information in this document . Release candidates \u00b6 The main Agent release manager will increment and build a new rc every day a bug fix needs to be tested until all QA is complete. Before each build is triggered: Merge any fixes that have been approved, then pull master Release all changed integrations with the exception of datadog_checks_dev For each fix merged, you must cherry-pick to the branch : The commit to master itself The release commit, so the shipped versions match the individually released integrations After all fixes have been cherry-picked: Push the changes to GitHub Tag with the appropriate rc number even if there were no changes Communication \u00b6 Update the #agent-release-sync channel with a daily status. #agent-integrations status: - TODO: X - In progress: X - Issues found: X - Awaiting build: X Release week \u00b6 After QA week ends the code freeze is lifted, even if there are items yet to be tested. The release manager will continue the same process outlined above. Finalize \u00b6 On the day of the final stable release, tag the branch with <MAJOR>.<MINOR>.0 . After the main Agent release manager confirms successful deployment to a few targets, create a branch based on master and run: ddev agent changelog ddev agent integrations Create a pull request and wait for approval before merging.","title":"Agent release"},{"location":"process/agent_release/#agent-release","text":"A new minor version of the Agent is released every 6 weeks (approximately). Each release ships a snapshot of integrations-core .","title":"Agent release"},{"location":"process/agent_release/#setup","text":"Configure your GitHub and Jira auth.","title":"Setup"},{"location":"process/agent_release/#freeze","text":"At midnight on the Friday before QA week we freeze, at which point the release manager will release all integrations with pending changes then branch off.","title":"Freeze"},{"location":"process/agent_release/#release","text":"Make a pull request to release any new integrations , then merge it and pull master Make a pull request to release all changed integrations , then merge it and pull master","title":"Release"},{"location":"process/agent_release/#branch","text":"Create a branch based on master named after the highest version of the Agent being released in the form <MAJOR>.<MINOR>.x Push the branch to GitHub","title":"Branch"},{"location":"process/agent_release/#tag","text":"Run: git tag <MAJOR>.<MINOR>.0-rc.1 -m <MAJOR>.<MINOR>.0-rc.1 git push origin <MAJOR>.<MINOR>.0-rc.1","title":"Tag"},{"location":"process/agent_release/#qa-week","text":"We test all changes to integrations that were introduced since the last release.","title":"QA week"},{"location":"process/agent_release/#create-items","text":"Create an item for every change in our board using the ddev release testable command. For example: ddev release testable 7.17.1 7.18.0-rc.1 would select all commits that were merged between the Git references. The command will display each change and prompt you to assign a team or skip. Purely documentation changes are automatically skipped. You must assign each item to a team member after creation and ensure no one is assigned to a change that they authored. If you would like to automate this, then add a jira_users_$team table in your configuration , with keys being GitHub usernames and values being their corresponding Jira IDs (not names). You can find current team member information in this document .","title":"Create items"},{"location":"process/agent_release/#release-candidates","text":"The main Agent release manager will increment and build a new rc every day a bug fix needs to be tested until all QA is complete. Before each build is triggered: Merge any fixes that have been approved, then pull master Release all changed integrations with the exception of datadog_checks_dev For each fix merged, you must cherry-pick to the branch : The commit to master itself The release commit, so the shipped versions match the individually released integrations After all fixes have been cherry-picked: Push the changes to GitHub Tag with the appropriate rc number even if there were no changes","title":"Release candidates"},{"location":"process/agent_release/#communication","text":"Update the #agent-release-sync channel with a daily status. #agent-integrations status: - TODO: X - In progress: X - Issues found: X - Awaiting build: X","title":"Communication"},{"location":"process/agent_release/#release-week","text":"After QA week ends the code freeze is lifted, even if there are items yet to be tested. The release manager will continue the same process outlined above.","title":"Release week"},{"location":"process/agent_release/#finalize","text":"On the day of the final stable release, tag the branch with <MAJOR>.<MINOR>.0 . After the main Agent release manager confirms successful deployment to a few targets, create a branch based on master and run: ddev agent changelog ddev agent integrations Create a pull request and wait for approval before merging.","title":"Finalize"},{"location":"process/integration_release/","text":"Integration release \u00b6 Each Agent integration has its own release cycle. Many integrations are actively developed and released often while some are rarely touched (usually indicating feature-completeness). Versioning \u00b6 All releases adhere to Semantic Versioning . Tags in the form <INTEGRATION_NAME>-<VERSION> are added to the Git repository. Therefore, it's possible to checkout and build the code for a certain version of a specific check. Setup \u00b6 Configure your GitHub auth. Identify changes \u00b6 Note If you already know which integration you'd like to release, skip this section. To see all checks that need to be released, run ddev release show ready . Steps \u00b6 Checkout and pull the most recent version of the master branch. git checkout master git pull Important Not using the latest version of master may cause errors in the build pipeline . Review which PRs were merged in between the latest release and the master branch. ddev release show changes <INTEGRATION> You should ensure that PR titles and changelog labels are correct. Create a release branch from master (suggested naming format is <USERNAME>/release-<INTEGRATION_NAME> ). This has the purpose of opening a PR so others can review the changelog. Important It is critical the branch name is not in the form <USERNAME>/<INTEGRATION_NAME>-<NEW_VERSION> because one of our Gitlab jobs is triggered whenever a Git reference matches that pattern, see !3843 & !3980 . Make the release. ddev release make <INTEGRATION> You may need to touch your Yubikey multiple times. This will automatically: update the version in <INTEGRATION>/datadog_checks/<INTEGRATION>/__about__.py update the changelog update the requirements-agent-release.txt file update in-toto metadata commit the above changes Push your branch to GitHub and create a pull request. Update the title of the PR to something like [Release] Bumped <INTEGRATION> version to <VERSION> . Ask for a review in Slack. Merge the pull request after approval. PyPI \u00b6 If you released datadog_checks_base or datadog_checks_dev then you will need to upload to PyPI for use by integrations-extras . ddev release upload datadog_checks_[base|dev] Metadata \u00b6 You need to run certain jobs if any changes modified integration metadata. See the Declarative Integration Pipeline wiki. Bulk releases \u00b6 To create a release for every integration that has changed, use all as the integration name in the ddev release make step above. ddev release make all You may also pass a comma-separated list of checks to skip using the --exclude option, e.g.: ddev release make all --exclude datadog_checks_dev Betas \u00b6 Creating pre-releases is the same workflow except you do not open a pull request but rather release directly from a branch. In the ddev release make step set --version to [major|minor|patch],[rc|alpha|beta] . For example, if the current version of an integration is 1.1.3 , the following command will bump it to 1.2.0-rc.1 : ddev release make <INTEGRATION> --version minor,rc After pushing the release commits to GitHub, run: ddev release tag <INTEGRATION> This manually triggers the build pipeline . To increment the version, omit the first part, e.g.: ddev release make <INTEGRATION> --version rc New integrations \u00b6 To bump a new integration to 1.0.0 if it is not already there, run: ddev release make <INTEGRATION> --new To ensure this for all integrations, run: ddev release make all --new If a release was created, run: ddev agent requirements Troubleshooting \u00b6 If you encounter errors when signing with your Yubikey, ensure you ran gpg --import <YOUR_KEY_ID>.gpg.pub . Releasers \u00b6 For whom it may concern, the following is a list of GPG public key fingerprints known to correspond to developers who, at the time of writing (28-02-2020), can trigger a build by signing in-toto metadata . Christine Chen 57CE 2495 EA48 D456 B9C4 BA4F 66E8 2239 9141 D9D3 36C0 82E7 38C7 B4A1 E169 11C0 D633 59C4 875A 1A9A Dave Coleman 8278 C406 C1BB F1F2 DFBB 5AD6 0AE7 E246 4F8F D375 98A5 37CD CCA2 8DFF B35B 0551 5D50 0742 90F6 422F Mike Garabedian F90C 0097 67F2 4B27 9DC2 C83D A227 6601 6CB4 CF1D 2669 6E67 28D2 0CB0 C1E0 D2BE 6643 5756 8398 9306 Thomas Herv\u00e9 59DB 2532 75A5 BD4E 55C7 C5AA 0678 55A2 8E90 3B3B E2BD 994F 95C0 BC0B B923 1D21 F752 1EC8 F485 90D0 Ofek Lev C295 CF63 B355 DFEB 3316 02F7 F426 A944 35BE 6F99 D009 8861 8057 D2F4 D855 5A62 B472 442C B7D3 AF42 Florimond Manca B023 B02A 0331 9CD8 D19A 4328 83ED 89A4 5548 48FC 0992 11D9 AA67 D21E 7098 7B59 7C7D CB06 C9F2 0C13 Julia Simon 4A54 09A2 3361 109C 047C C76A DC8A 42C2 8B95 0123 129A 26CF A726 3C85 98A6 94B0 8659 1366 CBA1 BF3C Florian Veaux 3109 1C85 5D78 7789 93E5 0348 9BFE 5299 D02F 83E9 7A73 0C5E 48B0 6986 1045 CF8B 8B2D 16D6 5DE4 C95E Alexandre Yang FBC6 3AE0 9D0C A9B4 584C 9D7F 4291 A11A 36EA 52CD F8D9 181D 9309 F8A4 957D 636A 27F8 F48B 18AE 91AA","title":"Integration release"},{"location":"process/integration_release/#integration-release","text":"Each Agent integration has its own release cycle. Many integrations are actively developed and released often while some are rarely touched (usually indicating feature-completeness).","title":"Integration release"},{"location":"process/integration_release/#versioning","text":"All releases adhere to Semantic Versioning . Tags in the form <INTEGRATION_NAME>-<VERSION> are added to the Git repository. Therefore, it's possible to checkout and build the code for a certain version of a specific check.","title":"Versioning"},{"location":"process/integration_release/#setup","text":"Configure your GitHub auth.","title":"Setup"},{"location":"process/integration_release/#identify-changes","text":"Note If you already know which integration you'd like to release, skip this section. To see all checks that need to be released, run ddev release show ready .","title":"Identify changes"},{"location":"process/integration_release/#steps","text":"Checkout and pull the most recent version of the master branch. git checkout master git pull Important Not using the latest version of master may cause errors in the build pipeline . Review which PRs were merged in between the latest release and the master branch. ddev release show changes <INTEGRATION> You should ensure that PR titles and changelog labels are correct. Create a release branch from master (suggested naming format is <USERNAME>/release-<INTEGRATION_NAME> ). This has the purpose of opening a PR so others can review the changelog. Important It is critical the branch name is not in the form <USERNAME>/<INTEGRATION_NAME>-<NEW_VERSION> because one of our Gitlab jobs is triggered whenever a Git reference matches that pattern, see !3843 & !3980 . Make the release. ddev release make <INTEGRATION> You may need to touch your Yubikey multiple times. This will automatically: update the version in <INTEGRATION>/datadog_checks/<INTEGRATION>/__about__.py update the changelog update the requirements-agent-release.txt file update in-toto metadata commit the above changes Push your branch to GitHub and create a pull request. Update the title of the PR to something like [Release] Bumped <INTEGRATION> version to <VERSION> . Ask for a review in Slack. Merge the pull request after approval.","title":"Steps"},{"location":"process/integration_release/#pypi","text":"If you released datadog_checks_base or datadog_checks_dev then you will need to upload to PyPI for use by integrations-extras . ddev release upload datadog_checks_[base|dev]","title":"PyPI"},{"location":"process/integration_release/#metadata","text":"You need to run certain jobs if any changes modified integration metadata. See the Declarative Integration Pipeline wiki.","title":"Metadata"},{"location":"process/integration_release/#bulk-releases","text":"To create a release for every integration that has changed, use all as the integration name in the ddev release make step above. ddev release make all You may also pass a comma-separated list of checks to skip using the --exclude option, e.g.: ddev release make all --exclude datadog_checks_dev","title":"Bulk releases"},{"location":"process/integration_release/#betas","text":"Creating pre-releases is the same workflow except you do not open a pull request but rather release directly from a branch. In the ddev release make step set --version to [major|minor|patch],[rc|alpha|beta] . For example, if the current version of an integration is 1.1.3 , the following command will bump it to 1.2.0-rc.1 : ddev release make <INTEGRATION> --version minor,rc After pushing the release commits to GitHub, run: ddev release tag <INTEGRATION> This manually triggers the build pipeline . To increment the version, omit the first part, e.g.: ddev release make <INTEGRATION> --version rc","title":"Betas"},{"location":"process/integration_release/#new-integrations","text":"To bump a new integration to 1.0.0 if it is not already there, run: ddev release make <INTEGRATION> --new To ensure this for all integrations, run: ddev release make all --new If a release was created, run: ddev agent requirements","title":"New integrations"},{"location":"process/integration_release/#troubleshooting","text":"If you encounter errors when signing with your Yubikey, ensure you ran gpg --import <YOUR_KEY_ID>.gpg.pub .","title":"Troubleshooting"},{"location":"process/integration_release/#releasers","text":"For whom it may concern, the following is a list of GPG public key fingerprints known to correspond to developers who, at the time of writing (28-02-2020), can trigger a build by signing in-toto metadata . Christine Chen 57CE 2495 EA48 D456 B9C4 BA4F 66E8 2239 9141 D9D3 36C0 82E7 38C7 B4A1 E169 11C0 D633 59C4 875A 1A9A Dave Coleman 8278 C406 C1BB F1F2 DFBB 5AD6 0AE7 E246 4F8F D375 98A5 37CD CCA2 8DFF B35B 0551 5D50 0742 90F6 422F Mike Garabedian F90C 0097 67F2 4B27 9DC2 C83D A227 6601 6CB4 CF1D 2669 6E67 28D2 0CB0 C1E0 D2BE 6643 5756 8398 9306 Thomas Herv\u00e9 59DB 2532 75A5 BD4E 55C7 C5AA 0678 55A2 8E90 3B3B E2BD 994F 95C0 BC0B B923 1D21 F752 1EC8 F485 90D0 Ofek Lev C295 CF63 B355 DFEB 3316 02F7 F426 A944 35BE 6F99 D009 8861 8057 D2F4 D855 5A62 B472 442C B7D3 AF42 Florimond Manca B023 B02A 0331 9CD8 D19A 4328 83ED 89A4 5548 48FC 0992 11D9 AA67 D21E 7098 7B59 7C7D CB06 C9F2 0C13 Julia Simon 4A54 09A2 3361 109C 047C C76A DC8A 42C2 8B95 0123 129A 26CF A726 3C85 98A6 94B0 8659 1366 CBA1 BF3C Florian Veaux 3109 1C85 5D78 7789 93E5 0348 9BFE 5299 D02F 83E9 7A73 0C5E 48B0 6986 1045 CF8B 8B2D 16D6 5DE4 C95E Alexandre Yang FBC6 3AE0 9D0C A9B4 584C 9D7F 4291 A11A 36EA 52CD F8D9 181D 9309 F8A4 957D 636A 27F8 F48B 18AE 91AA","title":"Releasers"}]}