metric_name,metric_type,interval,unit_name,per_unit_name,description,orientation,integration,short_name,curated_metric,sample_tags
litellm.api.key.budget.remaining_hours.metric,gauge,,hour,,Remaining hours for api key budget to be reset,0,litellm,,,
litellm.api.key.max_budget.metric,gauge,,dollar,,Maximum budget set for api key,0,litellm,,,
litellm.auth.failed_requests.count,count,,error,,Total failed_requests for auth service,0,litellm,,,
litellm.auth.latency.bucket,count,,,,Number of observations that fall into each upper_bound latency bucket for auth service,0,litellm,,,
litellm.auth.latency.count,count,,,,Total number of latency observations for auth service.,0,litellm,,,
litellm.auth.latency.sum,count,,millisecond,,Latency for auth service,0,litellm,,,
litellm.auth.total_requests.count,count,,request,,Total total_requests for auth service,0,litellm,,,
litellm.batch_write_to_db.failed_requests.count,count,,error,,Total failed_requests for batch_write_to_db service,0,litellm,,,
litellm.batch_write_to_db.latency.bucket,count,,,,Number of observations that fall into each upper_bound latency bucket for batch_write_to_db service,0,litellm,,,
litellm.batch_write_to_db.latency.count,count,,,,Total number of latency observations for batch_write_to_db service.,0,litellm,,,
litellm.batch_write_to_db.latency.sum,count,,millisecond,,Latency for batch_write_to_db service,0,litellm,,,
litellm.batch_write_to_db.total_requests.count,count,,request,,Total total_requests for batch_write_to_db service,0,litellm,,,
litellm.deployment.cooled_down.count,count,,event,,Number of times a deployment has been cooled down by LiteLLM load balancing logic. exception_status is the status of the exception that caused the deployment to be cooled down,0,litellm,,,
litellm.deployment.failed_fallbacks.count,count,,error,,Number of failed fallback requests from primary model -> fallback model,0,litellm,,,
litellm.deployment.failure_by_tag_responses.count,count,,error,,Total number of failed LLM API calls for a specific LLM deploymeny by custom metadata tags,0,litellm,,,
litellm.deployment.failure_responses.count,count,,error,,Total number of failed LLM API calls for a specific LLM deploymeny. exception_status is the status of the exception from the llm api,0,litellm,,,
litellm.deployment.latency_per_output_token.bucket,count,,,,Number of observations that fall into each upper_bound latency per output token bucket for deployment,0,litellm,,,
litellm.deployment.latency_per_output_token.count,count,,,,Total number of latency per output token observations for deployment.,0,litellm,,,
litellm.deployment.latency_per_output_token.sum,count,,millisecond,,Latency per output token,0,litellm,,,
litellm.deployment.state,gauge,,unit,,"The state of the deployment: 0 = healthy,1 = partial outage,2 = complete outage",0,litellm,,,
litellm.deployment.success_responses.count,count,,response,,Total number of successful LLM API calls via litellm,0,litellm,,,
litellm.deployment.successful_fallbacks.count,count,,response,,Number of successful fallback requests from primary model -> fallback model,0,litellm,,,
litellm.deployment.total_requests.count,count,,request,,Total number of LLM API calls via litellm - success + failure,0,litellm,,,
litellm.in_memory.daily_spend_update_queue.size,gauge,,item,,Gauge for in_memory_daily_spend_update_queue service,0,litellm,,,
litellm.in_memory.spend_update_queue.size,gauge,,item,,Gauge for in_memory_spend_update_queue service,0,litellm,,,
litellm.input.tokens.count,count,,token,,Total number of input tokens from LLM requests,0,litellm,,,
litellm.llm.api.failed_requests.metric.count,count,,error,,Deprecated - use litellm.proxy.failed_requests.metric. Total number of failed responses from proxy - the client did not get a success response from litellm proxy,0,litellm,,,
litellm.llm.api.latency.metric.bucket,count,,,,Number of observations that fall into each upper_bound latency bucket (seconds) for a model's LLM API call,0,litellm,,,
litellm.llm.api.latency.metric.count,count,,,,Total number of latency observations (seconds) for a model's LLM API call.,0,litellm,,,
litellm.llm.api.latency.metric.sum,count,,second,,Total latency (seconds) for a models LLM API call,0,litellm,,,
litellm.llm.api.time_to_first_token.metric.bucket,count,,,,Number of observations that fall into each upper_bound time to first token bucket for a model's LLM API call,0,litellm,,,
litellm.llm.api.time_to_first_token.metric.count,count,,,,Total number of time to first token observations for a model's LLM API call.,0,litellm,,,
litellm.llm.api.time_to_first_token.metric.sum,count,,second,,Time to first token for a models LLM API call,0,litellm,,,
litellm.output.tokens.count,count,,token,,Total number of output tokens from LLM requests,0,litellm,,,
litellm.overhead_latency.metric.bucket,count,,,,Number of observations that fall into each upper_bound overhead latency bucket (milliseconds) added by LiteLLM processing,0,litellm,,,
litellm.overhead_latency.metric.count,count,,,,Total number of overhead latency observations (milliseconds) added by LiteLLM processing.,0,litellm,,,
litellm.overhead_latency.metric.sum,count,,millisecond,,Latency overhead (milliseconds) added by LiteLLM processing,0,litellm,,,
litellm.pod_lock_manager.size,gauge,,item,,Gauge for pod_lock_manager service,0,litellm,,,
litellm.postgres.failed_requests.count,count,,error,,Total failed_requests for postgres service,0,litellm,,,
litellm.postgres.latency.bucket,count,,,,Number of observations that fall into each upper_bound latency bucket for postgres service,0,litellm,,,
litellm.postgres.latency.count,count,,,,Total number of latency observations for postgres service.,0,litellm,,,
litellm.postgres.latency.sum,count,,millisecond,,Latency for postgres service,0,litellm,,,
litellm.postgres.total_requests.count,count,,request,,Total total_requests for postgres service,0,litellm,,,
litellm.process.uptime.seconds,gauge,,second,,Start time of the process since unix epoch in seconds.,0,litellm,,,
litellm.provider.remaining_budget.metric,gauge,,dollar,,Remaining budget for provider - used when you set provider budget limits,0,litellm,,,
litellm.proxy.failed_requests.metric.count,count,,error,,Total number of failed responses from proxy - the client did not get a success response from litellm proxy,0,litellm,,,
litellm.proxy.pre_call.failed_requests.count,count,,error,,Total failed_requests for proxy_pre_call service,0,litellm,,,
litellm.proxy.pre_call.latency.bucket,count,,,,Number of observations that fall into each upper_bound latency bucket for proxy_pre_call service,0,litellm,,,
litellm.proxy.pre_call.latency.count,count,,,,Total number of latency observations for proxy_pre_call service.,0,litellm,,,
litellm.proxy.pre_call.latency.sum,count,,millisecond,,Latency for proxy_pre_call service,0,litellm,,,
litellm.proxy.pre_call.total_requests.count,count,,request,,Total total_requests for proxy_pre_call service,0,litellm,,,
litellm.proxy.total_requests.metric.count,count,,request,,Total number of requests made to the proxy server - track number of client side requests,0,litellm,,,
litellm.redis.daily_spend_update_queue.size,gauge,,item,,Gauge for redis_daily_spend_update_queue service,0,litellm,,,
litellm.redis.daily_tag_spend_update_queue.failed_requests.count,count,,error,,Total failed_requests for redis_daily_tag_spend_update_queue service,0,litellm,,,
litellm.redis.daily_tag_spend_update_queue.latency.bucket,count,,,,Number of observations that fall into each upper_bound latency bucket for redis_daily_tag_spend_update_queue service,0,litellm,,,
litellm.redis.daily_tag_spend_update_queue.latency.count,count,,,,Total number of latency observations for redis_daily_tag_spend_update_queue service.,0,litellm,,,
litellm.redis.daily_tag_spend_update_queue.latency.sum,count,,millisecond,,Latency for redis_daily_tag_spend_update_queue service,0,litellm,,,
litellm.redis.daily_tag_spend_update_queue.total_requests.count,count,,request,,Total total_requests for redis_daily_tag_spend_update_queue service,0,litellm,,,
litellm.redis.daily_team_spend_update_queue.failed_requests.count,count,,error,,Total failed_requests for redis_daily_team_spend_update_queue service,0,litellm,,,
litellm.redis.daily_team_spend_update_queue.latency.bucket,count,,,,Number of observations that fall into each upper_bound latency bucket for redis_daily_team_spend_update_queue service,0,litellm,,,
litellm.redis.daily_team_spend_update_queue.latency.count,count,,,,Total number of latency observations for redis_daily_team_spend_update_queue service.,0,litellm,,,
litellm.redis.daily_team_spend_update_queue.latency.sum,count,,millisecond,,Latency for redis_daily_team_spend_update_queue service,0,litellm,,,
litellm.redis.daily_team_spend_update_queue.total_requests.count,count,,request,,Total total_requests for redis_daily_team_spend_update_queue service,0,litellm,,,
litellm.redis.failed_requests.count,count,,error,,Total failed_requests for redis service,0,litellm,,,
litellm.redis.latency.bucket,count,,,,Latency for redis service,0,litellm,,,
litellm.redis.spend_update_queue.size,gauge,,item,,Gauge for redis_spend_update_queue service,0,litellm,,,
litellm.redis.total_requests.count,count,,request,,Total total_requests for redis service,0,litellm,,,
litellm.remaining.api_key.budget.metric,gauge,,dollar,,Remaining budget for api key,0,litellm,,,
litellm.remaining.api_key.requests_for_model,gauge,,request,,Remaining Requests API Key can make for model (model based rpm limit on key),0,litellm,,,
litellm.remaining.api_key.tokens_for_model,gauge,,token,,Remaining Tokens API Key can make for model (model based tpm limit on key),0,litellm,,,
litellm.remaining.requests,gauge,,request,,"remaining requests for model,returned from LLM API Provider",0,litellm,,,
litellm.remaining.team_budget.metric,gauge,,dollar,,Remaining budget for team,0,litellm,,,
litellm.remaining_tokens,gauge,,token,,"remaining tokens for model,returned from LLM API Provider",0,litellm,,,
litellm.request.total_latency.metric.bucket,count,,,,Number of observations that fall into each upper_bound total latency bucket (seconds) for a request to LiteLLM,0,litellm,,,
litellm.request.total_latency.metric.count,count,,,,Total number of total latency observations (seconds) for a request to LiteLLM.,0,litellm,,,
litellm.request.total_latency.metric.sum,count,,second,,Total latency (seconds) for a request to LiteLLM,0,litellm,,,
litellm.requests.metric.count,count,,request,,"Deprecated - use litellm.proxy.total_requests.metric.count. Total number of LLM calls to litellm - track total per API Key,team,user",0,litellm,,,
litellm.reset_budget_job.failed_requests.count,count,,error,,Total failed_requests for reset_budget_job service,0,litellm,,,
litellm.reset_budget_job.latency.bucket,count,,,,Latency for reset_budget_job service,0,litellm,,,
litellm.reset_budget_job.total_requests.count,count,,request,,Total total_requests for reset_budget_job service,0,litellm,,,
litellm.router.failed_requests.count,count,,error,,Total failed_requests for router service,0,litellm,,,
litellm.router.latency.bucket,count,,,,Number of observations that fall into each upper_bound latency bucket for router service,0,litellm,,,
litellm.router.latency.count,count,,,,Total number of latency observations for router service.,0,litellm,,,
litellm.router.latency.sum,count,,millisecond,,Latency for router service,0,litellm,,,
litellm.router.total_requests.count,count,,request,,Total total_requests for router service,0,litellm,,,
litellm.self.failed_requests.count,count,,error,,Total failed_requests for self service,0,litellm,,,
litellm.self.latency.bucket,count,,,,Number of observations that fall into each upper_bound latency bucket for self service,0,litellm,,,
litellm.self.latency.count,count,,,,Total number of latency observations for self service.,0,litellm,,,
litellm.self.latency.sum,count,,millisecond,,Latency for self service,0,litellm,,,
litellm.self.total_requests.count,count,,request,,Total total_requests for self service,0,litellm,,,
litellm.spend.metric.count,count,,dollar,,Total spend on LLM requests,0,litellm,,,
litellm.team.budget.remaining_hours.metric,gauge,,hour,,Remaining days for team budget to be reset,0,litellm,,,
litellm.team.max_budget.metric,gauge,,dollar,,Maximum budget set for team,0,litellm,,,
litellm.total.tokens.count,count,,token,,Total number of input + output tokens from LLM requests,0,litellm,,,
