{"healthy_endpoints":[{"api_base":"https://exampleopenaiendpoint-production.up.railway.app/","rpm":1000,"stream_timeout":0.001,"use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"openai/my-fake-model","cache":{"no-cache":true}},{"api_base":"https://exampleopenaiendpoint-production.up.railway.app/","rpm":1000,"stream_timeout":0.001,"use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"openai/my-fake-model-2","cache":{"no-cache":true}},{"api_base":"https://exampleopenaiendpoint-production.up.railway.app/","use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"openai/fake","cache":{"no-cache":true}},{"api_base":"https://exampleopenaiendpoint-production.up.railway.app/","rpm":1,"stream_timeout":0.001,"use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"openai/my-fake-model","cache":{"no-cache":true}},{"api_base":"https://exampleopenaiendpoint-production.up.railway.app/","use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"openai/my-fake-model","num_retries":50,"cache":{"no-cache":true}},{"api_base":"https://exampleopenaiendpoint-production.up.railway.app/","rpm":1000,"use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"openai/bad-model","cache":{"no-cache":true}},{"api_base":"https://exampleopenaiendpoint-production.up.railway.app/","timeout":1.0,"use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"openai/my-fake-model","cache":{"no-cache":true}}],"unhealthy_endpoints":[{"region_name":"eu","use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"gpt-3.5-turbo","cache":{"no-cache":true},"error":"litellm.RateLimitError: RateLimitError: OpenAIException - You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\nstack trace: Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 801, in acompletion\n    headers, response = await self.make_openai_chat_completion_request(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n    result = await func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 436, in make_openai_chat_completion_request\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 418, in make_openai_chat_completion_request\n    await openai_aclient.chat.completions.with_raw_response.create(\n        **data, timeout=timeout\n    )\n  File \"/usr/lib/python3.13/site-packages/openai/_legacy_response.py\", line 381, in wrapped\n    return cast(LegacyAPIResponse[R],","raw_request_typed_dict":{"raw_request_api_base":"https://api.openai.com/v1/","raw_request_body":{"model":"gpt-3.5-turbo","messages":[{"role":"user","content":"What's 1 + 1?"}],"extra_body":{}},"raw_request_headers":{},"error":null}},{"api_base":"https://openai-gpt-4-test-v-1.openai.azure.com/","api_version":"2023-05-15","use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"azure/gpt-4o-new-test","cache":{"no-cache":true},"error":"litellm.APIError: AzureException APIError - Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.\nstack trace: Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/azure/azure.py\", line 386, in acompletion\n    azure_client = self.get_azure_openai_client(\n        api_version=api_version,\n    ...<5 lines>...\n        litellm_params=litellm_params,\n    )\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/azure/common_utils.py\", line 296, in get_azure_openai_client\n    openai_client = AsyncAzureOpenAI(**azure_client_params)\n  File \"/usr/lib/python3.13/site-packages/openai/lib/azure.py\", line 468, in __init__\n    raise OpenAIError(\n        \"Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.\"\n    )\nopenai.OpenAIError: Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.\n\nDuring handling of the above exception, another","raw_request_typed_dict":null},{"api_base":"https://openai-gpt-4-test-v-1.openai.azure.com/","api_version":"2023-05-15","use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"azure/gpt-4o-new-test","cache":{"no-cache":true},"error":"litellm.APIError: AzureException APIError - Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.\nstack trace: Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/azure/azure.py\", line 386, in acompletion\n    azure_client = self.get_azure_openai_client(\n        api_version=api_version,\n    ...<5 lines>...\n        litellm_params=litellm_params,\n    )\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/azure/common_utils.py\", line 296, in get_azure_openai_client\n    openai_client = AsyncAzureOpenAI(**azure_client_params)\n  File \"/usr/lib/python3.13/site-packages/openai/lib/azure.py\", line 468, in __init__\n    raise OpenAIError(\n        \"Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.\"\n    )\nopenai.OpenAIError: Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.\n\nDuring handling of the above exception, another","raw_request_typed_dict":null},{"rpm":480,"timeout":300.0,"stream_timeout":60.0,"use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"gpt-3.5-turbo-1106","cache":{"no-cache":true},"error":"litellm.RateLimitError: RateLimitError: OpenAIException - You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\nstack trace: Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 801, in acompletion\n    headers, response = await self.make_openai_chat_completion_request(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n    result = await func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 436, in make_openai_chat_completion_request\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 418, in make_openai_chat_completion_request\n    await openai_aclient.chat.completions.with_raw_response.create(\n        **data, timeout=timeout\n    )\n  File \"/usr/lib/python3.13/site-packages/openai/_legacy_response.py\", line 381, in wrapped\n    return cast(LegacyAPIResponse[R],","raw_request_typed_dict":{"raw_request_api_base":"https://api.openai.com/v1/","raw_request_body":{"model":"gpt-3.5-turbo-1106","messages":[{"role":"user","content":"Hey how's it going?"}],"extra_body":{}},"raw_request_headers":{},"error":null}},{"api_base":"https://openai-gpt-4-test-v-1.openai.azure.com/","api_version":"2023-05-15","rpm":480,"timeout":300.0,"stream_timeout":60.0,"use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"azure/gpt-4o-new-test","cache":{"no-cache":true},"error":"litellm.APIError: AzureException APIError - Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.\nstack trace: Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/azure/azure.py\", line 386, in acompletion\n    azure_client = self.get_azure_openai_client(\n        api_version=api_version,\n    ...<5 lines>...\n        litellm_params=litellm_params,\n    )\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/azure/common_utils.py\", line 296, in get_azure_openai_client\n    openai_client = AsyncAzureOpenAI(**azure_client_params)\n  File \"/usr/lib/python3.13/site-packages/openai/lib/azure.py\", line 468, in __init__\n    raise OpenAIError(\n        \"Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.\"\n    )\nopenai.OpenAIError: Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.\n\nDuring handling of the above exception, another","raw_request_typed_dict":null},{"input_cost_per_second":0.00042,"use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"sagemaker/berri-benchmarking-Llama-2-70b-chat-hf-4","cache":{"no-cache":true},"error":"litellm.BadRequestError: litellm.BadRequestError: SagemakerException - Unable to locate credentials\nstack trace: Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 512, in acompletion\n    response = await init_response\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/sagemaker/completion/handler.py\", line 510, in async_completion\n    prepared_request = await asyncified_prepare_request(**prepared_request_args)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/asyncify.py\", line 57, in wrapper\n    return await anyio.to_thread.run_sync(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/anyio/to_thread.py\", line 56, in run_sync\n    return await get_async_backend().run_sync_in_worker_thread(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n        ^^^^^^^^^^^^^^^","raw_request_typed_dict":null},{"api_base":"https://openai-gpt-4-test-v-1.openai.azure.com/","api_version":"2023-05-15","use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"azure/azure-embedding-model","cache":{"no-cache":true},"error":"litellm.APIConnectionError: AzureException APIConnectionError - Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 3351, in aembedding\n    response = await init_response  # type: ignore\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/azure/azure.py\", line 688, in aembedding\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/azure/azure.py\", line 642, in aembedding\n    openai_aclient = self.get_azure_openai_client(\n        api_version=api_version,\n    ...<5 lines>...\n        litellm_params=litellm_params,\n    )\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/azure/common_utils.py\", line 296, in get_azure_openai_client\n    openai_client = AsyncAzureOpenAI(**azure_client_params)\n  File \"/usr/lib/python3.13/site-packages/openai/lib/azure.py\", line 468, in __init__\n    raise OpenAIError(\n        \"Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.\"\n    )\nopenai.OpenAIError: Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.\n\nstack trace: Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 3351, in aembedding\n    response = await init_response  # type: ignore\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/azure/azure.py\", line 688, in aembedding\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/azure/azure.py\", line 642, in aembedding\n    openai_aclient = self.get_azure_openai_client(\n        api_version=api_version,\n    ...<5 lines>...\n        litellm_params=litellm_params,\n    )\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/azure/common_utils.py\", line 296, in get_azure_openai_client\n    openai_client = AsyncAzureOpenAI(**azure_client_params)\n  File \"/usr/lib/python3.13/site-packages/openai/lib/azure.py\", line 468, in __init__\n    raise OpenAIError(\n        \"Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_T","raw_request_typed_dict":{"raw_request_api_base":"","raw_request_body":{"model":"azure-embedding-model","input":["test from litellm"]},"raw_request_headers":{},"error":null}},{"use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"openai/dall-e-3","cache":{"no-cache":true},"error":"litellm.BadRequestError: OpenAIException - You are not allowed to sample from this model\nstack trace: Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 801, in acompletion\n    headers, response = await self.make_openai_chat_completion_request(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n    result = await func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 436, in make_openai_chat_completion_request\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 418, in make_openai_chat_completion_request\n    await openai_aclient.chat.completions.with_raw_response.create(\n        **data, timeout=timeout\n    )\n  File \"/usr/lib/python3.13/site-packages/openai/_legacy_response.py\", line 381, in wrapped\n    return cast(LegacyAPIResponse[R],","raw_request_typed_dict":{"raw_request_api_base":"https://api.openai.com/v1/","raw_request_body":{"model":"dall-e-3","messages":[{"role":"user","content":"Hey how's it going?"}],"extra_body":{}},"raw_request_headers":{},"error":null}},{"use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"dall-e-3","cache":{"no-cache":true},"error":"litellm.BadRequestError: OpenAIException - You are not allowed to sample from this model\nstack trace: Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 801, in acompletion\n    headers, response = await self.make_openai_chat_completion_request(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n    result = await func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 436, in make_openai_chat_completion_request\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 418, in make_openai_chat_completion_request\n    await openai_aclient.chat.completions.with_raw_response.create(\n        **data, timeout=timeout\n    )\n  File \"/usr/lib/python3.13/site-packages/openai/_legacy_response.py\", line 381, in wrapped\n    return cast(LegacyAPIResponse[R],","raw_request_typed_dict":{"raw_request_api_base":"https://api.openai.com/v1/","raw_request_body":{"model":"dall-e-3","messages":[{"role":"user","content":"Hey how's it going?"}],"extra_body":{}},"raw_request_headers":{},"error":null}},{"api_base":"https://exampleopenaiendpoint-production.up.railway.app/","rpm":1000,"timeout":60.0,"use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"openai/bad-model","mock_timeout":true,"cache":{"no-cache":true}},{"use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"gpt-4.1-nano","cache":{"no-cache":true},"fallbacks":["gpt-4.1-nano-2025-04-14","gpt-4o-mini-audio-preview"],"max_tokens":1,"error":"litellm.RateLimitError: RateLimitError: OpenAIException - You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.. All fallback attempts failed. Enable verbose logging with `litellm.set_verbose=True` for details.\nstack trace: Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 5218, in ahealth_check\n    return await ahealth_check_wildcard_models(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 5158, in ahealth_check_wildcard_models\n    await acompletion(**model_params)\n  File \"/usr/lib/python3.13/site-packages/litellm/utils.py\", line 1492, in wrapper_async\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/utils.py\", line 1353, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 487, in acompletion\n    response = await async_completion_with_fallbacks(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","raw_request_typed_dict":{"raw_request_api_base":"https://api.openai.com/v1/","raw_request_body":{"model":"gpt-4o-mini-audio-preview","messages":[{"role":"user","content":"What's 1 + 1?"}],"max_tokens":1,"extra_body":{}},"raw_request_headers":{},"error":null}},{"use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"claude-instant-1.2","cache":{"no-cache":true},"fallbacks":["claude-3-haiku-20240307","claude-3-5-haiku-20241022"],"max_tokens":1,"error":"litellm.AuthenticationError: Missing Anthropic API Key - A call is being made to anthropic but no key is set either in the environment variables or via params. Please set `ANTHROPIC_API_KEY` in your environment vars. All fallback attempts failed. Enable verbose logging with `litellm.set_verbose=True` for details.\nstack trace: Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 5218, in ahealth_check\n    return await ahealth_check_wildcard_models(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 5158, in ahealth_check_wildcard_models\n    await acompletion(**model_params)\n  File \"/usr/lib/python3.13/site-packages/litellm/utils.py\", line 1492, in wrapper_async\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/utils.py\", line 1353, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 487, in acompletion\n    response = await async_completion_with_fallbacks(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","raw_request_typed_dict":null},{"use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"amazon.nova-micro-v1:0","cache":{"no-cache":true},"fallbacks":["us.amazon.nova-micro-v1:0","apac.amazon.nova-micro-v1:0"],"max_tokens":1,"error":"litellm.AuthenticationError: BedrockException Invalid Authentication - Unable to locate credentials. All fallback attempts failed. Enable verbose logging with `litellm.set_verbose=True` for details.\nstack trace: Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 5218, in ahealth_check\n    return await ahealth_check_wildcard_models(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 5158, in ahealth_check_wildcard_models\n    await acompletion(**model_params)\n  File \"/usr/lib/python3.13/site-packages/litellm/utils.py\", line 1492, in wrapper_async\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/utils.py\", line 1353, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 487, in acompletion\n    response = await async_completion_with_fallbacks(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","raw_request_typed_dict":null},{"use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"groq/llama-3.2-1b-preview","cache":{"no-cache":true},"fallbacks":["groq/llama-3.2-3b-preview","groq/llama3-8b-8192"],"max_tokens":1,"error":"litellm.BadRequestError: GroqException - {\"error\":{\"message\":\"Invalid API Key\",\"type\":\"invalid_request_error\",\"code\":\"invalid_api_key\"}}\n. All fallback attempts failed. Enable verbose logging with `litellm.set_verbose=True` for details.\nstack trace: Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 5218, in ahealth_check\n    return await ahealth_check_wildcard_models(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 5158, in ahealth_check_wildcard_models\n    await acompletion(**model_params)\n  File \"/usr/lib/python3.13/site-packages/litellm/utils.py\", line 1492, in wrapper_async\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/utils.py\", line 1353, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 487, in acompletion\n    response = await async_completion_with_fallbacks(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","raw_request_typed_dict":{"raw_request_api_base":"https://api.groq.com/openai/v1/chat/completions","raw_request_body":{"model":"llama3-8b-8192","messages":[{"role":"user","content":"Hey how's it going?"}],"max_tokens":1},"raw_request_headers":{"Content-Type":"ap****on"},"error":null}},{"use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"mistral/mistral-embed","cache":{"no-cache":true},"error":"litellm.AuthenticationError: AuthenticationError: MistralException - Error code: 401 - {'detail': 'Unauthorized'}\nstack trace: Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 1116, in aembedding\n    headers, response = await self.make_openai_embedding_request(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n    result = await func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 1069, in make_openai_embedding_request\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 1062, in make_openai_embedding_request\n    raw_response = await openai_aclient.embeddings.with_raw_response.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        **data, timeout=timeout\n        ^^^^^^^^^^^^^^^^^^^^^^^\n    )  # type: ignore\n    ^\n  File \"/us","raw_request_typed_dict":{"raw_request_api_base":"https://api.mistral.ai/v1","raw_request_body":{"model":"mistral-embed","input":["test from litellm"]},"raw_request_headers":{},"error":null}},{"use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"text-completion-openai/gpt-3.5-turbo-instruct","cache":{"no-cache":true},"error":"litellm.RateLimitError: RateLimitError: OpenAIException - Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\nstack trace: Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/openai/completion/handler.py\", line 179, in acompletion\n    raw_response = await openai_aclient.completions.with_raw_response.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        **data\n        ^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/openai/_legacy_response.py\", line 381, in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/openai/resources/completions.py\", line 1091, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n    ...<32 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","raw_request_typed_dict":{"raw_request_api_base":"https://api.openai.com/v1","raw_request_body":{"model":"gpt-3.5-turbo-instruct","prompt":"test from litellm","extra_body":{}},"raw_request_headers":{"content-type":"ap****on"},"error":null}},{"api_base":"https://exampleopenaiendpoint-production.up.railway.appxxxx/","use_in_pass_through":false,"use_litellm_proxy":false,"merge_reasoning_content_in_choices":false,"model":"openai/my-fake-model","cache":{"no-cache":true},"error":"litellm.APIError: APIError: OpenAIException - Connection error.\nstack trace: Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        pool_request.request\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n    raise exc\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 7","raw_request_typed_dict":{"raw_request_api_base":"https://exampleopenaiendpoint-production.up.railway.appxxxx/","raw_request_body":{"model":"my-fake-model","messages":[{"role":"user","content":"Hey how's it going?"}],"extra_body":{}},"raw_request_headers":{},"error":null}}],"healthy_count":7,"unhealthy_count":17}