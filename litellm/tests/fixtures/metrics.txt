# HELP python_gc_objects_collected_total Objects collected during gc
# TYPE python_gc_objects_collected_total counter
python_gc_objects_collected_total{generation="0"} 84906.0
python_gc_objects_collected_total{generation="1"} 13378.0
python_gc_objects_collected_total{generation="2"} 447.0
# HELP python_gc_objects_uncollectable_total Uncollectable objects found during GC
# TYPE python_gc_objects_uncollectable_total counter
python_gc_objects_uncollectable_total{generation="0"} 0.0
python_gc_objects_uncollectable_total{generation="1"} 0.0
python_gc_objects_uncollectable_total{generation="2"} 0.0
# HELP python_gc_collections_total Number of times this generation was collected
# TYPE python_gc_collections_total counter
python_gc_collections_total{generation="0"} 443.0
python_gc_collections_total{generation="1"} 40.0
python_gc_collections_total{generation="2"} 3.0
# HELP python_info Python platform information
# TYPE python_info gauge
python_info{implementation="CPython",major="3",minor="13",patchlevel="3",version="3.13.3"} 1.0
# HELP process_virtual_memory_bytes Virtual memory size in bytes.
# TYPE process_virtual_memory_bytes gauge
process_virtual_memory_bytes 8.27879424e+08
# HELP process_resident_memory_bytes Resident memory size in bytes.
# TYPE process_resident_memory_bytes gauge
process_resident_memory_bytes 4.265984e+08
# HELP process_start_time_seconds Start time of the process since unix epoch in seconds.
# TYPE process_start_time_seconds gauge
process_start_time_seconds 1.74957312267e+09
# HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.
# TYPE process_cpu_seconds_total counter
process_cpu_seconds_total 3.32
# HELP process_open_fds Number of open file descriptors.
# TYPE process_open_fds gauge
process_open_fds 22.0
# HELP process_max_fds Maximum number of open file descriptors.
# TYPE process_max_fds gauge
process_max_fds 1.048576e+06
# HELP litellm_redis_latency Latency for redis service
# TYPE litellm_redis_latency histogram
litellm_redis_latency_bucket{le="0.005",redis="redis"} 0.0
litellm_redis_latency_bucket{le="0.00625",redis="redis"} 0.0
litellm_redis_latency_bucket{le="0.0125",redis="redis"} 0.0
litellm_redis_latency_bucket{le="0.025",redis="redis"} 0.0
litellm_redis_latency_bucket{le="0.05",redis="redis"} 0.0
litellm_redis_latency_bucket{le="0.1",redis="redis"} 0.0
litellm_redis_latency_bucket{le="0.5",redis="redis"} 0.0
# HELP litellm_redis_failed_requests_total Total failed_requests for redis service
# TYPE litellm_redis_failed_requests_total counter
litellm_redis_failed_requests_total 0.0
# HELP litellm_redis_total_requests_total Total total_requests for redis service
# TYPE litellm_redis_total_requests_total counter
litellm_redis_total_requests_total 0.0
# HELP litellm_postgres_latency Latency for postgres service
# TYPE litellm_postgres_latency histogram
litellm_postgres_latency_bucket{le="0.005",postgres="postgres"} 39.0
litellm_postgres_latency_bucket{le="0.00625",postgres="postgres"} 39.0
litellm_postgres_latency_bucket{le="0.0125",postgres="postgres"} 42.0
litellm_postgres_latency_bucket{le="0.025",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="0.05",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="0.1",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="0.5",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="1.0",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="1.5",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="2.0",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="2.5",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="3.0",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="3.5",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="4.0",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="4.5",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="5.0",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="5.5",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="6.0",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="6.5",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="7.0",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="7.5",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="8.0",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="8.5",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="9.0",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="9.5",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="10.0",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="15.0",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="20.0",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="25.0",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="30.0",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="60.0",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="120.0",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="180.0",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="240.0",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="300.0",postgres="postgres"} 43.0
litellm_postgres_latency_bucket{le="+Inf",postgres="postgres"} 43.0
litellm_postgres_latency_count{postgres="postgres"} 43.0
litellm_postgres_latency_sum{postgres="postgres"} 0.13723799999999997
# HELP litellm_postgres_latency_created Latency for postgres service
# TYPE litellm_postgres_latency_created gauge
litellm_postgres_latency_created{postgres="postgres"} 1.7495731311202419e+09
# HELP litellm_postgres_failed_requests_total Total failed_requests for postgres service
# TYPE litellm_postgres_failed_requests_total counter
litellm_postgres_failed_requests_total 0.0
# HELP litellm_postgres_total_requests_total Total total_requests for postgres service
# TYPE litellm_postgres_total_requests_total counter
litellm_postgres_total_requests_total{postgres="postgres"} 43.0
# HELP litellm_postgres_total_requests_created Total total_requests for postgres service
# TYPE litellm_postgres_total_requests_created gauge
litellm_postgres_total_requests_created{postgres="postgres"} 1.7495731311202981e+09
# HELP litellm_batch_write_to_db_latency Latency for batch_write_to_db service
# TYPE litellm_batch_write_to_db_latency histogram
litellm_batch_write_to_db_latency_bucket{le="0.005",batch_write_to_db="batch_write_to_db"} 0.0
litellm_batch_write_to_db_latency_bucket{le="0.00625",batch_write_to_db="batch_write_to_db"} 0.0
litellm_batch_write_to_db_latency_bucket{le="0.0125",batch_write_to_db="batch_write_to_db"} 0.0
litellm_batch_write_to_db_latency_bucket{le="0.025",batch_write_to_db="batch_write_to_db"} 0.0
litellm_batch_write_to_db_latency_bucket{le="0.05",batch_write_to_db="batch_write_to_db"} 0.0
litellm_batch_write_to_db_latency_bucket{le="0.1",batch_write_to_db="batch_write_to_db"} 0.0
litellm_batch_write_to_db_latency_bucket{le="0.5",batch_write_to_db="batch_write_to_db"} 0.0
litellm_batch_write_to_db_latency_bucket{le="1.0",batch_write_to_db="batch_write_to_db"} 0.0
litellm_batch_write_to_db_latency_bucket{le="1.5",batch_write_to_db="batch_write_to_db"} 0.0
litellm_batch_write_to_db_latency_bucket{le="2.0",batch_write_to_db="batch_write_to_db"} 0.0
litellm_batch_write_to_db_latency_bucket{le="2.5",batch_write_to_db="batch_write_to_db"} 0.0
litellm_batch_write_to_db_latency_bucket{le="3.0",batch_write_to_db="batch_write_to_db"} 0.0
litellm_batch_write_to_db_latency_count{batch_write_to_db="batch_write_to_db"} 0.0
litellm_batch_write_to_db_latency_sum{batch_write_to_db="batch_write_to_db"} 0.0
# HELP litellm_batch_write_to_db_failed_requests_total Total failed_requests for batch_write_to_db service
# TYPE litellm_batch_write_to_db_failed_requests_total counter
litellm_batch_write_to_db_failed_requests_total 0.0
# HELP litellm_batch_write_to_db_total_requests_total Total total_requests for batch_write_to_db service
# TYPE litellm_batch_write_to_db_total_requests_total counter
litellm_batch_write_to_db_total_requests_total 0.0
# HELP litellm_reset_budget_job_latency Latency for reset_budget_job service
# TYPE litellm_reset_budget_job_latency histogram
litellm_reset_budget_job_latency_bucket{le="0.005",reset_budget_job="reset_budget_job"} 0.0
litellm_reset_budget_job_latency_bucket{le="0.00625",reset_budget_job="reset_budget_job"} 0.0
litellm_reset_budget_job_latency_bucket{le="0.0125",reset_budget_job="reset_budget_job"} 0.0
litellm_reset_budget_job_latency_bucket{le="0.025",reset_budget_job="reset_budget_job"} 0.0
litellm_reset_budget_job_latency_bucket{le="0.05",reset_budget_job="reset_budget_job"} 0.0
litellm_reset_budget_job_latency_bucket{le="0.1",reset_budget_job="reset_budget_job"} 0.0
litellm_reset_budget_job_latency_bucket{le="0.5",reset_budget_job="reset_budget_job"} 0.0
# HELP litellm_reset_budget_job_failed_requests_total Total failed_requests for reset_budget_job service
# TYPE litellm_reset_budget_job_failed_requests_total counter
litellm_reset_budget_job_failed_requests_total 0.0
# HELP litellm_reset_budget_job_total_requests_total Total total_requests for reset_budget_job service
# TYPE litellm_reset_budget_job_total_requests_total counter
litellm_reset_budget_job_total_requests_total 0.0
# HELP litellm_self_latency Latency for self service
# TYPE litellm_self_latency histogram
litellm_self_latency_bucket{le="0.005",self="self"} 0.0
litellm_self_latency_bucket{le="0.00625",self="self"} 0.0
litellm_self_latency_bucket{le="0.0125",self="self"} 0.0
litellm_self_latency_bucket{le="0.025",self="self"} 0.0
litellm_self_latency_bucket{le="0.05",self="self"} 0.0
litellm_self_latency_bucket{le="0.1",self="self"} 0.0
litellm_self_latency_bucket{le="0.5",self="self"} 0.0
litellm_self_latency_count{self="self"} 0.0
litellm_self_latency_sum{self="self"} 0.0
# HELP litellm_self_failed_requests_total Total failed_requests for self service
# TYPE litellm_self_failed_requests_total counter
litellm_self_failed_requests_total 0.0
# HELP litellm_self_total_requests_total Total total_requests for self service
# TYPE litellm_self_total_requests_total counter
litellm_self_total_requests_total 0.0
# HELP litellm_router_latency Latency for router service
# TYPE litellm_router_latency histogram
litellm_router_latency_bucket{le="0.005",router="router"} 0.0
litellm_router_latency_bucket{le="0.00625",router="router"} 0.0
litellm_router_latency_bucket{le="0.0125",router="router"} 0.0
litellm_router_latency_bucket{le="0.025",router="router"} 0.0
litellm_router_latency_bucket{le="0.05",router="router"} 0.0
litellm_router_latency_bucket{le="0.1",router="router"} 0.0
litellm_router_latency_bucket{le="0.5",router="router"} 0.0
litellm_router_latency_count{router="router"} 0.0
litellm_router_latency_sum{router="router"} 0.0
# HELP litellm_router_failed_requests_total Total failed_requests for router service
# TYPE litellm_router_failed_requests_total counter
litellm_router_failed_requests_total 0.0
# HELP litellm_router_total_requests_total Total total_requests for router service
# TYPE litellm_router_total_requests_total counter
litellm_router_total_requests_total 0.0
# HELP litellm_auth_latency Latency for auth service
# TYPE litellm_auth_latency histogram
litellm_auth_latency_bucket{auth="auth",le="0.005"} 1.0
litellm_auth_latency_bucket{auth="auth",le="0.00625"} 1.0
litellm_auth_latency_bucket{auth="auth",le="0.0125"} 1.0
litellm_auth_latency_bucket{auth="auth",le="0.025"} 1.0
litellm_auth_latency_bucket{auth="auth",le="0.05"} 1.0
litellm_auth_latency_bucket{auth="auth",le="0.1"} 1.0
litellm_auth_latency_bucket{auth="auth",le="0.5"} 1.0
litellm_auth_latency_bucket{auth="auth",le="1.0"} 1.0
litellm_auth_latency_bucket{auth="auth",le="1.5"} 1.0
litellm_auth_latency_bucket{auth="auth",le="2.0"} 1.0
litellm_auth_latency_bucket{auth="auth",le="2.5"} 1.0
litellm_auth_latency_bucket{auth="auth",le="3.0"} 1.0
litellm_auth_latency_bucket{auth="auth",le="3.5"} 1.0
litellm_auth_latency_bucket{auth="auth",le="4.0"} 1.0
litellm_auth_latency_bucket{auth="auth",le="4.5"} 1.0
litellm_auth_latency_bucket{auth="auth",le="5.0"} 1.0
litellm_auth_latency_bucket{auth="auth",le="5.5"} 1.0
litellm_auth_latency_bucket{auth="auth",le="6.0"} 1.0
litellm_auth_latency_bucket{auth="auth",le="6.5"} 1.0
litellm_auth_latency_bucket{auth="auth",le="7.0"} 1.0
litellm_auth_latency_bucket{auth="auth",le="7.5"} 1.0
litellm_auth_latency_bucket{auth="auth",le="8.0"} 1.0
litellm_auth_latency_bucket{auth="auth",le="8.5"} 1.0
litellm_auth_latency_bucket{auth="auth",le="9.0"} 1.0
litellm_auth_latency_bucket{auth="auth",le="9.5"} 1.0
litellm_auth_latency_bucket{auth="auth",le="10.0"} 1.0
litellm_auth_latency_bucket{auth="auth",le="15.0"} 1.0
litellm_auth_latency_bucket{auth="auth",le="20.0"} 1.0
litellm_auth_latency_bucket{auth="auth",le="25.0"} 1.0
litellm_auth_latency_bucket{auth="auth",le="30.0"} 1.0
litellm_auth_latency_bucket{auth="auth",le="60.0"} 1.0
litellm_auth_latency_bucket{auth="auth",le="120.0"} 1.0
litellm_auth_latency_bucket{auth="auth",le="180.0"} 1.0
litellm_auth_latency_bucket{auth="auth",le="240.0"} 1.0
litellm_auth_latency_bucket{auth="auth",le="300.0"} 1.0
litellm_auth_latency_bucket{auth="auth",le="+Inf"} 1.0
litellm_auth_latency_count{auth="auth"} 1.0
litellm_auth_latency_sum{auth="auth"} 0.00017118453979492188
# HELP litellm_auth_latency_created Latency for auth service
# TYPE litellm_auth_latency_created gauge
litellm_auth_latency_created{auth="auth"} 1.74957314351157e+09
# HELP litellm_auth_failed_requests_total Total failed_requests for auth service
# TYPE litellm_auth_failed_requests_total counter
litellm_auth_failed_requests_total 0.0
# HELP litellm_auth_total_requests_total Total total_requests for auth service
# TYPE litellm_auth_total_requests_total counter
litellm_auth_total_requests_total{auth="auth"} 1.0
# HELP litellm_auth_total_requests_created Total total_requests for auth service
# TYPE litellm_auth_total_requests_created gauge
litellm_auth_total_requests_created{auth="auth"} 1.7495731435116324e+09
# HELP litellm_proxy_pre_call_latency Latency for proxy_pre_call service
# TYPE litellm_proxy_pre_call_latency histogram
litellm_proxy_pre_call_latency_bucket{le="0.005",proxy_pre_call="proxy_pre_call"} 0.0
litellm_proxy_pre_call_latency_bucket{le="0.00625",proxy_pre_call="proxy_pre_call"} 0.0
litellm_proxy_pre_call_latency_bucket{le="0.0125",proxy_pre_call="proxy_pre_call"} 0.0
litellm_proxy_pre_call_latency_bucket{le="0.025",proxy_pre_call="proxy_pre_call"} 0.0
litellm_proxy_pre_call_latency_bucket{le="0.05",proxy_pre_call="proxy_pre_call"} 0.0
litellm_proxy_pre_call_latency_bucket{le="0.1",proxy_pre_call="proxy_pre_call"} 0.0
litellm_proxy_pre_call_latency_bucket{le="0.5",proxy_pre_call="proxy_pre_call"} 0.0
litellm_proxy_pre_call_latency_count{proxy_pre_call="proxy_pre_call"} 0.0
litellm_proxy_pre_call_latency_sum{proxy_pre_call="proxy_pre_call"} 0.0
# HELP litellm_proxy_pre_call_failed_requests_total Total failed_requests for proxy_pre_call service
# TYPE litellm_proxy_pre_call_failed_requests_total counter
litellm_proxy_pre_call_failed_requests_total 0.0
# HELP litellm_proxy_pre_call_total_requests_total Total total_requests for proxy_pre_call service
# TYPE litellm_proxy_pre_call_total_requests_total counter
litellm_proxy_pre_call_total_requests_total 0.0
# HELP litellm_pod_lock_manager_size Gauge for pod_lock_manager service
# TYPE litellm_pod_lock_manager_size gauge
litellm_pod_lock_manager_size 0.0
# HELP litellm_in_memory_daily_spend_update_queue_size Gauge for in_memory_daily_spend_update_queue service
# TYPE litellm_in_memory_daily_spend_update_queue_size gauge
litellm_in_memory_daily_spend_update_queue_size 0.0
# HELP litellm_redis_daily_spend_update_queue_size Gauge for redis_daily_spend_update_queue service
# TYPE litellm_redis_daily_spend_update_queue_size gauge
litellm_redis_daily_spend_update_queue_size 0.0
# HELP litellm_redis_daily_team_spend_update_queue_latency Latency for redis_daily_team_spend_update_queue service
# TYPE litellm_redis_daily_team_spend_update_queue_latency histogram
litellm_redis_daily_team_spend_update_queue_latency_bucket{le="0.005",redis_daily_team_spend_update_queue="redis_daily_team_spend_update_queue"} 0.0
litellm_redis_daily_team_spend_update_queue_latency_bucket{le="0.00625",redis_daily_team_spend_update_queue="redis_daily_team_spend_update_queue"} 0.0
litellm_redis_daily_team_spend_update_queue_latency_bucket{le="0.0125",redis_daily_team_spend_update_queue="redis_daily_team_spend_update_queue"} 0.0
litellm_redis_daily_team_spend_update_queue_latency_bucket{le="0.025",redis_daily_team_spend_update_queue="redis_daily_team_spend_update_queue"} 0.0
litellm_redis_daily_team_spend_update_queue_latency_bucket{le="0.05",redis_daily_team_spend_update_queue="redis_daily_team_spend_update_queue"} 0.0
litellm_redis_daily_team_spend_update_queue_latency_bucket{le="0.1",redis_daily_team_spend_update_queue="redis_daily_team_spend_update_queue"} 0.0
litellm_redis_daily_team_spend_update_queue_latency_bucket{le="0.5",redis_daily_team_spend_update_queue="redis_daily_team_spend_update_queue"} 0.0
litellm_redis_daily_team_spend_update_queue_latency_count{redis_daily_team_spend_update_queue="redis_daily_team_spend_update_queue"} 0.0
litellm_redis_daily_team_spend_update_queue_latency_sum{redis_daily_team_spend_update_queue="redis_daily_team_spend_update_queue"} 0.0
# HELP litellm_redis_daily_team_spend_update_queue_failed_requests_total Total failed_requests for redis_daily_team_spend_update_queue service
# TYPE litellm_redis_daily_team_spend_update_queue_failed_requests_total counter
litellm_redis_daily_team_spend_update_queue_failed_requests_total 0.0
# HELP litellm_redis_daily_team_spend_update_queue_total_requests_total Total total_requests for redis_daily_team_spend_update_queue service
# TYPE litellm_redis_daily_team_spend_update_queue_total_requests_total counter
litellm_redis_daily_team_spend_update_queue_total_requests_total 0.0
# HELP litellm_redis_daily_tag_spend_update_queue_latency Latency for redis_daily_tag_spend_update_queue service
# TYPE litellm_redis_daily_tag_spend_update_queue_latency histogram
litellm_redis_daily_tag_spend_update_queue_latency_bucket{le="0.005",redis_daily_tag_spend_update_queue="redis_daily_tag_spend_update_queue"} 0.0
litellm_redis_daily_tag_spend_update_queue_latency_bucket{le="0.00625",redis_daily_tag_spend_update_queue="redis_daily_tag_spend_update_queue"} 0.0
litellm_redis_daily_tag_spend_update_queue_latency_bucket{le="0.0125",redis_daily_tag_spend_update_queue="redis_daily_tag_spend_update_queue"} 0.0
litellm_redis_daily_tag_spend_update_queue_latency_bucket{le="0.025",redis_daily_tag_spend_update_queue="redis_daily_tag_spend_update_queue"} 0.0
litellm_redis_daily_tag_spend_update_queue_latency_bucket{le="0.05",redis_daily_tag_spend_update_queue="redis_daily_tag_spend_update_queue"} 0.0
litellm_redis_daily_tag_spend_update_queue_latency_bucket{le="0.1",redis_daily_tag_spend_update_queue="redis_daily_tag_spend_update_queue"} 0.0
litellm_redis_daily_tag_spend_update_queue_latency_bucket{le="0.5",redis_daily_tag_spend_update_queue="redis_daily_tag_spend_update_queue"} 0.0
litellm_redis_daily_tag_spend_update_queue_latency_count{redis_daily_tag_spend_update_queue="redis_daily_tag_spend_update_queue"} 0.0
litellm_redis_daily_tag_spend_update_queue_latency_sum{redis_daily_tag_spend_update_queue="redis_daily_tag_spend_update_queue"} 0.0
# HELP litellm_redis_daily_tag_spend_update_queue_failed_requests_total Total failed_requests for redis_daily_tag_spend_update_queue service
# TYPE litellm_redis_daily_tag_spend_update_queue_failed_requests_total counter
litellm_redis_daily_tag_spend_update_queue_failed_requests_total 0.0
# HELP litellm_redis_daily_tag_spend_update_queue_total_requests_total Total total_requests for redis_daily_tag_spend_update_queue service
# TYPE litellm_redis_daily_tag_spend_update_queue_total_requests_total counter
litellm_redis_daily_tag_spend_update_queue_total_requests_total 0.0
# HELP litellm_in_memory_spend_update_queue_size Gauge for in_memory_spend_update_queue service
# TYPE litellm_in_memory_spend_update_queue_size gauge
litellm_in_memory_spend_update_queue_size 0.0
# HELP litellm_redis_spend_update_queue_size Gauge for redis_spend_update_queue service
# TYPE litellm_redis_spend_update_queue_size gauge
litellm_redis_spend_update_queue_size 0.0
# HELP litellm_proxy_failed_requests_metric_total Total number of failed responses from proxy - the client did not get a success response from litellm proxy
# TYPE litellm_proxy_failed_requests_metric_total counter
litellm_proxy_failed_requests_metric_total{api_key_alias="None",end_user="None",exception_class="Exception",exception_status="None",hashed_api_key="",requested_model="",route="/metrics/",team="None",team_alias="None",user="None"} 7.0
# HELP litellm_proxy_failed_requests_metric_created Total number of failed responses from proxy - the client did not get a success response from litellm proxy
# TYPE litellm_proxy_failed_requests_metric_created gauge
litellm_proxy_failed_requests_metric_created{api_key_alias="None",end_user="None",exception_class="Exception",exception_status="None",hashed_api_key="",requested_model="",route="/metrics/",team="None",team_alias="None",user="None"} 1.7495731315073106e+09
# HELP litellm_proxy_total_requests_metric_total Total number of requests made to the proxy server - track number of client side requests
# TYPE litellm_proxy_total_requests_metric_total counter
litellm_proxy_total_requests_metric_total{api_key_alias="None",end_user="None",hashed_api_key="",requested_model="",route="/metrics/",status_code="None",team="None",team_alias="None",user="None",user_email="None"} 7.0
# HELP litellm_proxy_total_requests_metric_created Total number of requests made to the proxy server - track number of client side requests
# TYPE litellm_proxy_total_requests_metric_created gauge
litellm_proxy_total_requests_metric_created{api_key_alias="None",end_user="None",hashed_api_key="",requested_model="",route="/metrics/",status_code="None",team="None",team_alias="None",user="None",user_email="None"} 1.7495731315073416e+09
# HELP litellm_request_total_latency_metric Total latency (seconds) for a request to LiteLLM
# TYPE litellm_request_total_latency_metric histogram
litellm_request_total_latency_metric_bucket{le="0.005",request_total_latency_metric="request_total_latency_metric"} 0.0
litellm_request_total_latency_metric_bucket{le="0.00625",request_total_latency_metric="request_total_latency_metric"} 0.0
litellm_request_total_latency_metric_bucket{le="0.0125",request_total_latency_metric="request_total_latency_metric"} 0.0
litellm_request_total_latency_metric_bucket{le="0.025",request_total_latency_metric="request_total_latency_metric"} 0.0
litellm_request_total_latency_metric_bucket{le="0.05",request_total_latency_metric="request_total_latency_metric"} 0.0
litellm_request_total_latency_metric_bucket{le="0.1",request_total_latency_metric="request_total_latency_metric"} 0.0
litellm_request_total_latency_metric_bucket{le="0.5",request_total_latency_metric="request_total_latency_metric"} 0.0
litellm_request_total_latency_metric_count{request_total_latency_metric="request_total_latency_metric"} 0.0
litellm_request_total_latency_metric_sum{request_total_latency_metric="request_total_latency_metric"} 0.0
# HELP litellm_llm_api_latency_metric Total latency (seconds) for a models LLM API call
# TYPE litellm_llm_api_latency_metric histogram
litellm_llm_api_latency_metric_bucket{le="0.005",llm_api_latency_metric="llm_api_latency_metric"} 0.0
litellm_llm_api_latency_metric_bucket{le="0.00625",llm_api_latency_metric="llm_api_latency_metric"} 0.0
litellm_llm_api_latency_metric_bucket{le="0.0125",llm_api_latency_metric="llm_api_latency_metric"} 0.0
litellm_llm_api_latency_metric_bucket{le="0.025",llm_api_latency_metric="llm_api_latency_metric"} 0.0
litellm_llm_api_latency_metric_bucket{le="0.05",llm_api_latency_metric="llm_api_latency_metric"} 0.0
litellm_llm_api_latency_metric_bucket{le="0.1",llm_api_latency_metric="llm_api_latency_metric"} 0.0
litellm_llm_api_latency_metric_bucket{le="0.5",llm_api_latency_metric="llm_api_latency_metric"} 0.0
litellm_llm_api_latency_metric_count{llm_api_latency_metric="llm_api_latency_metric"} 0.0
litellm_llm_api_latency_metric_sum{llm_api_latency_metric="llm_api_latency_metric"} 0.0
# HELP litellm_llm_api_time_to_first_token_metric Time to first token for a models LLM API call
# TYPE litellm_llm_api_time_to_first_token_metric histogram
litellm_llm_api_time_to_first_token_metric_bucket{le="0.005",llm_api_time_to_first_token_metric="llm_api_time_to_first_token_metric"} 0.0
litellm_llm_api_time_to_first_token_metric_bucket{le="0.00625",llm_api_time_to_first_token_metric="llm_api_time_to_first_token_metric"} 0.0
litellm_llm_api_time_to_first_token_metric_bucket{le="0.0125",llm_api_time_to_first_token_metric="llm_api_time_to_first_token_metric"} 0.0
litellm_llm_api_time_to_first_token_metric_bucket{le="0.025",llm_api_time_to_first_token_metric="llm_api_time_to_first_token_metric"} 0.0
litellm_llm_api_time_to_first_token_metric_bucket{le="0.05",llm_api_time_to_first_token_metric="llm_api_time_to_first_token_metric"} 0.0
litellm_llm_api_time_to_first_token_metric_bucket{le="0.1",llm_api_time_to_first_token_metric="llm_api_time_to_first_token_metric"} 0.0
litellm_llm_api_time_to_first_token_metric_bucket{le="0.5",llm_api_time_to_first_token_metric="llm_api_time_to_first_token_metric"} 0.0
litellm_llm_api_time_to_first_token_metric_count{llm_api_time_to_first_token_metric="llm_api_time_to_first_token_metric"} 0.0
litellm_llm_api_time_to_first_token_metric_sum{llm_api_time_to_first_token_metric="llm_api_time_to_first_token_metric"} 0.0
# HELP litellm_spend_metric_total Total spend on LLM requests
# TYPE litellm_spend_metric_total counter
litellm_spend_metric_total 0.0
# HELP litellm_total_tokens_total Total number of input + output tokens from LLM requests
# TYPE litellm_total_tokens_total counter
litellm_total_tokens_total 0.0
# HELP litellm_input_tokens_total Total number of input tokens from LLM requests
# TYPE litellm_input_tokens_total counter
litellm_input_tokens_total 0.0
# HELP litellm_output_tokens_total Total number of output tokens from LLM requests
# TYPE litellm_output_tokens_total counter
litellm_output_tokens_total 0.0
# HELP litellm_remaining_team_budget_metric Remaining budget for team
# TYPE litellm_remaining_team_budget_metric gauge
litellm_remaining_team_budget_metric 0.0
# HELP litellm_team_max_budget_metric Maximum budget set for team
# TYPE litellm_team_max_budget_metric gauge
litellm_team_max_budget_metric 0.0
# HELP litellm_team_budget_remaining_hours_metric Remaining days for team budget to be reset
# TYPE litellm_team_budget_remaining_hours_metric gauge
litellm_team_budget_remaining_hours_metric 0.0
# HELP litellm_remaining_api_key_budget_metric Remaining budget for api key
# TYPE litellm_remaining_api_key_budget_metric gauge
litellm_remaining_api_key_budget_metric 0.0
# HELP litellm_api_key_max_budget_metric Maximum budget set for api key
# TYPE litellm_api_key_max_budget_metric gauge
litellm_api_key_max_budget_metric 0.0
# HELP litellm_api_key_budget_remaining_hours_metric Remaining hours for api key budget to be reset
# TYPE litellm_api_key_budget_remaining_hours_metric gauge
litellm_api_key_budget_remaining_hours_metric 0.0
# HELP litellm_remaining_api_key_requests_for_model Remaining Requests API Key can make for model (model based rpm limit on key)
# TYPE litellm_remaining_api_key_requests_for_model gauge
litellm_remaining_api_key_requests_for_model 0.0
# HELP litellm_remaining_api_key_tokens_for_model Remaining Tokens API Key can make for model (model based tpm limit on key)
# TYPE litellm_remaining_api_key_tokens_for_model gauge
litellm_remaining_api_key_tokens_for_model 0.0
# HELP litellm_remaining_requests LLM Deployment Analytics - remaining requests for model, returned from LLM API Provider
# TYPE litellm_remaining_requests gauge
litellm_remaining_requests 0.0
# HELP litellm_remaining_tokens remaining tokens for model, returned from LLM API Provider
# TYPE litellm_remaining_tokens gauge
litellm_remaining_tokens 0.0
# HELP litellm_overhead_latency_metric Latency overhead (milliseconds) added by LiteLLM processing
# TYPE litellm_overhead_latency_metric histogram
litellm_overhead_latency_metric_bucket{le="0.005",overhead_latency_metric="overhead_latency_metric"} 0.0
litellm_overhead_latency_metric_bucket{le="0.00625",overhead_latency_metric="overhead_latency_metric"} 0.0
litellm_overhead_latency_metric_bucket{le="0.0125",overhead_latency_metric="overhead_latency_metric"} 0.0
litellm_overhead_latency_metric_bucket{le="0.025",overhead_latency_metric="overhead_latency_metric"} 0.0
litellm_overhead_latency_metric_bucket{le="0.05",overhead_latency_metric="overhead_latency_metric"} 0.0
litellm_overhead_latency_metric_bucket{le="0.1",overhead_latency_metric="overhead_latency_metric"} 0.0
litellm_overhead_latency_metric_bucket{le="0.5",overhead_latency_metric="overhead_latency_metric"} 0.0
litellm_overhead_latency_metric_count{overhead_latency_metric="overhead_latency_metric"} 0.0
litellm_overhead_latency_metric_sum{overhead_latency_metric="overhead_latency_metric"} 0.0
# HELP litellm_provider_remaining_budget_metric Remaining budget for provider - used when you set provider budget limits
# TYPE litellm_provider_remaining_budget_metric gauge
litellm_provider_remaining_budget_metric 0.0
# HELP litellm_deployment_state LLM Deployment Analytics - The state of the deployment: 0 = healthy, 1 = partial outage, 2 = complete outage
# TYPE litellm_deployment_state gauge
litellm_deployment_state{api_base="",api_provider="None",litellm_model_name="",model_id=""} 1.0
# HELP litellm_deployment_cooled_down_total LLM Deployment Analytics - Number of times a deployment has been cooled down by LiteLLM load balancing logic. exception_status is the status of the exception that caused the deployment to be cooled down
# TYPE litellm_deployment_cooled_down_total counter
litellm_deployment_cooled_down_total 0.0
# HELP litellm_deployment_success_responses_total LLM Deployment Analytics - Total number of successful LLM API calls via litellm
# TYPE litellm_deployment_success_responses_total counter
litellm_deployment_success_responses_total 0.0
# HELP litellm_deployment_failure_responses_total LLM Deployment Analytics - Total number of failed LLM API calls for a specific LLM deploymeny. exception_status is the status of the exception from the llm api
# TYPE litellm_deployment_failure_responses_total counter
litellm_deployment_failure_responses_total{api_base="",api_key_alias="None",api_provider="None",exception_class="Exception",exception_status="None",hashed_api_key="",litellm_model_name="",model_id="",requested_model="",team="None",team_alias="None"} 7.0
# HELP litellm_deployment_failure_responses_created LLM Deployment Analytics - Total number of failed LLM API calls for a specific LLM deploymeny. exception_status is the status of the exception from the llm api
# TYPE litellm_deployment_failure_responses_created gauge
litellm_deployment_failure_responses_created{api_base="",api_key_alias="None",api_provider="None",exception_class="Exception",exception_status="None",hashed_api_key="",litellm_model_name="",model_id="",requested_model="",team="None",team_alias="None"} 1.74957313150608e+09
# HELP litellm_deployment_failure_by_tag_responses_total Total number of failed LLM API calls for a specific LLM deploymeny by custom metadata tags
# TYPE litellm_deployment_failure_by_tag_responses_total counter
litellm_deployment_failure_by_tag_responses_total 0.0
# HELP litellm_deployment_total_requests_total LLM Deployment Analytics - Total number of LLM API calls via litellm - success + failure
# TYPE litellm_deployment_total_requests_total counter
litellm_deployment_total_requests_total{api_base="",api_key_alias="None",api_provider="None",hashed_api_key="",litellm_model_name="",model_id="",requested_model="",team="None",team_alias="None"} 7.0
# HELP litellm_deployment_total_requests_created LLM Deployment Analytics - Total number of LLM API calls via litellm - success + failure
# TYPE litellm_deployment_total_requests_created gauge
litellm_deployment_total_requests_created{api_base="",api_key_alias="None",api_provider="None",hashed_api_key="",litellm_model_name="",model_id="",requested_model="",team="None",team_alias="None"} 1.7495731315060964e+09
# HELP litellm_deployment_latency_per_output_token LLM Deployment Analytics - Latency per output token
# TYPE litellm_deployment_latency_per_output_token histogram
litellm_deployment_latency_per_output_token_bucket{le="0.005",deployment_latency_per_output_token="deployment_latency_per_output_token"} 0.0
litellm_deployment_latency_per_output_token_bucket{le="0.00625",deployment_latency_per_output_token="deployment_latency_per_output_token"} 0.0
litellm_deployment_latency_per_output_token_bucket{le="0.0125",deployment_latency_per_output_token="deployment_latency_per_output_token"} 0.0
litellm_deployment_latency_per_output_token_bucket{le="0.025",deployment_latency_per_output_token="deployment_latency_per_output_token"} 0.0
litellm_deployment_latency_per_output_token_bucket{le="0.05",deployment_latency_per_output_token="deployment_latency_per_output_token"} 0.0
litellm_deployment_latency_per_output_token_bucket{le="0.1",deployment_latency_per_output_token="deployment_latency_per_output_token"} 0.0
litellm_deployment_latency_per_output_token_bucket{le="0.5",deployment_latency_per_output_token="deployment_latency_per_output_token"} 0.0
litellm_deployment_latency_per_output_token_count{deployment_latency_per_output_token="deployment_latency_per_output_token"} 0.0
litellm_deployment_latency_per_output_token_sum{deployment_latency_per_output_token="deployment_latency_per_output_token"} 0.0
# HELP litellm_deployment_successful_fallbacks_total LLM Deployment Analytics - Number of successful fallback requests from primary model -> fallback model
# TYPE litellm_deployment_successful_fallbacks_total counter
litellm_deployment_successful_fallbacks_total 0.0
# HELP litellm_deployment_failed_fallbacks_total LLM Deployment Analytics - Number of failed fallback requests from primary model -> fallback model
# TYPE litellm_deployment_failed_fallbacks_total counter
litellm_deployment_failed_fallbacks_total 0.0
# HELP litellm_llm_api_failed_requests_metric_total deprecated - use litellm_proxy_failed_requests_metric
# TYPE litellm_llm_api_failed_requests_metric_total counter
litellm_llm_api_failed_requests_metric_total{api_key_alias="None",end_user="None",hashed_api_key="",model="",team="None",team_alias="None",user="None"} 7.0
# HELP litellm_llm_api_failed_requests_metric_created deprecated - use litellm_proxy_failed_requests_metric
# TYPE litellm_llm_api_failed_requests_metric_created gauge
litellm_llm_api_failed_requests_metric_created{api_key_alias="None",end_user="None",hashed_api_key="",model="",team="None",team_alias="None",user="None"} 1.7495731315059812e+09
# HELP litellm_requests_metric_total deprecated - use litellm_proxy_total_requests_metric. Total number of LLM calls to litellm - track total per API Key, team, user
# TYPE litellm_requests_metric_total counter
litellm_requests_metric_total 0.0