## All options defined here are available to all instances.
##
## WARNING: To avoid blindly collecting offsets and lag for an unbounded number
## of partitions (as could be the case after enabling monitor_unlisted_consumer_groups
## or monitor_all_broker_highwatermarks) the check collects metrics for at most 500 partitions.
#
init_config:

    ## @param kafka_timeout - integer - optional - default: 5
    ## Customizes the Kafka connection timeout.
    #
    # kafka_timeout: 5

    ## @param service - string - optional
    ## Attach the tag `service:<SERVICE>` to every metric, event, and service check emitted by this integration.
    ##
    ## Additionally, this sets the default `service` for every log source.
    #
    # service: <SERVICE>

## Every instance is scheduled independently of the others.
#
instances:

    ## @param kafka_connect_str - string or list of strings - required
    ## Kafka endpoints and port to connect to.
    ##
    ## In a production environment, it's often useful to specify multiple
    ## Kafka nodes for a single check instance. This way you
    ## only generate a single check process, but if one host goes down,
    ## KafkaClient tries contacting the next host.
    ## Details: https://github.com/DataDog/dd-agent/issues/2943
    ##
    ## You may specify a single server like:
    ##
    ##   kafka_connect_str: server:9092
    ##
    ## or multiple servers like:
    ##
    ##   kafka_connect_str:
    ##   - server1:9092
    ##   - server2:9092
    #
  - kafka_connect_str: <KAFKA_CONNECT_STR>

    ## @param kafka_client_api_version - string - optional
    ## Specify the highest client protocol version supported by all brokers in the cluster.
    ##
    ## This is a performance optimization. If this is not set, then the check automatically probes
    ## the cluster for broker version during the connection bootstrapping process. Explicitly setting
    ## this bypasses that probe, saving 3-5 network calls depending on the broker version. Note that
    ## probing randomly picks a broker to probe, so in a mixed-version cluster, probing returns a
    ## non-deterministic result.
    #
    # kafka_client_api_version: 2.3.0

    ## @param consumer_groups - mapping - optional
    ## Each level is optional. Any empty values are fetched from the Kafka cluster.
    ## You can have empty partitions (example: <CONSUMER_NAME_2>), topics
    ## (example: <CONSUMER_NAME_3>),
    ## and even consumer_groups. If you do not specify `consumer_groups`,
    ## you must set `monitor_unlisted_consumer_groups` to true. 
    ##
    ## If both `consumer_groups` and `monitor_unlisted_consumer_groups` are used,
    ## then `consumer_groups` is ignored.
    #
    # consumer_groups:
    #   <CONSUMER_NAME_1>:
    #     <TOPIC_NAME_1>:
    #     - 0
    #     - 1
    #     - 4
    #     - 12
    #   <CONSUMER_NAME_2>:
    #     <TOPIC_NAME_2>: []
    #   <CONSUMER_NAME_3>: {}

    ## @param consumer_groups_regex - mapping - optional
    ## Set consumer groups and topics as regex strings.
    ## If both `consumer_groups_regex` and `consumer_groups` are filled out,
    ## both configuration options will be used. 
    ## If `monitor_unlisted_consumer_groups` is enabled, then
    ## `consumer_groups_regex` is ignored.
    #
    # consumer_groups_regex:
    #   <CONSUMER_NAME_1>:
    #     ^(?!<TOPIC_NAME_1>):
    #     - 0
    #     - 1
    #     - 4
    #     - 12
    #   my_cons.+:
    #     <TOPIC_NAME_2>: []
    #   test_consumer.+: {}

    ## @param monitor_unlisted_consumer_groups - boolean - optional - default: false
    ## Setting `monitor_unlisted_consumer_groups` to `true` tells the check to discover all consumer groups
    ## and fetch all their known offsets. If this is not set to true, you must specify `consumer_groups`.
    ##
    ## WARNING: This feature requires that your Kafka brokers be version >= 0.10.2. It is impossible to
    ## support this feature on older brokers because they do not provide a way to determine the mapping
    ## of consumer groups to topics. For details, see KIP-88. For older Kafka brokers, the consumer groups
    ## must be specified. This requirement only applies to the brokers, not the consumers--they can be any version.
    #
    # monitor_unlisted_consumer_groups: false

    ## @param consumer_queued_max_messages_kbytes - integer - optional - default: 1024
    ## The consumer very aggressively caches messages in the background (tuned for very high throughput).
    ## To reduce memory usage, tune down queued.max.messages.kbytes (maximum cache size per partition).
    ## Override the kafka default to 1MB for the integration check to optimize
    ## memory consumption to avoid potential out of memory (OOM) kill. (Default setting is 1GB per Kafka client)
    #
    # consumer_queued_max_messages_kbytes: 1024

    ## @param close_admin_client - boolean - optional - default: true
    ## Release AdminClient at the end of execution for garbage collection. Originally, we kept the same AdminClient 
    ## running over the entire life of the check. By deallocating the AdminClient after the check run, we should free 
    ## up any memory used in the client, although the performance of the check run would be slower (since we will 
    ## need to reconnect with a new client). Set this config option to false to improve performance.
    #
    # close_admin_client: true

    ## @param monitor_all_broker_highwatermarks - boolean - optional - default: false
    ## Setting monitor_all_broker_highwatermarks to `true` tells the check to
    ## discover and fetch the broker highwater mark offsets for all kafka topics in
    ## the cluster. Otherwise highwater mark offsets will only be fetched for topic
    ## partitions where that check run has already fetched a consumer offset. Internal
    ## Kafka topics like __consumer_offsets, __transaction_state, etc are always excluded.
    #
    # monitor_all_broker_highwatermarks: false

    ## @param security_protocol - string - optional - default: PLAINTEXT
    ## Protocol used to communicate with brokers.
    ## Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.
    ## Default: PLAINTEXT.
    #
    # security_protocol: PLAINTEXT

    ## @param sasl_mechanism - string - optional
    ## String picking sasl mechanism when security_protocol is SASL_PLAINTEXT or SASL_SSL.
    ## Valid values are: PLAIN, GSSAPI, OAUTHBEARER, SCRAM-SHA-256, SCRAM-SHA-512.
    #
    # sasl_mechanism: PLAIN

    ## @param sasl_plain_username - string - optional
    ## Username for sasl PLAIN or SCRAM authentication.
    #
    # sasl_plain_username: <USERNAME>

    ## @param sasl_plain_password - string - optional
    ## Password for sasl PLAIN or SCRAM authentication.
    #
    # sasl_plain_password: <PASSWORD>

    ## @param sasl_kerberos_keytab - string - optional
    ## Path to Kerberos keytab file.
    ## Note: If this config option is not used, the check will attempt to find
    ## the keytab file using the environment variable KRB5_CLIENT_KTNAME.
    ## Using `sasl_kerberos_keytab` will eventually be required and using the
    ## environment variable directly will be deprecated.
    #
    # sasl_kerberos_keytab: <SASL_KERBEROS_KEYTAB>

    ## @param sasl_kerberos_principal - string - optional - default: kafkaclient
    ## Kerberos principal to use.
    #
    # sasl_kerberos_principal: kafkaclient

    ## @param sasl_kerberos_service_name - string - optional - default: kafka
    ## Service name to include in GSSAPI sasl mechanism handshake.
    #
    # sasl_kerberos_service_name: kafka

    ## @param sasl_kerberos_domain_name - string - optional
    ## Kerberos domain name to use in GSSAPI sasl mechanism handshake.
    #
    # sasl_kerberos_domain_name: localhost

    ## Settings for when `sasl_mechanism` is set to `OAUTHBEARER`.
    #
    # sasl_oauth_token_provider:

        ## @param url - string - required
        ## The token endpoint.
        #
        # url: <URL>

        ## @param client_id - string - required
        ## The client identifier.
        #
        # client_id: <CLIENT_ID>

        ## @param client_secret - string - required
        ## The client secret.
        #
        # client_secret: <CLIENT_SECRET>

    ## @param tls_verify - boolean - optional - default: true
    ## Instructs the check to validate the TLS certificate(s) of the service(s).
    #
    # tls_verify: true

    ## @param tls_ca_cert - string - optional
    ## The path to a file of concatenated CA certificates in PEM format or a directory
    ## containing several CA certificates in PEM format. If a directory, the directory
    ## must have been processed using the c_rehash utility supplied with OpenSSL. See:
    ## https://www.openssl.org/docs/manmaster/man3/SSL_CTX_load_verify_locations.html
    ##
    ## Setting this implicitly sets `tls_verify` to true.
    #
    # tls_ca_cert: <CA_CERT_PATH>

    ## @param tls_cert - string - optional
    ## The path to a single file in PEM format containing a certificate as well as any
    ## number of CA certificates needed to establish the certificate's authenticity for
    ## use when connecting to services. It may also contain an unencrypted private key to use.
    ##
    ## Setting this implicitly sets `tls_verify` to true.
    #
    # tls_cert: <CERT_PATH>

    ## @param tls_private_key - string - optional
    ## The unencrypted private key to use for `tls_cert` when connecting to services. This is
    ## required if `tls_cert` is set and it does not already contain a private key.
    ##
    ## Setting this implicitly sets `tls_verify` to true.
    #
    # tls_private_key: <PRIVATE_KEY_PATH>

    ## @param tls_private_key_password - string - optional
    ## Optional password to decrypt tls_private_key.
    ##
    ## Setting this implicitly sets `tls_verify` to true.
    #
    # tls_private_key_password: <PRIVATE_KEY_PASSWORD>

    ## @param tls_validate_hostname - boolean - optional - default: true
    ## Verifies that the server's cert hostname matches the one requested.
    #
    # tls_validate_hostname: true

    ## @param tls_crlfile - string - optional
    ## Filename path containing the CRL to check for certificate expiration.
    ## By default, no CRL check is done. When providing a file, only the leaf certificate
    ## will be checked against this CRL.
    #
    # tls_crlfile: <SSL_FILE_PATH>

    ## @param tags - list of strings - optional
    ## A list of tags to attach to every metric and service check emitted by this instance.
    ##
    ## Learn more about tagging at https://docs.datadoghq.com/tagging
    #
    # tags:
    #   - <KEY_1>:<VALUE_1>
    #   - <KEY_2>:<VALUE_2>

    ## @param service - string - optional
    ## Attach the tag `service:<SERVICE>` to every metric, event, and service check emitted by this integration.
    ##
    ## Overrides any `service` defined in the `init_config` section.
    #
    # service: <SERVICE>

    ## @param min_collection_interval - number - optional - default: 15
    ## This changes the collection interval of the check. For more information, see:
    ## https://docs.datadoghq.com/developers/write_agent_check/#collection-interval
    #
    # min_collection_interval: 15

    ## @param empty_default_hostname - boolean - optional - default: false
    ## This forces the check to send metrics with no hostname.
    ##
    ## This is useful for cluster-level checks.
    #
    # empty_default_hostname: false

    ## @param metric_patterns - mapping - optional
    ## A mapping of metrics to include or exclude, with each entry being a regular expression.
    ##
    ## Metrics defined in `exclude` will take precedence in case of overlap.
    #
    # metric_patterns:
    #   include:
    #   - <INCLUDE_REGEX>
    #   exclude:
    #   - <EXCLUDE_REGEX>
