init_config:
  # Customize the ZooKeeper connection timeout here
  # zk_timeout: 5
  # Customize the Kafka connection timeout here
  # kafka_timeout: 5
  # Customize max number of retries per failed query to Kafka
  # kafka_retries: 3
  # Customize the number of seconds that must elapse between running this check.
  # When checking Kafka offsets stored in Zookeeper, a single run of this check
  # must stat zookeeper more than the number of consumers * topic_partitions
  # that you're monitoring. If that number is greater than 100, it's recommended
  # to increase this value to avoid hitting zookeeper too hard.
  # https://docs.datadoghq.com/agent/faq/how-do-i-change-the-frequency-of-an-agent-check/
  # min_collection_interval: 600
  #
  # Please note that to avoid blindly collecting offsets and lag for an
  # unbounded number of partitions (as could be the case after introducing
  # the self discovery of consumer groups, topics and partitions) the check
  # will collect at metrics for at most 200 partitions.


instances:
  # In a production environment, it's often useful to specify multiple
  # Kafka / Zookeper nodes for a single check instance. This way you
  # only generate a single check process, but if one host goes down,
  # KafkaClient / KazooClient will try contacting the next host.
  # Details: https://github.com/DataDog/dd-agent/issues/2943
  #
  # If you wish to only collect consumer offsets from kafka, because
  # you're using the new style consumers, you can comment out all
  # zk_* configuration elements below.
  # Please note that unlisted consumer groups are not supported at
  # the moment when zookeeper consumer offset collection is disabled.
  - kafka_connect_str:
      - localhost:9092
      - another_kafka_broker:9092
    zk_connect_str:
      - localhost:2181
      - another_zookeeper:2181
    # zk_iteration_ival: 1  # how many seconds between ZK consumer offset
                            # collections. If kafka consumer offsets disabled
                            # this has no effect.
    # zk_prefix: /0.8

    ### SASL Username/Password Configuration ###

    ## @param security_protocol - string - required
    ## Protocol used to communicate with brokers. 
    ## Valid values are: PLAINTEXT, SSL. Default: PLAINTEXT.
    #
    security_protocol: PLAINTEXT
    
    ## @param sasl_mechanism - string - optional
    ## String picking sasl mechanism when security_protocol is SASL_PLAINTEXT or SASL_SSL. 
    ## Currently only PLAIN is supported.
    #
    # sasl_mechanism: SASL_PLAINTEXT

    ## @param sasl_plain_username - string - optional
    ## Username for sasl PLAIN authentication.
    #
    # sasl_plain_username: username

    ## @param sasl_plain_password - string - optional
    ## Password for sasl PLAIN authentication.
    #
    # sasl_plain_password: password

    ## @param sasl_kerberos_service_name - string - optional - default:kafka
    ## Service name to include in GSSAPI sasl mechanism handshake.
    #
    # sasl_kerberos_service_name: kafka 

    ## @param sasl_kerberos_domain_name - string - optional - default:one of the bootstrap servers
    ## Kerberos domain name to use in GSSAPI sasl mechanism handshake. 
    #
    # sasl_kerberos_domain_name: localhost

    ### SSL Configuration ###

    ## @param ssl_context - string - optional
    ## Pre-configured SSLContext for wrapping socket connections. 
    ## If provided, all other ssl_* configurations will be ignored. 
    #
    # ssl_context: 

    ## @param ssl_check_hostname - string - optional - default:True
    ## Flag to configure whether SSL handshake should verify that the 
    ## certificate matches the broker’s hostname.
    #
    # ssl_check_hostname: True

    ## @param ssl_cafile - string - optional
    ## Optional filename of CA file to use in certificate verification.
    #
    # ssl_cafile: /path/to/pem/file

    ## @param ssl_certfile - string - optional
    ## Optional filename of file in PEM format containing the client certificate, 
    ## as well as any CA certificates needed to establish the certificate’s authenticity.
    #
    # ssl_certfile: /path/to/pem/file

    ## @param ssl_keyfile - string - optional
    ## Optional filename containing the client private key.
    #
    # ssl_keyfile: /path/to/key/file

    ## @param ssl_password - string - optional
    ## Optional password to be used when loading the certificate chain. 
    #
    # ssl_password: password1

    ## @param ssl_crlfile - string - optional
    ## Optional filename containing the CRL to check for certificate expiration.
    ## By default, no CRL check is done. When providing a file, only the leaf certificate
    ## will be checked against this CRL. The CRL can only be checked with Python 3.4+ or 2.7.9+. 
    #
    # ssl_crlfile:

    # kafka_consumer_offsets: false
    consumer_groups:
      my_consumer:  # consumer group name
        my_topic: [0, 1, 4, 12]  # topic_name: list of partitions
      # Note that each level of values is optional (this is currently only available
      # when using zookeeper to store consumer groups). Any omitted values will be
      # fetched from Zookeeper. You can omit partitions (example: myconsumer2),
      # topics (example: myconsumer3), and even consumer_groups. If you omit
      # consumer_groups, you must set 'monitor_unlisted_consumer_groups': True.
      # If a value is omitted, the parent value must still be it's expected type,
      # which is typically a dict.
      myconsumer2:
        mytopic0:
      myconsumer3:
    # Setting monitor_unlisted_consumer_groups to True will tell the check to
    # discover and fetch all offsets for all consumer groups stored in zookeeper.
    # While this is convenient, it can also put a lot of load on zookeeper, so
    # use judiciously.
    monitor_unlisted_consumer_groups: False
    # Setting tags will allow custom tags to be sent with metrics.
    # tags:
    #   - optional:tag1
