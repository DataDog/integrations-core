## WARNING: To avoid blindly collecting offsets and lag for an unbounded number
## of partitions (as could be the case after enabling monitor_unlisted_consumer_groups
## or monitor_all_broker_highwatermarks) the check collects metrics for at most 500 partitions.

## DEPRECATION NOTICE: In the early days of Kafka, consumer offsets were stored in Zookeeper.
## So this check currently supports fetching consumer offsets from both Kafka and Zookeeper.
## However, Kafka 0.9 (released in 2015) deprecated Zookeeper storage. As a result, we have also
## deprecated fetching consumer offsets from Zookeeper and at some point in the future that
## functionality will be removed from this check.

init_config:

  ## @param kafka_timeout - integer - optional - default: 5
  ## Customizes the Kafka connection timeout.
  #
  # kafka_timeout: 5

  ## @param zk_timeout - integer - optional - default: 5
  ## DEPRECATED: Customizes the ZooKeeper connection timeout.
  #
  # zk_timeout: 5

instances:

    ## @param kafka_connect_str - list of strings - required
    ## Kafka endpoints and port to connect to.
    ##
    ## In a production environment, it's often useful to specify multiple
    ## Kafka nodes for a single check instance. This way you
    ## only generate a single check process, but if one host goes down,
    ## KafkaClient tries contacting the next host.
    ## Details: https://github.com/DataDog/dd-agent/issues/2943
    #
  - kafka_connect_str:
    - localhost:9092
    # - <KAFKA_BROKER_ENDPOINT>:9092

    ## @param kafka_client_api_version - string - optional
    ## Specify the highest client protocol version supported by all brokers in the cluster.
    ##
    ## This is a performance optimization. If this is not set, then the check automatically probes
    ## the cluster for broker version during the connection bootstrapping process. Explicitly setting
    ## this bypasses that probe, saving 3-5 network calls depending on the broker version. Note that
    ## probing randomly picks a broker to probe, so in a mixed-version cluster, probing returns a
    ## non-deterministic result.
    #
    # kafka_client_api_version: "2.3.0"

    ## @param consumer_groups - object - optional
    ## Each level is optional. Any empty values are fetched from the Kafka cluster.
    ## You can have empty partitions (example: <CONSUMER_NAME_2>), topics (example: <CONSUMER_NAME_3>),
    ## and even consumer_groups. If you omit consumer_groups, you must set `monitor_unlisted_consumer_groups` to true.
    ##
    ## Deprecation notice: Omitting various levels works for zookeeper-based consumers. However, all
    ## functionality related to fetching offsets from Zookeeper is deprecated and will be removed from this
    ## check in the future.
    #
    # consumer_groups:
    #   <CONSUMER_NAME_1>:
    #     <TOPIC_NAME_1>: [0, 1, 4, 12]
    #   <CONSUMER_NAME_2>:
    #     <TOPIC_NAME_2>: []
    #   <CONSUMER_NAME_3>: {}

    ## @param monitor_unlisted_consumer_groups - boolean - optional
    ## Setting monitor_unlisted_consumer_groups to `true` tells the check to discover all consumer groups
    ## and fetch all their known offsets. If this is not set to true, you must specify consumer_groups.
    ##
    ## WARNING: This feature requires that your Kafka brokers be version >= 0.10.2. It is impossible to
    ## support this feature on older brokers because they do not provide a way to determine the mapping
    ## of consumer groups to topics. For details, see KIP-88. For older Kafka brokers, the consumer groups
    ## must be specified. This requirement only applies to the brokers, not the consumers--they can be any version.
    ##
    ## Deprecation notice: This feature works for Zookeeper-based consumers. However, all
    ## functionality related to fetching offsets from Zookeeper is deprecated and will be removed from this
    ## check in the future.
    #
    # monitor_unlisted_consumer_groups: false

    ## @param monitor_all_broker_highwatermarks - boolean - optional
    ## Setting monitor_all_broker_highwatermarks to `true` tells the check to
    ## discover and fetch the broker highwater mark offsets for all kafka topics in
    ## the cluster. Otherwise highwater mark offsets will only be fetched for topic
    ## partitions where that check run has already fetched a consumer offset. Internal
    ## Kafka topics like __consumer_offsets, __transaction_state, etc are always excluded.
    #
    # monitor_all_broker_highwatermarks: false

    ## @param tags - list of key:value string - optional
    ## List of tags to attach to every metric and service check emitted by this integration.
    ##
    ## Learn more about tagging at https://docs.datadoghq.com/tagging
    #
    # tags:
    #   - <KEY_1>:<VALUE_1>
    #   - <KEY_2>:<VALUE_2>

    ## @param security_protocol - string - optional
    ## Protocol used to communicate with brokers.
    ## Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.
    ## Default: PLAINTEXT.
    #
    # security_protocol: PLAINTEXT

    ## @param sasl_mechanism - string - optional
    ## String picking sasl mechanism when security_protocol is SASL_PLAINTEXT or SASL_SSL.
    ## Valid values are: PLAIN, GSSAPI, OAUTHBEARER, SCRAM-SHA-256, SCRAM-SHA-512.
    #
    # sasl_mechanism: PLAIN

    ## @param sasl_plain_username - string - optional
    ## Username for sasl PLAIN or SCRAM authentication.
    #
    # sasl_plain_username: <USERNAME>

    ## @param sasl_plain_password - string - optional
    ## Password for sasl PLAIN or SCRAM authentication.
    #
    # sasl_plain_password: <PASSWORD>

    ## @param sasl_kerberos_service_name - string - optional - default: kafka
    ## Service name to include in GSSAPI sasl mechanism handshake.
    #
    # sasl_kerberos_service_name: kafka

    ## @param sasl_kerberos_domain_name - string - optional - default: one of the bootstrap servers
    ## Kerberos domain name to use in GSSAPI sasl mechanism handshake.
    #
    # sasl_kerberos_domain_name: localhost

    ## @param ssl_context - string - optional
    ## Pre-configured SSLContext for wrapping socket connections.
    ## If provided, all other ssl_* configurations are ignored.
    #
    # ssl_context: <SSL_CONTEXT>

    ## @param ssl_check_hostname - string - optional - default: true
    ## Flag to configure whether SSL handshake should verify that the
    ## certificate matches the broker’s hostname.
    #
    # ssl_check_hostname: true

    ## @param ssl_cafile - string - optional
    ## Filename of CA file path to use in certificate verification.
    #
    # ssl_cafile: <CA_FILE_PATH>

    ## @param ssl_certfile - string - optional
    ## Filename path of file in PEM format containing the client certificate,
    ## as well as any CA certificates needed to establish the certificate’s authenticity.
    #
    # ssl_certfile: <CERT_FILE_PATH>

    ## @param ssl_keyfile - string - optional
    ## Optional filename containing the client private key.
    #
    # ssl_keyfile: <KEY_FILE_PATH>

    ## @param ssl_password - string - optional
    ## Password to be used when loading the certificate chain.
    #
    # ssl_password: <PASSWORD>

    ## @param ssl_crlfile - string - optional
    ## Filename path containing the CRL to check for certificate expiration.
    ## By default, no CRL check is done. When providing a file, only the leaf certificate
    ## will be checked against this CRL. The CRL can only be checked with Python 3.4+ or 2.7.9+.
    #
    # ssl_crlfile: <SSL_FILE_PATH>

    ## DEPRECATED:
    ## The following settings are only used when fetching consumer offsets from Zookeeper.
    ## This functionality is deprecated and will be removed at some point in the future.

    ## @param zk_connect_str - list of objects - required
    ## Deprecated: Zookeeper endpoints and port to connect to.
    ## In a production environment, it's often useful to specify multiple
    ## Zookeeper nodes for a single check instance. This way you
    ## only generate a single check process, but if one host goes down,
    ## KafkaClient / KazooClient tries contacting the next host.
    ## Details: https://github.com/DataDog/dd-agent/issues/2943
    #
    # zk_connect_str:
    # - localhost:2181
    # - <ZOOKEEPER_ENDPOINT>:2181

    ## @param zk_prefix - string - optional
    ## Deprecated: Zookeeper chroot prefix under which kafka data is living in zookeeper.
    ## If kafka is connecting to `my-zookeeper:2181/kafka` then the `zk_prefix` is `/kafka`.
    #
    # zk_prefix: <ZK_PREFIX>

    ## @param kafka_consumer_offsets - boolean - optional - default: false
    ## Deprecated: This setting only applies if zk_connect_str is set
    ## Set to true to fetch consumer offsets from both Zookeeper and Kafka
    ## Set to false to fetch consumer offsets only from Zookeeper.
    #
    # kafka_consumer_offsets: false
