## All options defined here are available to all instances.
##
## WARNING: To avoid blindly collecting offsets and lag for an unbounded number
## of partitions (as could be the case after enabling monitor_unlisted_consumer_groups
## or monitor_all_broker_highwatermarks) the check collects metrics for at most 500 partitions.
##
## DEPRECATION NOTICE: In the early days of Kafka, consumer offsets were stored in Zookeeper.
## So this check currently supports fetching consumer offsets from both Kafka and Zookeeper.
## However, Kafka 0.9 (released in 2015) deprecated Zookeeper storage. As a result, we have also
## deprecated fetching consumer offsets from Zookeeper.
#
init_config:

    ## @param kafka_timeout - integer - optional - default: 5
    ## Customizes the Kafka connection timeout.
    #
    # kafka_timeout: 5

    ## @param zk_timeout - integer - optional - default: 5
    ## DEPRECATED: Customizes the ZooKeeper connection timeout.
    #
    # zk_timeout: 5

    ## @param service - string - optional
    ## Attach the tag `service:<SERVICE>` to every metric, event, and service check emitted by this integration.
    ##
    ## Additionally, this sets the default `service` for every log source.
    #
    # service: <SERVICE>

## Every instance is scheduled independent of the others.
#
instances:

    ## @param kafka_connect_str - string or list of strings - required
    ## Kafka endpoints and port to connect to.
    ##
    ## In a production environment, it's often useful to specify multiple
    ## Kafka nodes for a single check instance. This way you
    ## only generate a single check process, but if one host goes down,
    ## KafkaClient tries contacting the next host.
    ## Details: https://github.com/DataDog/dd-agent/issues/2943
    ##
    ## You may specify a single server like:
    ##
    ##   kafka_connect_str: server:9092
    ##
    ## or multiple servers like:
    ##
    ##   kafka_connect_str:
    ##   - server1:9092
    ##   - server2:9092
    #
  - kafka_connect_str: <KAFKA_CONNECT_STR>

    ## @param kafka_client_api_version - string - optional
    ## Specify the highest client protocol version supported by all brokers in the cluster.
    ##
    ## This is a performance optimization. If this is not set, then the check automatically probes
    ## the cluster for broker version during the connection bootstrapping process. Explicitly setting
    ## this bypasses that probe, saving 3-5 network calls depending on the broker version. Note that
    ## probing randomly picks a broker to probe, so in a mixed-version cluster, probing returns a
    ## non-deterministic result.
    #
    # kafka_client_api_version: 2.3.0

    ## @param consumer_groups - mapping - optional
    ## Each level is optional. Any empty values are fetched from the Kafka cluster.
    ## You can have empty partitions (example: <CONSUMER_NAME_2>), topics (example: <CONSUMER_NAME_3>),
    ## and even consumer_groups. If you omit consumer_groups, you must set `monitor_unlisted_consumer_groups` to true.
    ##
    ## Deprecation notice: Omitting various levels works for zookeeper-based consumers. However, all
    ## functionality related to fetching offsets from Zookeeper is deprecated.
    #
    # consumer_groups:
    #   <CONSUMER_NAME_1>:
    #     <TOPIC_NAME_1>:
    #     - 0
    #     - 1
    #     - 4
    #     - 12
    #   <CONSUMER_NAME_2>:
    #     <TOPIC_NAME_2>: []
    #   <CONSUMER_NAME_3>: {}

    ## @param monitor_unlisted_consumer_groups - boolean - optional - default: false
    ## Setting monitor_unlisted_consumer_groups to `true` tells the check to discover all consumer groups
    ## and fetch all their known offsets. If this is not set to true, you must specify consumer_groups.
    ##
    ## WARNING: This feature requires that your Kafka brokers be version >= 0.10.2. It is impossible to
    ## support this feature on older brokers because they do not provide a way to determine the mapping
    ## of consumer groups to topics. For details, see KIP-88. For older Kafka brokers, the consumer groups
    ## must be specified. This requirement only applies to the brokers, not the consumers--they can be any version.
    ##
    ## Deprecation notice: Functionality related to consumers fetching offsets from Zookeeper is deprecated.
    #
    # monitor_unlisted_consumer_groups: false

    ## @param monitor_all_broker_highwatermarks - boolean - optional - default: false
    ## Setting monitor_all_broker_highwatermarks to `true` tells the check to
    ## discover and fetch the broker highwater mark offsets for all kafka topics in
    ## the cluster. Otherwise highwater mark offsets will only be fetched for topic
    ## partitions where that check run has already fetched a consumer offset. Internal
    ## Kafka topics like __consumer_offsets, __transaction_state, etc are always excluded.
    #
    # monitor_all_broker_highwatermarks: false

    ## @param security_protocol - string - optional - default: PLAINTEXT
    ## Protocol used to communicate with brokers.
    ## Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.
    ## Default: PLAINTEXT.
    #
    # security_protocol: PLAINTEXT

    ## @param sasl_mechanism - string - optional
    ## String picking sasl mechanism when security_protocol is SASL_PLAINTEXT or SASL_SSL.
    ## Valid values are: PLAIN, GSSAPI, OAUTHBEARER, SCRAM-SHA-256, SCRAM-SHA-512.
    #
    # sasl_mechanism: PLAIN

    ## @param sasl_plain_username - string - optional
    ## Username for sasl PLAIN or SCRAM authentication.
    #
    # sasl_plain_username: <USERNAME>

    ## @param sasl_plain_password - string - optional
    ## Password for sasl PLAIN or SCRAM authentication.
    #
    # sasl_plain_password: <PASSWORD>

    ## @param sasl_kerberos_service_name - string - optional - default: kafka
    ## Service name to include in GSSAPI sasl mechanism handshake.
    #
    # sasl_kerberos_service_name: kafka

    ## @param sasl_kerberos_domain_name - string - optional
    ## Kerberos domain name to use in GSSAPI sasl mechanism handshake.
    #
    # sasl_kerberos_domain_name: localhost

    ## @param tls_verify - boolean - optional - default: true
    ## Instructs the check to validate the TLS certificate(s) of the service(s).
    #
    # tls_verify: true

    ## @param tls_ca_cert - string - optional
    ## The path to a file of concatenated CA certificates in PEM format or a directory
    ## containing several CA certificates in PEM format. If a directory, the directory
    ## must have been processed using the c_rehash utility supplied with OpenSSL. See:
    ## https://www.openssl.org/docs/manmaster/man3/SSL_CTX_load_verify_locations.html
    ##
    ## Setting this implicitly sets `tls_verify` to true.
    #
    # tls_ca_cert: <CA_CERT_PATH>

    ## @param tls_cert - string - optional
    ## The path to a single file in PEM format containing a certificate as well as any
    ## number of CA certificates needed to establish the certificate's authenticity for
    ## use when connecting to services. It may also contain an unencrypted private key to use.
    ##
    ## Setting this implicitly sets `tls_verify` to true.
    #
    # tls_cert: <CERT_PATH>

    ## @param tls_private_key - string - optional
    ## The unencrypted private key to use for `tls_cert` when connecting to services. This is
    ## required if `tls_cert` is set and it does not already contain a private key.
    ##
    ## Setting this implicitly sets `tls_verify` to true.
    #
    # tls_private_key: <PRIVATE_KEY_PATH>

    ## @param tls_private_key_password - string - optional
    ## Optional password to decrypt tls_private_key.
    ##
    ## Setting this implicitly sets `tls_verify` to true.
    #
    # tls_private_key_password: <PRIVATE_KEY_PASSWORD>

    ## @param tls_validate_hostname - boolean - optional - default: true
    ## Verifies that the server's cert hostname matches the one requested.
    #
    # tls_validate_hostname: true

    ## @param tls_crlfile - string - optional
    ## Filename path containing the CRL to check for certificate expiration.
    ## By default, no CRL check is done. When providing a file, only the leaf certificate
    ## will be checked against this CRL.
    #
    # tls_crlfile: <SSL_FILE_PATH>

    ## @param broker_requests_batch_size - integer - optional - default: 30
    ## The OffsetRequests sent to each broker are batched by the kafka_consumer check in groups of 30 by default.
    ## If the batch size is too big, you may see KafkaTimeoutError exceptions in the logs while
    ## running the wakeup calls.
    ## If the batch size is too small, the check will take longer to run.
    #
    # broker_requests_batch_size: 30

    ## @param tags - list of strings - optional
    ## A list of tags to attach to every metric and service check emitted by this instance.
    ##
    ## Learn more about tagging at https://docs.datadoghq.com/tagging
    #
    # tags:
    #   - <KEY_1>:<VALUE_1>
    #   - <KEY_2>:<VALUE_2>

    ## @param service - string - optional
    ## Attach the tag `service:<SERVICE>` to every metric, event, and service check emitted by this integration.
    ##
    ## Overrides any `service` defined in the `init_config` section.
    #
    # service: <SERVICE>

    ## @param min_collection_interval - number - optional - default: 15
    ## This changes the collection interval of the check. For more information, see:
    ## https://docs.datadoghq.com/developers/write_agent_check/#collection-interval
    #
    # min_collection_interval: 15

    ## @param empty_default_hostname - boolean - optional - default: false
    ## This forces the check to send metrics with no hostname.
    ##
    ## This is useful for cluster-level checks.
    #
    # empty_default_hostname: false

    ## @param zk_connect_str - string or list of strings - optional
    ## DEPRECATION NOTICE: This option is only used for fetching consumer offsets
    ## from Zookeeper and is deprecated.
    ## Zookeeper endpoints and port to connect to.
    ## In a production environment, it's often useful to specify multiple
    ## Zookeeper nodes for a single check instance. This way you
    ## only generate a single check process, but if one host goes down,
    ## KafkaClient / KazooClient tries contacting the next host.
    ## Details: https://github.com/DataDog/dd-agent/issues/2943
    ##
    ## You may specify a single server like:
    ##
    ##   zk_connect_str: localhost:2181
    ##
    ## or multiple servers like:
    ##
    ##   zk_connect_str:
    ##   - server1:2181
    ##   - server2:2181
    #
    # zk_connect_str: <ZK_CONNECT_STR>

    ## @param zk_prefix - string - optional
    ## DEPRECATION NOTICE: This option is only used for fetching consumer offsets
    ## from Zookeeper and is deprecated.
    ## Zookeeper chroot prefix under which kafka data is living in zookeeper.
    ## If kafka is connecting to `my-zookeeper:2181/kafka` then the `zk_prefix` is `/kafka`.
    #
    # zk_prefix: <ZK_PREFIX>

    ## @param kafka_consumer_offsets - boolean - optional - default: false
    ## DEPRECATION NOTICE: This option is only used for fetching consumer offsets
    ## from Zookeeper and is deprecated.
    ## This setting only applies if `zk_connect_str` is set and cannot work with
    ## `monitor_unlisted_consumer_groups` since the generated list comes from Zookeeper.
    ## Set to true to fetch consumer offsets from both Zookeeper and Kafka
    ## Set to false to fetch consumer offsets only from Zookeeper.
    #
    # kafka_consumer_offsets: false
