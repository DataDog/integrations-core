name: Kafka Consumer
files:
- name: kafka_consumer.yaml
  options:
  - template: init_config
    overrides:
      description: |
        All options defined here are available to all instances.

        WARNING: To avoid blindly collecting offsets and lag for an unbounded number
        of partitions (as could be the case after enabling monitor_unlisted_consumer_groups
        or monitor_all_broker_highwatermarks) the check collects metrics for at most 500 partitions.
    options:
    - name: kafka_timeout
      description: Customizes the Kafka connection timeout.
      value:
        type: integer
        example: 5
      display_default: 5
    - template: init_config/default
  - template: instances
    options:
      - name: kafka_connect_str
        description: |
          Kafka endpoints and port to connect to.

          In a production environment, it's often useful to specify multiple
          Kafka nodes for a single check instance. This way you
          only generate a single check process, but if one host goes down,
          KafkaClient tries contacting the next host.
          Details: https://github.com/DataDog/dd-agent/issues/2943

          You may specify a single server like:

            kafka_connect_str: server:9092

          or multiple servers like:

            kafka_connect_str:
            - server1:9092
            - server2:9092
        required: true
        value:
          anyOf:
          - type: string
          - type: array
            items:
              type: string
      - name: kafka_client_api_version
        description: |
          Specify the highest client protocol version supported by all brokers in the cluster.

          This is a performance optimization. If this is not set, then the check automatically probes
          the cluster for broker version during the connection bootstrapping process. Explicitly setting
          this bypasses that probe, saving 3-5 network calls depending on the broker version. Note that
          probing randomly picks a broker to probe, so in a mixed-version cluster, probing returns a
          non-deterministic result.
        value:
          type: string
          example: "2.3.0"
          display_default: null
      - name: consumer_groups
        description: |
          Each level is optional. Any empty values are fetched from the Kafka cluster.
          You can have empty partitions (example: <CONSUMER_NAME_2>), topics
          (example: <CONSUMER_NAME_3>),
          and even consumer_groups. If you do not specify `consumer_groups`,
          you must set `monitor_unlisted_consumer_groups` to true. 
          
          If both `consumer_groups` and `monitor_unlisted_consumer_groups` are used,
          then `consumer_groups` is ignored.
        value:
          type: object
          example:
            <CONSUMER_NAME_1>:
              <TOPIC_NAME_1>: [0, 1, 4, 12]
            <CONSUMER_NAME_2>:
              <TOPIC_NAME_2>: []
            <CONSUMER_NAME_3>: {}
      - name: consumer_groups_regex
        description: |
          Set consumer groups and topics as regex strings.
          If both `consumer_groups_regex` and `consumer_groups` are filled out,
          both configuration options will be used. 
          If `monitor_unlisted_consumer_groups` is enabled, then
          `consumer_groups_regex` is ignored.
        value:
          type: object
          example:
            <CONSUMER_NAME_1>:
              ^(?!<TOPIC_NAME_1>): [0, 1, 4, 12]
            my_cons.+:
              <TOPIC_NAME_2>: []
            test_consumer.+: {}
      - name: monitor_unlisted_consumer_groups
        description: |
          Setting `monitor_unlisted_consumer_groups` to `true` tells the check to discover all consumer groups
          and fetch all their known offsets. If this is not set to true, you must specify `consumer_groups`.

          WARNING: This feature requires that your Kafka brokers be version >= 0.10.2. It is impossible to
          support this feature on older brokers because they do not provide a way to determine the mapping
          of consumer groups to topics. For details, see KIP-88. For older Kafka brokers, the consumer groups
          must be specified. This requirement only applies to the brokers, not the consumers--they can be any version.
        value:
          type: boolean
          example: false
      - name: consumer_queued_max_messages_kbytes
        description: |
          The consumer very aggressively caches messages in the background (tuned for very high throughput).
          To reduce memory usage, tune down queued.max.messages.kbytes (maximum cache size per partition).
          Override the kafka default to 1MB for the integration check to optimize
          memory consumption to avoid potential out of memory (OOM) kill. (Default setting is 1GB per Kafka client)
        value:
          type: integer
          example: 1024
      - name: close_admin_client
        description: |
          Release AdminClient at the end of execution for garbage collection. Originally, we kept the same AdminClient 
          running over the entire life of the check. By deallocating the AdminClient after the check run, we should free 
          up any memory used in the client, although the performance of the check run would be slower (since we will 
          need to reconnect with a new client). Set this config option to false to improve performance.
        value:
          type: boolean
          example: true
      - name: monitor_all_broker_highwatermarks
        description: |
          Setting monitor_all_broker_highwatermarks to `true` tells the check to
          discover and fetch the broker highwater mark offsets for all kafka topics in
          the cluster. Otherwise highwater mark offsets will only be fetched for topic
          partitions where that check run has already fetched a consumer offset. Internal
          Kafka topics like __consumer_offsets, __transaction_state, etc are always excluded.
        value:
          type: boolean
          example: false
      - name: security_protocol
        description: |
          Protocol used to communicate with brokers.
          Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.
          Default: PLAINTEXT.
        display_default: PLAINTEXT
        value:
          type: string
          example: PLAINTEXT
      - name: sasl_mechanism
        description: |
          String picking sasl mechanism when security_protocol is SASL_PLAINTEXT or SASL_SSL.
          Valid values are: PLAIN, GSSAPI, OAUTHBEARER, SCRAM-SHA-256, SCRAM-SHA-512.
        value:
          type: string
          example: PLAIN
          display_default: null
      - name: sasl_plain_username
        description: Username for sasl PLAIN or SCRAM authentication.
        value:
          type: string
          example: <USERNAME>
      - name: sasl_plain_password
        description: Password for sasl PLAIN or SCRAM authentication.
        value:
          type: string
          example: <PASSWORD>
      - name: sasl_kerberos_keytab
        description: |
          Path to Kerberos keytab file.
          Note: If this config option is not used, the check will attempt to find
          the keytab file using the environment variable KRB5_CLIENT_KTNAME.
          Using `sasl_kerberos_keytab` will eventually be required and using the
          environment variable directly will be deprecated.
        value:
          type: string
      - name: sasl_kerberos_principal
        description: |
          Kerberos principal to use.
        value:
          type: string
          example: kafkaclient
      - name: sasl_kerberos_service_name
        description: Service name to include in GSSAPI sasl mechanism handshake.
        value:
          type: string
          example: kafka
      - name: sasl_kerberos_domain_name
        description: Kerberos domain name to use in GSSAPI sasl mechanism handshake.
        value:
          type: string
          example: localhost
          display_default: null
      - name: sasl_oauth_token_provider
        description: |
          Settings for when `sasl_mechanism` is set to `OAUTHBEARER`.
        options:
        - name: url
          required: true
          enabled: false
          description: |
            The token endpoint.
          value:
            type: string
        - name: client_id
          required: true
          enabled: false
          description: |
            The client identifier.
          value:
            type: string
        - name: client_secret
          secret: true
          required: true
          enabled: false
          description: |
            The client secret.
          value:
            type: string
      - template: instances/tls
      - name: tls_crlfile
        description: |
          Filename path containing the CRL to check for certificate expiration.
          By default, no CRL check is done. When providing a file, only the leaf certificate
          will be checked against this CRL.
        value:
          type: string
          example: <SSL_FILE_PATH>
      - template: instances/default
