init_config:
  ## @param global_custom_queries - list - optional
  ## See `custom_queries` defined below.
  ##
  ## Global custom queries can be applied to all instances using the
  ## `use_global_custom_queries` setting at the instance level.
  #
  # global_custom_queries:
  #   - metric_prefix: ibm_db2
  #     query: <QUERY>
  #     columns: <COLUMNS>
  #     tags: <TAGS>

instances:
    ## @param db - string - required
    ## The name of the database to establish a connection with.
    #
  - db: <DATABASE_NAME>

    ## @param username - string - required
    ## The database user to authenticate as.
    #
    username: <USERNAME>

    ## @param password - string - required
    ## The password of `username`.
    #
    password: <PASSWORD>

    ## @param host - string - optional
    ## The hostname used to connect to a remote or uncataloged database.
    #
    # host: <HOSTNAME>

    ## @param port - integer - optional - default: 50000
    ## The port used to connect to a remote or uncataloged database.
    #
    # port: 50000

    ## @param tls_cert - string - optional
    ## The path to a SSL certicate to use, when connecting to a database in a secure fashion. It
    ## has to be accessible and readable by the agent user.
    #
    # tls_cert: <PATH>

    ## @param tags - list of key:value strings - optional
    ## List of tags to attach to every metric and service check emitted by this instance.
    ##
    ## Learn more about tagging at https://docs.datadoghq.com/tagging
    #
    # tags:
    #   - <KEY_1>:<VALUE_1>
    #   - <KEY_2>:<VALUE_2>

    ## @param use_global_custom_queries - string - optional - default: true
    ## How `global_custom_queries` should be used for this instance. There are 3 options:
    ##
    ## 1. true - `global_custom_queries` will override `custom_queries`
    ## 2. false - `custom_queries` will override `global_custom_queries`
    ## 2. extend - `global_custom_queries` will be used in addition to any `custom_queries`
    #
    # use_global_custom_queries: true

    ## @param custom_queries - list - optional
    ## Each query must have 3 fields:
    ##
    ## 1. metric_prefix - This is what each metric will start with.
    ## 2. query - This is the SQL to execute. It can be a simple statement or a multi-line script.
    ## 3. columns - This is a list representing each column, ordered sequentially
    ##              from left to right. The number of columns must equal the number
    ##              of columns returned in the query.
    ##              There are 2 required pieces of data:
    ##                a. name - This is the suffix to append to the metric_prefix
    ##                          in order to form the full metric name. If `type` is
    ##                          `tag`, this column will instead be considered a tag
    ##                          and will be applied to every metric collected by
    ##                          this particular query.
    ##                b. type - This is the submission method (gauge, count, rate, etc.).
    ##                          This can also be set to `tag` to tag each metric in the row
    ##                          with the name and value of the item in this column. You can
    ##                          use the `count` type to perform aggregation for queries that
    ##                          return multiple rows with the same or no tags.
    ## 4. tags (optional) - A list of tags to apply to each metric.
    #
    # custom_queries:
    #   - metric_prefix: ibm_db2
    #     query: |  # Use the pipe if you require a multi-line script.
    #       SELECT files_closed,
    #              tbsp_name
    #       FROM TABLE(MON_GET_TABLESPACE(NULL, -1))
    #     columns:
    #       # Columns without a name are ignored, put this for any column you wish to skip:
    #       # - {}
    #       - name: tablespace.files_closed
    #         type: monotonic_count
    #       - name: tablespace
    #         type: tag
    #     tags:
    #       - test:ibm_db2

## Log Section (Available for Agent >=6.0)
##
## type - mandatory - Type of log input source (tcp / udp / file / windows_event)
## port / path / channel_path - mandatory - Set port if type is tcp or udp. Set path if type is file. Set channel_path if type is windows_event
## service - mandatory - Name of the service that generated the log
## source  - mandatory - Attribute that defines which Integration sent the logs
## tags: - optional - Add tags to the collected logs
##
## Discover Datadog log collection: https://docs.datadoghq.com/logs/log_collection/
#
# logs:
#   - source: ibm_db2
#     type: file
#     path: /home/db2inst1/sqllib/db2dump/db2diag.log
#     service: db2sysc
#     log_processing_rules:
#       - type: multi_line
#         name: new_log_start_with_date
#         pattern: \d{4}\-(0?[1-9]|[12][0-9]|3[01])\-(0?[1-9]|1[012])
