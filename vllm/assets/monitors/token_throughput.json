{
    "version": 2,
    "created_at": "2024-06-26",
    "last_updated_at": "2024-06-26",
    "title": "vLLM application token usage is high",
    "description": "The more tokens a language model generates per second, the more work it is doing. This monitor alerts you if a model that vLLM is serving is generating a lot of tokens, which indicates high load. This can result in lower response times and more resource consumption.",
    "tags": [
        "integration:vllm"
    ],
    "definition": {
        "name": "vLLM application token usage is high",
        "type": "query alert",
        "query": "avg(last_15m):avg:vllm.avg.generation_throughput.toks_per_s{*} >= 10000",
        "message": "The average number of tokens generated every second by your vLLM application is high.",
        "tags": [
            "integration:vllm"
        ],
        "options": {
            "thresholds": {
                "critical": 10000
            },
            "notify_audit": false,
            "include_tags": false,
            "avalanche_window": 20,
            "new_host_delay": 300,
            "silenced": {}
        },
        "priority": null,
        "restriction_policy": {
            "bindings": []
        }
    }
}
