{
  "manifest_version": "2.0.0",
  "app_uuid": "355886f0-31ae-44a9-9638-6951ad0f3039",
  "app_id": "vllm",
  "display_on_public_website": true,
  "tile": {
    "overview": "README.md#Overview",
    "configuration": "README.md#Setup",
    "support": "README.md#Support",
    "changelog": "CHANGELOG.md",
    "description": "vLLM is a library for LLM inference and serving",
    "title": "vLLM",
    "media": [],
    "classifier_tags": [
      "Supported OS::Linux",
      "Supported OS::Windows",
      "Supported OS::macOS",
      "Category::Log Collection",
      "Category::AI/ML",
      "Submitted Data Type::Metrics",
      "Offering::Integration"
    ],
    "resources": [
      {
        "resource_type": "blog",
        "url": "https://www.datadoghq.com/blog/vllm-integration/"
      }
    ]
  },
  "assets": {
    "integration": {
      "auto_install": true,
      "source_type_id": 17652503,
      "source_type_name": "vLLM",
      "configuration": {
        "spec": "assets/configuration/spec.yaml"
      },
      "events": {
        "creates_events": false
      },
      "metrics": {
        "prefix": "vllm.",
        "check": "vllm.num_requests.running",
        "metadata_path": "metadata.csv"
      },
      "service_checks": {
        "metadata_path": "assets/service_checks.json"
      },
      "process_signatures": [
        "vllm.entrypoints.openai.api_server"
      ]
    },
    "monitors": {
      "vLLM application token usage is high": "assets/monitors/token_throughput.json",
      "Average Request Latency is High": "assets/monitors/latency.json"
    },
    "dashboards": {
      "vLLM Overview": "assets/dashboards/overview.json"
    },
    "saved_views": {
      "errors": "assets/saved_views/errors.json"
    }
  },
  "author": {
    "support_email": "help@datadoghq.com",
    "name": "Datadog",
    "homepage": "https://www.datadoghq.com",
    "sales_email": "info@datadoghq.com"
  }
}
