## All options defined here are available to all instances.
#
init_config:

    ## @param service - string - optional
    ## Attach the tag `service:<SERVICE>` to every metric, event, and service check emitted by this integration.
    ##
    ## Additionally, this sets the default `service` for every log source.
    #
    # service: <SERVICE>

## Every instance is scheduled independently of the others.
#
instances:

    ## @param remote_config_id - string - required
    ## Remote configuration ID for tracking purposes.
    ## This ID is automatically set by the Datadog backend via remote configuration.
    ## This parameter is REQUIRED - the integration will not start without it.
    #
  - remote_config_id: kafka-read-1732128000

    ## @param kafka_connect_str - string - required
    ## The Kafka connection string (bootstrap servers).
    #
    kafka_connect_str: localhost:9092

    ## @param kafka_cluster_id - string - optional - default: MkU3OEVBNTcwNTJENDM2Qk
    ## Expected Kafka cluster ID for verification.
    ## If provided, the integration will verify that the connected Kafka cluster
    ## matches this ID before executing any actions. This prevents accidentally
    ## running actions against the wrong cluster.
    #
    # kafka_cluster_id: MkU3OEVBNTcwNTJENDM2Qk

    ## @param security_protocol - string - optional - default: PLAINTEXT
    ## Protocol used to communicate with brokers.
    ## Valid values: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL
    #
    # security_protocol: PLAINTEXT

    ## @param sasl_mechanism - string - optional - default: PLAIN
    ## SASL mechanism to use for authentication.
    ## Valid values: PLAIN, SCRAM-SHA-256, SCRAM-SHA-512
    #
    # sasl_mechanism: PLAIN

    ## @param sasl_plain_username - string - optional
    ## Username for SASL PLAIN authentication
    #
    # sasl_plain_username: <SASL_PLAIN_USERNAME>

    ## @param sasl_plain_password - string - optional
    ## Password for SASL PLAIN authentication
    #
    # sasl_plain_password: <SASL_PLAIN_PASSWORD>

    ## @param read_messages - mapping - optional
    ## Configuration for reading messages from Kafka topics.
    ## Messages are streamed in real-time and sent to Datadog as they arrive.
    ## The check has a 20-second timeout for the entire operation.
    ## Supports JSON, BSON, Protobuf, and Avro with optional Schema Registry integration.
    ## Filtering is applied after deserialization using jq-style expressions.
    #
    # read_messages: {}

    ## @param create_topic - mapping - optional
    ## Configuration for creating a new Kafka topic.
    #
    # create_topic: {}

    ## @param update_topic_config - mapping - optional
    ## Configuration for updating an existing topic's configuration.
    #
    # update_topic_config: {}

    ## @param delete_topic - mapping - optional
    ## Configuration for deleting a Kafka topic.
    ## WARNING: This operation is irreversible and will cause data loss.
    #
    # delete_topic: {}

    ## @param delete_consumer_group - mapping - optional
    ## Configuration for deleting a consumer group.
    ## The consumer group must have no active members.
    #
    # delete_consumer_group: {}

    ## @param update_consumer_group_offsets - mapping - optional
    ## Configuration for updating consumer group offsets.
    ## WARNING: Can cause duplicate processing or data loss.
    ## Consumer group should be stopped (no active members).
    #
    # update_consumer_group_offsets: {}

    ## @param produce_message - mapping - optional
    ## Configuration for producing a message to a Kafka topic.
    ## All values (key, value, headers) must be base64-encoded to ensure
    ## safe transmission via YAML and support for binary data.
    #
    # produce_message: {}

    ## @param tags - list of strings - optional
    ## A list of tags to attach to every metric and service check emitted by this instance.
    ##
    ## Learn more about tagging at https://docs.datadoghq.com/tagging
    #
    # tags:
    #   - <KEY_1>:<VALUE_1>
    #   - <KEY_2>:<VALUE_2>

    ## @param service - string - optional
    ## Attach the tag `service:<SERVICE>` to every metric, event, and service check emitted by this integration.
    ##
    ## Overrides any `service` defined in the `init_config` section.
    #
    # service: <SERVICE>

    ## @param min_collection_interval - number - optional - default: 15
    ## This changes the collection interval of the check. For more information, see:
    ## https://docs.datadoghq.com/developers/write_agent_check/#collection-interval
    #
    # min_collection_interval: 15

    ## @param empty_default_hostname - boolean - optional - default: false
    ## This forces the check to send metrics with no hostname.
    ##
    ## This is useful for cluster-level checks.
    #
    # empty_default_hostname: false

    ## @param metric_patterns - mapping - optional
    ## A mapping of metrics to include or exclude, with each entry being a regular expression.
    ##
    ## Metrics defined in `exclude` will take precedence in case of overlap.
    #
    # metric_patterns:
    #   include:
    #   - <INCLUDE_REGEX>
    #   exclude:
    #   - <EXCLUDE_REGEX>
