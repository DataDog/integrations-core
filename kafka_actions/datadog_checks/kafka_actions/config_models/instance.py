# (C) Datadog, Inc. 2025-present
# All rights reserved
# Licensed under a 3-clause BSD style license (see LICENSE)

# This file is autogenerated.
# To change this file you should edit assets/configuration/spec.yaml and then run the following commands:
#     ddev -x validate config -s <INTEGRATION_NAME>
#     ddev -x validate models -s <INTEGRATION_NAME>

from __future__ import annotations

from types import MappingProxyType
from typing import Any, Optional

from pydantic import BaseModel, ConfigDict, Field, field_validator, model_validator

from datadog_checks.base.utils.functions import identity
from datadog_checks.base.utils.models import validation

from . import defaults, validators


class CreateTopic(BaseModel):
    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        frozen=True,
    )
    cluster: str = Field(..., description='Kafka cluster identifier', examples=['prod-kafka-1'])
    configs: Optional[MappingProxyType[str, Any]] = Field(
        None,
        description='Topic configuration parameters.\nCommon configs: retention.ms, compression.type, min.insync.replicas, etc.\n',
        examples=[{'compression.type': 'snappy', 'retention.ms': '604800000'}],
    )
    num_partitions: int = Field(..., description='Number of partitions', examples=[6])
    replication_factor: int = Field(..., description='Replication factor', examples=[3])
    topic: str = Field(..., description='Topic name to create', examples=['orders-v2'])


class DeleteConsumerGroup(BaseModel):
    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        frozen=True,
    )
    cluster: str = Field(..., description='Kafka cluster identifier', examples=['prod-kafka-1'])
    consumer_group: str = Field(..., description='Consumer group ID to delete', examples=['old-service-v1'])


class DeleteTopic(BaseModel):
    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        frozen=True,
    )
    cluster: str = Field(..., description='Kafka cluster identifier', examples=['prod-kafka-1'])
    topic: str = Field(..., description='Topic name to delete', examples=['old-orders'])


class MetricPatterns(BaseModel):
    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        frozen=True,
    )
    exclude: Optional[tuple[str, ...]] = None
    include: Optional[tuple[str, ...]] = None


class ProduceMessage(BaseModel):
    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        frozen=True,
    )
    cluster: str = Field(..., description='Kafka cluster identifier', examples=['prod-kafka-1'])
    headers: Optional[MappingProxyType[str, Any]] = Field(
        None,
        description='Message headers (metadata). Values must be base64-encoded.\nExample: source: "datadog-agent" -> base64 -> "ZGF0YWRvZy1hZ2VudA=="\n',
        examples=[{'source': 'ZGF0YWRvZy1hZ2VudA=='}],
    )
    key: Optional[str] = Field(
        None,
        description='Message key (optional), base64-encoded.\nIf not provided, the message will have a null key in Kafka.\nExample: "12345" -> base64 -> "MTIzNDU="\n',
        examples=['MTIzNDU='],
    )
    partition: Optional[int] = Field(
        -1, description='Specific partition (-1 for automatic partitioning)', examples=[-1]
    )
    topic: str = Field(..., description='Topic to produce to', examples=['test-topic'])
    value: str = Field(
        ...,
        description='Message value, base64-encoded.\nExample: \'{"order_id": "12345"}\' -> base64 -> "eyJvcmRlcl9pZCI6ICIxMjM0NSJ9"\n',
        examples=['eyJvcmRlcl9pZCI6ICIxMjM0NSIsICJzdGF0dXMiOiAicGVuZGluZyJ9'],
    )


class ReadMessages(BaseModel):
    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        frozen=True,
    )
    cluster: str = Field(..., description='Kafka cluster identifier', examples=['prod-kafka-1'])
    consumer_group_id: Optional[str] = Field(
        None,
        description='Consumer group ID (optional).\nIf not provided, defaults to: datadog-agent-<remote_config_id>\n',
        examples=['datadog-agent-kafka-read-1732128000'],
    )
    filter: Optional[str] = Field(
        None,
        description='jq-style expression to filter messages (optional).\nFiltering happens AFTER deserialization.\nExamples: \'.value.price > 100\', \'.value.user.country == "US"\'\n',
        examples=['.value.status == "failed"'],
    )
    key_format: Optional[str] = Field(
        'json',
        description='Message key format:\n- string: Plain UTF-8 text (most common for keys)\n- json: JSON data (strict validation, fails if not valid JSON)\n- bson: BSON (Binary JSON) data\n- protobuf: Protocol Buffers\n- avro: Apache Avro\n',
        examples=['json'],
    )
    key_schema: Optional[str] = Field(None, description='Schema definition for protobuf/avro key')
    key_uses_schema_registry: Optional[bool] = Field(False, description='Whether key uses Schema Registry format')
    max_scanned_messages: Optional[int] = Field(
        1000,
        description='Maximum number of messages to scan through from Kafka.\nIf this limit is reached before n_messages_retrieved matching messages are found,\nthe operation stops and the event will indicate the limit was hit.\nThis prevents scanning through millions of messages.\n',
        examples=[1000],
    )
    n_messages_retrieved: Optional[int] = Field(
        10,
        description='Maximum number of matching messages to retrieve and send to Datadog.\nThe operation stops when this many matching messages are found.\n',
        examples=[10],
    )
    partition: Optional[int] = Field(
        -1, description='Specific partition to read from (-1 for all partitions)', examples=[-1]
    )
    start_offset: Optional[int] = Field(
        -1, description='Starting offset (-1 for latest, -2 for earliest, or specific offset)', examples=[-1]
    )
    topic: str = Field(..., description='Topic to read messages from', examples=['orders'])
    value_format: Optional[str] = Field(
        'json',
        description='Message value format:\n- json: JSON data (strict validation, fails if not valid JSON)\n- bson: BSON (Binary JSON) data\n- string: Plain UTF-8 text (use for non-JSON text messages)\n- protobuf: Protocol Buffers\n- avro: Apache Avro\nNote: If any message fails deserialization, the read_messages action will stop immediately.\nEnsure the format matches the actual messages in your topic.\n',
        examples=['json'],
    )
    value_schema: Optional[str] = Field(None, description='Schema definition for protobuf/avro value')
    value_uses_schema_registry: Optional[bool] = Field(False, description='Whether value uses Schema Registry format')


class Offset(BaseModel):
    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        frozen=True,
    )
    offset: int = Field(..., description='New offset value')
    partition: int = Field(..., description='Partition number')
    topic: str = Field(..., description='Topic name')


class UpdateConsumerGroupOffsets(BaseModel):
    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        frozen=True,
    )
    cluster: str = Field(..., description='Kafka cluster identifier', examples=['prod-kafka-1'])
    consumer_group: str = Field(..., description='Consumer group ID to update', examples=['order-processor'])
    offsets: tuple[Offset, ...] = Field(
        ...,
        description='List of topic-partition-offset tuples to update',
        examples=[
            [{'offset': 1000, 'partition': 0, 'topic': 'orders'}, {'offset': 1500, 'partition': 1, 'topic': 'orders'}]
        ],
    )


class UpdateTopicConfig(BaseModel):
    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        frozen=True,
    )
    cluster: str = Field(..., description='Kafka cluster identifier', examples=['prod-kafka-1'])
    configs: Optional[MappingProxyType[str, Any]] = Field(
        None,
        description='Configuration parameters to update',
        examples=[{'max.message.bytes': '2097152', 'retention.ms': '1209600000'}],
    )
    delete_configs: Optional[tuple[str, ...]] = Field(
        None, description='Configuration keys to reset to defaults', examples=[['retention.bytes', 'compression.type']]
    )
    num_partitions: Optional[int] = Field(
        None, description='New partition count (can only increase, cannot decrease)', examples=[12]
    )
    topic: str = Field(..., description='Topic name to update', examples=['orders'])


class InstanceConfig(BaseModel):
    model_config = ConfigDict(
        validate_default=True,
        arbitrary_types_allowed=True,
        frozen=True,
    )
    create_topic: Optional[CreateTopic] = None
    delete_consumer_group: Optional[DeleteConsumerGroup] = None
    delete_topic: Optional[DeleteTopic] = None
    disable_generic_tags: Optional[bool] = None
    empty_default_hostname: Optional[bool] = None
    enable_legacy_tags_normalization: Optional[bool] = None
    kafka_cluster_id: Optional[str] = None
    kafka_connect_str: str
    metric_patterns: Optional[MetricPatterns] = None
    min_collection_interval: Optional[float] = None
    produce_message: Optional[ProduceMessage] = None
    read_messages: Optional[ReadMessages] = None
    remote_config_id: str
    sasl_mechanism: Optional[str] = None
    sasl_plain_password: Optional[str] = None
    sasl_plain_username: Optional[str] = None
    security_protocol: Optional[str] = None
    service: Optional[str] = None
    tags: Optional[tuple[str, ...]] = None
    update_consumer_group_offsets: Optional[UpdateConsumerGroupOffsets] = None
    update_topic_config: Optional[UpdateTopicConfig] = None

    @model_validator(mode='before')
    def _initial_validation(cls, values):
        return validation.core.initialize_config(getattr(validators, 'initialize_instance', identity)(values))

    @field_validator('*', mode='before')
    def _validate(cls, value, info):
        field = cls.model_fields[info.field_name]
        field_name = field.alias or info.field_name
        if field_name in info.context['configured_fields']:
            value = getattr(validators, f'instance_{info.field_name}', identity)(value, field=field)
        else:
            value = getattr(defaults, f'instance_{info.field_name}', lambda: value)()

        return validation.utils.make_immutable(value)

    @model_validator(mode='after')
    def _final_validation(cls, model):
        return validation.core.check_model(getattr(validators, 'check_instance', identity)(model))
