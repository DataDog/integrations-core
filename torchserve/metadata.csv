metric_name,metric_type,interval,unit_name,per_unit_name,description,orientation,integration,short_name,curated_metric
torchserve.openmetrics.cpu.utilization,gauge,,percent,,CPU utilization on host.,0,torchserve,cpu utilization,
torchserve.openmetrics.disk.available,gauge,,gigabyte,,Disk available on host.,0,torchserve,disk available,
torchserve.openmetrics.disk.used,gauge,,gigabyte,,Memory used on host.,0,torchserve,disk used,
torchserve.openmetrics.disk.utilization,gauge,,percent,,Disk utilization on host.,0,torchserve,disk utilization,
torchserve.openmetrics.memory.available,gauge,,megabyte,,Memory available on host.,0,torchserve,memory available,
torchserve.openmetrics.memory.used,gauge,,megabyte,,Memory used on host.,0,torchserve,memory used,
torchserve.openmetrics.memory.utilization,gauge,,percent,,Memory utilization on host.,0,torchserve,memory utilization,
torchserve.openmetrics.queue.time,gauge,,millisecond,,Time spent by a job in request queue in Milliseconds.,0,torchserve,queue time,
torchserve.openmetrics.requests.2xx.count,count,,request,,Total number of requests with response in 200-300 status code range.,1,torchserve,requests 2xx,
torchserve.openmetrics.requests.4xx.count,count,,request,,Total number of requests with response in 400-500 status code range.,1,torchserve,requests 4xx,
torchserve.openmetrics.requests.5xx.count,count,,request,,Total number of requests with response status code above 500.,1,torchserve,requests 5xx,
torchserve.openmetrics.inference.latency.count,count,,microsecond,,Total inference latency in Microseconds.,1,torchserve,inference latency,
torchserve.openmetrics.inference.count,count,,request,,Total number of inference requests received.,1,torchserve,inference,
torchserve.openmetrics.queue.latency.count,count,,microsecond,,Total queue latency in Microseconds.,1,torchserve,queue latency,
torchserve.openmetrics.worker.load_time,gauge,,millisecond,,Time taken by worker to load model in Milliseconds.,0,torchserve,worker load time,
torchserve.openmetrics.worker.thread_time,gauge,,millisecond,,Time spent in worker thread excluding backend response time in Milliseconds.,0,torchserve,worker thread time,
torchserve.openmetrics.handler_time,gauge,,millisecond,,Time spent in backend handler.,0,torchserve,handler time,
torchserve.openmetrics.prediction_time,gauge,,millisecond,,Backend prediction time.,0,torchserve,prediction time,
torchserve.openmetrics.gpu.utilization,gauge,,percent,,GPU utilization on host.,0,torchserve,gpu utilization,
torchserve.openmetrics.gpu.memory.utilization,gauge,,percent,,GPU memory utilization on host.,0,torchserve,gpu memory utilization,
torchserve.openmetrics.gpu.memory.used,gauge,,megabyte,,GPU memory used on host.,0,torchserve,gpu memory used,
torchserve.management_api.models,gauge,,,,Total number of models.,0,torchserve,models total,
torchserve.management_api.model.versions,gauge,,,,Total number of versions for a given model.,0,torchserve,models version total,
torchserve.management_api.model.workers.current,gauge,,,,Current number of workers of a given model.,0,torchserve,models workers current,
torchserve.management_api.model.workers.min,gauge,,,,Minimum number of workers defined of a given model.,0,torchserve,models workers min,
torchserve.management_api.model.workers.max,gauge,,,,Maximum number of workers defined of a given model.,0,torchserve,models workers max,
torchserve.management_api.model.batch_size,gauge,,,,Maximum batch size that a model is expected to handle.,0,torchserve,models batch size,
torchserve.management_api.model.max_batch_delay,gauge,,millisecond,,The maximum batch delay time in ms TorchServe waits to receive batch_size number of requests.,0,torchserve,models max batch delay,
torchserve.management_api.model.is_loaded_at_startup,gauge,,,,"Whether or not the model was loaded when TorchServe started. 1 if true, 0 otherwise.",0,torchserve,models loaded at startup,
torchserve.management_api.model.version.is_default,gauge,,,,"Whether or not this version of the model is the default one. 1 if true, 0 otherwise.",0,torchserve,model version default,
torchserve.management_api.model.worker.memory_usage,gauge,,byte,,Memory used by the worker in byte.,0,torchserve,models worker memory,
torchserve.management_api.model.worker.is_gpu,gauge,,,,"Whether or not this worker is using a GPU. 1 if true, 0 otherwise.",0,torchserve,model worker gpu,
torchserve.management_api.model.worker.status,gauge,,,,"The status of a given worker. 1 if ready, 2 if loading, 3 if unloading, 0 otherwise.",0,torchserve,model worker status,
