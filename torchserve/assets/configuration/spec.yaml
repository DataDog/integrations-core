name: TorchServe
files:
- name: torchserve.yaml
  options:
  - template: init_config
    options:
    - template: init_config/openmetrics
    - template: init_config/http
      overrides:
        proxy.hidden: true
        skip_proxy.hidden: true
        timeout.hidden: true
    - template: init_config/default
      overrides:
        service.hidden: true
  - template: instances
    multiple_instances_defined: true
    options:
      - name: OpenMetrics
        description: | 
          TorchServe OpenMetrics endpoint configuration
        options:
          - template: instances/openmetrics
            overrides:
              openmetrics_endpoint.required: false
              openmetrics_endpoint.value.example: http://localhost:8082/metrics
              exclude_labels.value.example:
                - hostname
                - Hostname
              rename_labels.value.example:
                Level: level
                ModelName: model_name
                WorkerName: worker_name
              exclude_metrics_by_labels.description: |
                A mapping of labels to exclude metrics with matching label name and their corresponding metric values. 
                To match all values of a label, set it to `true`.
    
                Note: Label filtering happens before `rename_labels`.
    
                For example, the following configuration instructs the check to exclude all metrics with
                a label `worker` or a label `pid` with the value of either `23` or `42`.
    
                  exclude_metrics_by_labels:
                    worker: true
                    pid:
                    - '23'
                    - '42'
              cache_shared_labels.description: |
                When `share_labels` is set, it instructs the check to cache labels collected 
                from the first payload for improved performance.
            
                Set this to `false` to compute label sharing for every payload at the risk 
                of potentially increased memory usage.
      - name: Inference API
        description: TorchServe Inference API endpoint configuration
        options:
          - name: inference_api_url
            required: false
            description: The TorchServe Inference API url to connect to.
            value:
              example: "http://localhost:8080"
              type: string
          - template: instances/http
          - template: instances/default
      - name: Management API
        description: TorchServe Management API endpoint configuration
        options:
          - name: management_api_url
            required: false
            description: The TorchServe Management API url to connect to.
            value:
              example: "http://localhost:8081"
              type: string
          - name: limit
            description: |
              Maximum number of models to be 'autodiscovered'.
            value:
              type: integer
              example: 100
          - name: include
            description: |
              List of regular expressions for models that will be 'autodiscovered'.
            value:
              type: array
              items:
                type: string
              example:
                - .*
          - name: exclude
            description: |
              List of regular expressions with the patterns of models that will not be 'autodiscovered'
            value:
              type: array
              items:
                type: string
          - name: interval
            description: |
              Refresh interval in second to use for the models to be 'autodiscovered'. 
              By default, we refresh the list each time the check runs.
            value:
              type: integer
          - name: submit_events
            description: |
              By default we submit events to Datadog. You can set this option to `false` to disable submitting the events.
            value:
              type: boolean
              example: true
          - template: instances/http
          - template: instances/default
  - template: logs
    example:
    - type: file
      path: /var/log/torchserve/model_log.log
      source: torchserve
      service: <SERVICE>
    - type: file
      path: /var/log/torchserve/ts_log.log
      source: torchserve
      service: <SERVICE>
    - type: file
      path: /var/log/torchserve/access_log.log
      source: torchserve
      service: <SERVICE>