services:

  tgi:
    image: ghcr.io/huggingface/text-generation-inference:3.3.5-intel-cpu
    container_name: hugging_face_tgi
    ports: 
      - "8080:80"
    privileged: true
    cap_add:
      - sys_nice
    devices:
      - /dev/dri
    ipc: host
    shm_size: 1g
    network_mode: host
    volumes:
      - tgi_data:/data
    environment:
      - MODEL_ID=teknium/OpenHermes-2.5-Mistral-7B
    command: --model-id teknium/OpenHermes-2.5-Mistral-7B --cuda-graphs 0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 10m

volumes:
  tgi_data:
