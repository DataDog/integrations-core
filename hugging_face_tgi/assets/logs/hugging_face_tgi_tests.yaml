id: hugging_face_tgi
tests:
 -
  sample: "2025-09-09T11:29:51.795563Z  INFO generate_stream{parameters=GenerateParameters { best_of: None, temperature: None, repetition_penalty: None, frequency_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(20), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None, grammar: None, adapter_id: None } total_time=\"1.194364886s\" validation_time=\"204.821µs\" queue_time=\"53.525µs\" inference_time=\"1.194106715s\" time_per_token=\"59.705335ms\" seed=\"None\"}: text_generation_router::server: router/src/server.rs:637: Success"
  result:
    custom:
      date: 1757417391795
      hugging_face_tgi:
        component: "text_generation_router"
        file: "router/src/server.rs:637"
        inference_time: "1.194106715s"
        operation_type: "generate_stream"
        parameters:
         -
          type: "GenerateParameters"
         -
          watermark: false
          seed: "None"
          do_sample: false
          top_p: "None"
          frequency_penalty: "None"
          grammar: "None"
          best_of: "None"
          stop: "[]"
          truncate: "None"
          top_n_tokens: "None"
          max_new_tokens: "Some(20)"
          top_k: "None"
          decoder_input_details: false
          temperature: "None"
          typical_p: "None"
          details: false
          adapter_id: "None"
          return_full_text: "None"
          repetition_penalty: "None"
        queue_time: "53.525µs"
        seed: "None"
        sub_component: "server"
        time_per_token: "59.705335ms"
        total_time: "1.194364886s"
        validation_time: "204.821µs"
      status: "INFO"
    message: "Success"
    status: "info"
    tags:
     - "source:LOGS_SOURCE"
    timestamp: 1757417391795
 -
  sample: "2025-09-09T11:28:03.840209Z ERROR chat_completions{parameters=\"GenerateParameters { best_of: None, temperature: None, repetition_penalty: None, frequency_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: true, max_new_tokens: Some(20), return_full_text: None, stop: [], truncate: None, watermark: false, details: true, decoder_input_details: false, seed: None, top_n_tokens: None, grammar: None, adapter_id: None }\"}:async_stream:generate_stream: text_generation_router::infer: router/src/infer/mod.rs:126: `inputs` tokens + `max_new_tokens` must be <= 512. Given: 1864 `inputs` tokens and 20 `max_new_tokens`"
  result:
    custom:
      date: 1757417283840
      hugging_face_tgi:
        component: "text_generation_router"
        file: "router/src/infer/mod.rs:126"
        operation_sub_type: "async_stream:generate_stream"
        operation_type: "chat_completions"
        parameters:
         -
          type: "GenerateParameters"
         -
          watermark: false
          seed: "None"
          do_sample: true
          top_p: "None"
          frequency_penalty: "None"
          grammar: "None"
          best_of: "None"
          stop: "[]"
          truncate: "None"
          top_n_tokens: "None"
          max_new_tokens: "Some(20)"
          top_k: "None"
          decoder_input_details: false
          temperature: "None"
          typical_p: "None"
          details: true
          adapter_id: "None"
          return_full_text: "None"
          repetition_penalty: "None"
        sub_component: "infer"
      status: "ERROR"
    message: "`inputs` tokens + `max_new_tokens` must be <= 512. Given: 1864 `inputs` tokens and 20 `max_new_tokens`"
    status: "error"
    tags:
     - "source:LOGS_SOURCE"
    timestamp: 1757417283840
 -
  sample: "2025-09-08T15:41:01.566464Z  WARN text_generation_router::server: router/src/server.rs:1906: Invalid hostname, defaulting to 0.0.0.0"
  result:
    custom:
      date: 1757346061566
      hugging_face_tgi:
        component: "text_generation_router"
        file: "router/src/server.rs:1906"
        sub_component: "server"
      status: "WARN"
    message: "Invalid hostname, defaulting to 0.0.0.0"
    status: "warn"
    tags:
     - "source:LOGS_SOURCE"
    timestamp: 1757346061566
 -
  sample: "2025-09-08T15:38:42.366067Z  INFO download: text_generation_launcher: Starting check and download process for teknium/OpenHermes-2.5-Mistral-7B"
  result:
    custom:
      date: 1757345922366
      hugging_face_tgi:
        component: "text_generation_launcher"
      status: "INFO"
    message: "Starting check and download process for teknium/OpenHermes-2.5-Mistral-7B"
    status: "info"
    tags:
     - "source:LOGS_SOURCE"
    timestamp: 1757345922366
 -
  sample: |-
    2025-09-08T15:38:40.500145Z  INFO text_generation_launcher: Args {
        model_id: "teknium/OpenHermes-2.5-Mistral-7B",
        revision: None,
        validation_workers: 2,
        sharded: None,
        num_shard: None,
        quantize: None,
        speculate: None,
        dtype: None,
        kv_cache_dtype: None,
        trust_remote_code: false,
        max_concurrent_requests: 128,
        max_best_of: 2,
        max_stop_sequences: 4,
        max_top_n_tokens: 5,
        max_input_tokens: None,
        max_input_length: None,
        max_total_tokens: None,
        waiting_served_ratio: 0.3,
        max_batch_prefill_tokens: Some(
            512,
        ),
        max_batch_total_tokens: None,
        max_waiting_tokens: 20,
        max_batch_size: None,
        cuda_graphs: None,
        hostname: "ip-172-31-21-18",
        port: 80,
        prometheus_port: 9000,
        shard_uds_path: "/tmp/text-generation-server",
        master_addr: "localhost",
        master_port: 29500,
        huggingface_hub_cache: None,
        weights_cache_override: None,
        disable_custom_kernels: false,
        cuda_memory_fraction: 1.0,
        rope_scaling: None,
        rope_factor: None,
        json_output: false,
        otlp_endpoint: None,
        otlp_service_name: "text-generation-inference.router",
        cors_allow_origin: [],
        api_key: None,
        watermark_gamma: None,
        watermark_delta: None,
        ngrok: false,
        ngrok_authtoken: None,
        ngrok_edge: None,
        tokenizer_config_path: None,
        disable_grammar_support: false,
        env: false,
        max_client_batch_size: 4,
        lora_adapters: None,
        usage_stats: On,
        payload_limit: 2000000,
        enable_prefill_logprobs: false,
        graceful_termination_timeout: 90,
    }
  result:
    custom:
      date: 1757345920500
      hugging_face_tgi:
       -
        component: "text_generation_launcher"
       -
        validation_workers: 2
        cuda_graphs: "None"
        usage_stats: "On"
        sharded: "None"
        trust_remote_code: false
        max_total_tokens: "None"
        hostname: "ip-172-31-21-18"
        max_input_length: "None"
        max_batch_size: "None"
        shard_uds_path: "/tmp/text-generation-server"
        waiting_served_ratio: 0.3
        num_shard: "None"
        graceful_termination_timeout: 90
        json_output: false
        dtype: "None"
        kv_cache_dtype: "None"
        payload_limit: 2000000
        max_stop_sequences: 4
        tokenizer_config_path: "None"
        revision: "None"
        weights_cache_override: "None"
        lora_adapters: "None"
        port: 80
        max_input_tokens: "None"
        cuda_memory_fraction: 1.0
        otlp_service_name: "text-generation-inference.router"
        max_top_n_tokens: 5
        rope_factor: "None"
        watermark_delta: "None"
        ngrok: false
        disable_grammar_support: false
        max_waiting_tokens: 20
        quantize: "None"
        disable_custom_kernels: false
        max_concurrent_requests: 128
        max_client_batch_size: 4
        rope_scaling: "None"
        huggingface_hub_cache: "None"
        speculate: "None"
        max_best_of: 2
        model_id: "teknium/OpenHermes-2.5-Mistral-7B"
        env: false
        master_addr: "localhost"
        watermark_gamma: "None"
        ngrok_authtoken: "None"
        api_key: "None"
        prometheus_port: 9000
        ngrok_edge: "None"
        cors_allow_origin: "[]"
        master_port: 29500
        otlp_endpoint: "None"
        enable_prefill_logprobs: false
        max_batch_total_tokens: "None"
      status: "INFO"
    message: |-
      2025-09-08T15:38:40.500145Z  INFO text_generation_launcher: Args {
          model_id: "teknium/OpenHermes-2.5-Mistral-7B",
          revision: None,
          validation_workers: 2,
          sharded: None,
          num_shard: None,
          quantize: None,
          speculate: None,
          dtype: None,
          kv_cache_dtype: None,
          trust_remote_code: false,
          max_concurrent_requests: 128,
          max_best_of: 2,
          max_stop_sequences: 4,
          max_top_n_tokens: 5,
          max_input_tokens: None,
          max_input_length: None,
          max_total_tokens: None,
          waiting_served_ratio: 0.3,
          max_batch_prefill_tokens: Some(
              512,
          ),
          max_batch_total_tokens: None,
          max_waiting_tokens: 20,
          max_batch_size: None,
          cuda_graphs: None,
          hostname: "ip-172-31-21-18",
          port: 80,
          prometheus_port: 9000,
          shard_uds_path: "/tmp/text-generation-server",
          master_addr: "localhost",
          master_port: 29500,
          huggingface_hub_cache: None,
          weights_cache_override: None,
          disable_custom_kernels: false,
          cuda_memory_fraction: 1.0,
          rope_scaling: None,
          rope_factor: None,
          json_output: false,
          otlp_endpoint: None,
          otlp_service_name: "text-generation-inference.router",
          cors_allow_origin: [],
          api_key: None,
          watermark_gamma: None,
          watermark_delta: None,
          ngrok: false,
          ngrok_authtoken: None,
          ngrok_edge: None,
          tokenizer_config_path: None,
          disable_grammar_support: false,
          env: false,
          max_client_batch_size: 4,
          lora_adapters: None,
          usage_stats: On,
          payload_limit: 2000000,
          enable_prefill_logprobs: false,
          graceful_termination_timeout: 90,
      }
    status: "info"
    tags:
     - "source:LOGS_SOURCE"
    timestamp: 1757345920500
