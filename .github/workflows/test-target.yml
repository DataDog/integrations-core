# This is a copy of the test target workflow created for testing updates on the workflow without impacting the on used in our PRs.
# Since this workflow is used through workflow dispatch, it needs to be in master to be able tot be executed. This means that we cannot
# easily update it to test someth8ing without impacting onging runs.
name: ZZ - Test Target

on:
  workflow_call:
    inputs:
      job-name:
        required: true
        type: string
      target:
        required: true
        type: string
      target-env:
        description: "Specific target env to run"
        required: false
        default: ""
        type: string
      platform:
        required: true
        type: string
      runner:
        required: true
        type: string
      repo:
        required: true
        type: string

      python-version:
        required: false
        default: "3.13"
        type: string
      benchmark:
        required: false
        default: false
        type: boolean
      latest:
        required: false
        default: false
        type: boolean
      minimum-base-package:
        required: false
        default: false
        type: boolean
      # integrations-core repo no longer tests Python 2, only Python 3.
      # We keep the options in the workflow for community and marketplace partners.
      # They may still continue to test Python 2 and Agent 6.
      test-py2:
        required: false
        default: false
        type: boolean
      test-py3:
        required: false
        default: true
        type: boolean
      agent-image:
        required: false
        default: "datadog/agent-dev:master-py3"
        type: string
      agent-image-py2:
        required: false
        default: "datadog/agent-dev:master-py2"
        type: string
      agent-image-windows:
        required: false
        default: "datadog/agent-dev:master-py3-win-servercore"
        type: string
      agent-image-windows-py2:
        required: false
        default: "datadog/agent-dev:master-py2-win-servercore"
        type: string
      setup-env-vars:
        required: false
        default: ""
        type: string
      pytest-args:
        description: "Arguments to pass to pytest"
        required: false
        type: string
        default: ""
      context:
        description: "Context where the test is run. This is used for CI visibility descovery."
        required: false
        default: "standard"
        type: string
      step-timeout-minutes:
        description: "Timeout in minutes for ddev test steps"
        required: false
        default: 60
        type: number

defaults:
  run:
    shell: bash

jobs:
  run:
    name: "${{ inputs.job-name }}${{ inputs.target-env && format('-{0}', inputs.target-env) || '' }}"
    runs-on: ${{ fromJson(inputs.runner) }}

    permissions:
       # needed for compute-matrix in test-target.yml
       contents: read
    steps:

    - name: Set up Windows
      if: runner.os == 'Windows'
      run: |-
        # Enable disk performance counters
        diskperf -y

    - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

    # This step sets the following environment variables for the entire workflow via setup-test-env.sh:
    #   FORCE_COLOR
    #   TEST_RESULTS_BASE_DIR
    #   PYTHON_VERSION
    #   PYTHONUNBUFFERED
    #   SKIP_ENV_NAME
    #   DDEV_E2E_AGENT
    #   DDEV_E2E_AGENT_PY2
    #   DD_ENV
    #   DD_SERVICE
    #   DD_TRACE_ANALYTICS_ENABLED
    #   DD_CIVISIBILITY_ENABLED
    #   DD_CIVISIBILITY_AGENTLESS_ENABLED
    #   DD_CIVISIBILITY_AUTO_INSTRUMENTATION_PROVIDER
    #   DD_PROFILING_ENABLED
    #   DD_SITE
    #   DD_API_KEY
    #   MINIMUM_BASE_PACKAGE_PREFIX
    #   TEST_RESULTS_DIR
    #   DD_TAGS
    #   DOCKER_USERNAME
    #   DOCKER_ACCESS_TOKEN
    #   ORACLE_DOCKER_USERNAME
    #   ORACLE_DOCKER_PASSWORD
    #   DD_GITHUB_USER
    #   DD_GITHUB_TOKEN
    - name: Set environment variables
      run: bash .github/workflows/scripts/setup-test-env.sh
      env:
        INPUT_PYTHON_VERSION: ${{ inputs.python-version }}
        INPUT_TEST_PY2: ${{ inputs.test-py2 }}
        INPUT_TEST_PY3: ${{ inputs.test-py3 }}
        INPUT_PLATFORM: ${{ inputs.platform }}
        INPUT_AGENT_IMAGE: ${{ inputs.agent-image }}
        INPUT_AGENT_IMAGE_PY2: ${{ inputs.agent-image-py2 }}
        INPUT_AGENT_IMAGE_WINDOWS: ${{ inputs.agent-image-windows }}
        INPUT_AGENT_IMAGE_WINDOWS_PY2: ${{ inputs.agent-image-windows-py2 }}
        INPUT_TARGET: ${{ inputs.target }}
        INPUT_REPO: ${{ inputs.repo }}
        INPUT_CONTEXT: ${{ inputs.context }}
        INPUT_MINIMUM_BASE_PACKAGE: ${{ inputs.minimum-base-package }}
        INPUT_JOB_NAME: "${{ inputs.job-name }}${{ inputs.target-env && format('-{0}', inputs.target-env) || '' }}"
        DD_API_KEY_SECRET: ${{ secrets.DD_API_KEY }}
        SETUP_ENV_VARS: ${{ inputs.setup-env-vars }}
        DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
        DOCKER_ACCESS_TOKEN: ${{ secrets.DOCKER_ACCESS_TOKEN }}
        ORACLE_DOCKER_USERNAME: ${{ secrets.ORACLE_DOCKER_USERNAME }}
        ORACLE_DOCKER_PASSWORD: ${{ secrets.ORACLE_DOCKER_PASSWORD }}
        DD_GITHUB_USER: ${{ github.actor }}
        DD_GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python 2.7
      if: inputs.test-py2
      run: |
        if [ "$RUNNER_OS" == "Linux" ]; then
          sudo apt update
          sudo apt install python2 python2-dev
          sudo update-alternatives --install /usr/bin/python python /usr/bin/python2 1
        elif [ "$RUNNER_OS" == "Windows" ]; then
          choco install python2
        else
          echo "$RUNNER_OS not supported"
          exit 1
        fi
      shell: bash

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065 # v5.6.0
      with:
        python-version: "${{ env.PYTHON_VERSION }}"
        cache: 'pip'

    - name: Restore cache
      if: inputs.repo == 'core'
      uses: actions/cache/restore@5a3ec84eff668545956fd18022155c47e93e2684 # v4.2.3
      with:
        path: ${{ runner.os == 'Windows' && '~\AppData\Local\pip\Cache' || runner.os == 'macOS' && '~/Library/Caches/pip' || '~/.cache/pip' }}
        key: >-
          ${{ format(
            'v01-python-{0}-{1}-{2}-{3}',
            env.pythonLocation,
            hashFiles('datadog_checks_base/pyproject.toml'),
            hashFiles('datadog_checks_dev/pyproject.toml'),
            hashFiles('ddev/pyproject.toml')
          )}}
        restore-keys: |-
          v01-python-${{ env.pythonLocation }}

    - name: Install ddev from local folder
      if: inputs.repo == 'core'
      run: |-
        pip install -e ./datadog_checks_dev[cli]
        pip install -e ./ddev

    - name: Install ddev from PyPI
      if: inputs.repo != 'core'
      run: pip install ddev

    - name: Configure ddev
      run: |-
        ddev config set upgrade_check false
        ddev config set repos.${{ inputs.repo }} .
        ddev config set repo ${{ inputs.repo }}

    - name: Lint
      timeout-minutes: ${{ inputs.step-timeout-minutes }}
      run: |-
        ddev test --lint ${{ inputs.target }} || {
          echo "::error::Lint failed!"
          echo "::error::Please update to the latest version of ddev to ensure you are using the latest linting rules and then run 'ddev test --fmt ${{ inputs.target }}' to fix formatting issues."
          exit 1
        }

    - name: Prepare for testing
      timeout-minutes: ${{ inputs.step-timeout-minutes }}
      run: ddev ci setup ${{ inputs.target-env && format('{0}:{1}', inputs.target, inputs.target-env) || inputs.target }}

    - name: Run Unit & Integration tests
      if: inputs.latest != true
      timeout-minutes: ${{ inputs.step-timeout-minutes }}
      run: bash .github/workflows/scripts/run-unit-integration-tests.sh
      env:
        INPUT_REPO: ${{ inputs.repo }}
        INPUT_TARGET: ${{ inputs.target }}
        INPUT_PLATFORM: ${{ inputs.platform }}
        INPUT_MINIMUM_BASE_PACKAGE: ${{ inputs.minimum-base-package }}
        INPUT_TARGET_STR: ${{ inputs.target-env && format('{0}:{1}', inputs.target, inputs.target-env) || inputs.target }}
        INPUT_PYTEST_ARGS: ${{ inputs.pytest-args }}

    - name: Run E2E tests
      if: (inputs.repo == 'core' && !inputs.minimum-base-package) || inputs.repo != 'core'
      timeout-minutes: ${{ inputs.step-timeout-minutes }}
      run: bash .github/workflows/scripts/run-e2e-tests.sh
      env:
        INPUT_REPO: ${{ inputs.repo }}
        INPUT_TARGET: ${{ inputs.target }}
        INPUT_PLATFORM: ${{ inputs.platform }}
        INPUT_TARGET_ENV: ${{ inputs.target-env || 'all' }}
        INPUT_PYTEST_ARGS: ${{ inputs.pytest-args }}
        INPUT_SESSION_NAME: "e2e-tests"

    - name: Run benchmarks
      if: inputs.benchmark
      timeout-minutes: ${{ inputs.step-timeout-minutes }}
      env:
        DDEV_TEST_ENABLE_TRACING: "${{ inputs.repo == 'core' && (inputs.target != 'sqlserver' || inputs.platform != 'windows') && '1' || '0' }}"
        DD_TEST_SESSION_NAME: "benchmarks"
      run: ddev test --bench --junit ${{ inputs.target-env && format('{0}:{1}', inputs.target, inputs.target-env) || inputs.target }}

    - name: Run tests and verify support for the latest version
      if: inputs.latest
      timeout-minutes: ${{ inputs.step-timeout-minutes }}
      env:
        DDEV_TEST_ENABLE_TRACING: "${{ inputs.repo == 'core' && (inputs.target != 'sqlserver' || inputs.platform != 'windows') && '1' || '0' }}"
        DD_TEST_SESSION_NAME: "latest-tests"
      run: ddev test --latest --junit ${{ inputs.target-env && format('{0}:{1}', inputs.target, inputs.target-env) || inputs.target }}

    - name: Run E2E tests for the latest version
      if: inputs.latest
      timeout-minutes: ${{ inputs.step-timeout-minutes }}
      run: bash .github/workflows/scripts/run-e2e-tests.sh
      env:
        INPUT_REPO: ${{ inputs.repo }}
        INPUT_TARGET: "${{ inputs.target }}:latest"
        INPUT_PLATFORM: ${{ inputs.platform }}
        INPUT_PYTEST_ARGS: ${{ inputs.pytest-args }}
        INPUT_SESSION_NAME: "latest-e2e-tests"
        INPUT_IS_LATEST: "true"

    - name: Finalize test results
      if: always()
      run: |-
        mkdir -p "${{ env.TEST_RESULTS_DIR }}"
        if [[ -d ${{ inputs.target }}/junit ]]; then
          mv ${{ inputs.target }}/junit/*.xml "${{ env.TEST_RESULTS_DIR }}"
        fi

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
      with:
        name: "${{ env.MINIMUM_BASE_PACKAGE_PREFIX }}test-results-${{ inputs.target }}${{ inputs.target-env && format('-{0}', inputs.target-env) || '' }}-${{ inputs.platform }}"
        path: "${{ env.TEST_RESULTS_BASE_DIR }}"

    - name: Upload local_clone debug info (ddev only)
      if: always() && inputs.target == 'ddev'
      uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
      with:
        name: "${{ env.MINIMUM_BASE_PACKAGE_PREFIX }}local-clone-debug-${{ inputs.platform }}"
        path: "local_clone_debug.txt"
        if-no-files-found: ignore

    - name: Upload coverage data as artifact
      if: >
        !github.event.repository.private &&
        always() &&
        inputs.pytest-args != '-m flaky'
      # Flaky tests will have low coverage, don't include in artifacts to avoid pipeline issues
      uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
      with:
        name: "${{ env.MINIMUM_BASE_PACKAGE_PREFIX }}coverage-${{ inputs.target }}${{ inputs.target-env && format('-{0}', inputs.target-env) || '' }}-${{ inputs.platform }}"
        path: "${{ inputs.target }}/coverage.xml"
        if-no-files-found: ignore
