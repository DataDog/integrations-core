## All options defined here are available to all instances.
#
init_config:

    ## @param global_custom_queries - list of mappings - optional
    ## See `custom_queries` defined below.
    ##
    ## Global custom queries can be applied to all instances using the
    ## `use_global_custom_queries` setting at the instance level.
    #
    # global_custom_queries:
    #   - query: <QUERY>
    #     columns: <COLUMNS>
    #     tags: <TAGS>

    ## @param service - string - optional
    ## Attach the tag `service:<SERVICE>` to every metric, event, and service check emitted by this integration.
    ##
    ## Additionally, this sets the default `service` for every log source.
    #
    # service: <SERVICE>

## Every instance is scheduled independently of the others.
#
instances:

    ## @param host - string - optional
    ## MySQL host to connect to.
    ## NOTE: Even if the host is "localhost", the agent connects to MySQL using TCP/IP, unless you also
    ## provide a value for the sock key (below).
    #
  - host: localhost

    ## @param username - string - optional
    ## Username used to connect to MySQL.
    #
    username: datadog

    ## @param password - string - optional
    ## Password associated to the MySQL user.
    #
    password: <PASSWORD>

    ## @param port - number - optional - default: 3306
    ## Port to use when connecting to MySQL.
    #
    port: 3306

    ## @param reported_hostname - string - optional
    ## Set the reported hostname for this instance. This value overrides the hostname detected by the Agent
    ## and can be useful to set a custom hostname when connecting to a remote database through a proxy.
    #
    # reported_hostname: <REPORTED_HOSTNAME>

    ## @param sock - string - optional
    ## Path to a Unix Domain Socket to use when connecting to MySQL (instead of a TCP socket).
    ## If you specify a socket you dont need to specify a host.
    #
    # sock: <SOCK>

    ## @param charset - string - optional
    ## Charset you want to use.
    #
    # charset: utf8

    ## @param defaults_file - string - optional
    ## Path to an alternative configuration mechanism file.
    ## If providing this then there is no need to specify host, port, user, pass or socket.
    #
    # defaults_file: <DEFAULTS_FILE>

    ## @param connect_timeout - number - optional - default: 10
    ## Maximum number of seconds to wait before timing out when connecting to MySQL.
    #
    # connect_timeout: 10

    ## @param ssl - mapping - optional
    ## Use this section to configure a TLS connection between the Agent and MySQL.
    ##
    ## The following fields are supported:
    ##
    ## key: Path to a key file.
    ## cert: Path to a cert file.
    ## ca: Path to a CA bundle file.
    ## check_hostname: Set to false to ignore the strict relationship between the certificate and the hostname.
    #
    # ssl:
    #   key: <KEY_FILE_PATH>
    #   cert: <CERT_FILE_PATH>
    #   ca: <CA_PATH_FILE>
    #   check_hostname: true

    ## @param only_custom_queries - boolean - optional - default: false
    ## Set this parameter to `true` if you want to skip the integration's default metrics collection.
    ## Only metrics specified in `custom_queries` will be collected.
    #
    # only_custom_queries: false

    ## @param use_global_custom_queries - string - optional - default: true
    ## How `global_custom_queries` should be used for this instance. There are 3 options:
    ##
    ## 1. true - `global_custom_queries` override `custom_queries`.
    ## 2. false - `custom_queries` override `global_custom_queries`.
    ## 3. extend - `global_custom_queries` are used in addition to any `custom_queries`.
    #
    # use_global_custom_queries: 'true'

    ## @param custom_queries - list of mappings - optional
    ## Each query must have 2 fields, and can have a third optional field:
    ##
    ## 1. query - The SQL to execute. It can be a simple statement or a multi-line script.
    ##            Use the pipe `|` if you require a multi-line script.
    ## 2. columns - The list representing each column, ordered sequentially from left to right.
    ##              The number of columns must equal the number of columns returned in the query.
    ##              There are 2 required pieces of data:
    ##                a. name - The suffix to append to `<INTEGRATION>.` to form
    ##                          the full metric name. If `type` is `tag`, this column is
    ##                          considered a tag and applied to every
    ##                          metric collected by this particular query.
    ##                b. type - The submission method (gauge, monotonic_count, etc.).
    ##                          This can also be set to `tag` to tag each metric in the row
    ##                          with the name and value of the item in this column. You can
    ##                          use the `count` type to perform aggregation for queries that
    ##                          return multiple rows with the same or no tags.
    ##              Columns without a name are ignored. To skip a column, enter:
    ##                - {}
    ## 3. tags (optional) - A list of tags to apply to each metric.
    #
    # custom_queries:
    #   - query: SELECT foo, COUNT(*) FROM table.events GROUP BY foo
    #     columns:
    #     - name: foo
    #       type: tag
    #     - name: event.total
    #       type: gauge
    #     tags:
    #     - test:mysql

    ## @param additional_status - list of mappings - optional
    ## Set this parameter to collect additional MySQL status variables as metrics
    ## from "SHOW GLOBAL STATUS"
    ## See MySQL documentation for available variables: https://dev.mysql.com/doc/refman/8.0/en/show-status.html
    ##
    ## The following fields are supported:
    ##
    ## name : MySQL status name on "SHOW GLOBAL STATUS"
    ## metric_name : metric name
    ## type : gauge | rate | count | ...
    #
    # additional_status:
    #   - name: innodb_rows_read
    #     metric_name: mysql.innodb.rows_read
    #     type: rate

    ## @param additional_variable - list of mappings - optional
    ## Set this parameter to collect additional MySQL variables as metrics
    ## from "SHOW GLOBAL VARIABLES"
    ## See MySQL documentation for available variables: https://dev.mysql.com/doc/refman/8.0/en/show-variables.html
    ##
    ## The following fields are supported:
    ##
    ## name : MySQL variable name on "SHOW GLOBAL VARIABLES"
    ## metric_name : metric name
    ## type : gauge | rate | count | ...
    #
    # additional_variable:
    #   - name: long_query_time
    #     metric_name: mysql.performance.long_query_time
    #     type: gauge

    ## @param max_custom_queries - integer - optional - default: 20
    ## Set the maximum number of custom queries to execute with this integration.
    ##
    ## WARNING: This only applies to the deprecated `queries` option.
    #
    # max_custom_queries: 20

    ## @param queries - list of mappings - optional
    ## Define custom queries to collect custom metrics from your MySQL database.
    ##
    ## See https://docs.datadoghq.com/integrations/guide/mysql-custom-queries to learn more.
    ##
    ## WARNING: This option is deprecated and will be removed in a future release.
    ## Use the `custom_queries` option instead.
    #
    # queries:
    #   - query: <QUERY>
    #     metric: <METRIC_NAME>
    #     tags:
    #     - <METRIC_TAG_KEY>:<METRIC_TAG_VALUE>
    #     type: <METRIC_TYPE>
    #     field: <FIELD_NAME>

    ## Enable options to collect extra metrics from your MySQL integration.
    #
    # options:

        ## @param replication - boolean - optional - default: false
        ## Set to `true` to collect replication metrics or group replication metrics.
        ## These metrics are only collected from the specified `host`. If you want to collect replication metrics
        ## from the source or primary node and the replica or secondary nodes you need to set separate check
        ## instances to collect them.
        #
        # replication: false

        ## @param replication_channel - string - optional
        ## If using multiple sources, set the channel name to monitor.
        #
        # replication_channel: <REPLICATION_CHANNEL>

        ## @param replication_non_blocking_status - boolean - optional - default: false
        ## Set to `true` to grab slave count in a non-blocking manner (requires `performance_schema`);
        #
        # replication_non_blocking_status: false

        ## @param galera_cluster - boolean - optional - default: false
        ## Set to `true` to collect Galera cluster metrics.
        #
        # galera_cluster: false

        ## @param extra_status_metrics - boolean - optional - default: false
        ## Set to `true` to enable extra status metrics.
        ##
        ## See also the MySQL metrics listing: https://docs.datadoghq.com/integrations/mysql/#metrics
        #
        # extra_status_metrics: false

        ## @param extra_innodb_metrics - boolean - optional - default: false
        ## Set to `true` to enable extra InnoDB metrics.
        ##
        ## See also the MySQL metrics listing: https://docs.datadoghq.com/integrations/mysql/#metrics
        #
        # extra_innodb_metrics: false

        ## @param disable_innodb_metrics - boolean - optional - default: false
        ## Set to `true` only if experiencing issues with older (unsupported) versions of MySQL
        ## that do not run or have InnoDB engine support.
        ##
        ## If this flag is enabled, you will only receive a small subset of metrics.
        ##
        ## see also the MySQL metrics listing: https://docs.datadoghq.com/integrations/mysql/#metrics
        #
        # disable_innodb_metrics: false

        ## @param schema_size_metrics - boolean - optional - default: false
        ## Set to `true` to collect schema size metrics.
        ##
        ## Note that this runs a heavy query against your database to compute the relevant metrics
        ## for all your existing schemas. Due to the nature of these calls, if you have a
        ## high number of tables and schemas, this may have a negative impact on your database performance.
        ## Only tables and schemas for which the user has been granted SELECT privileges are collected. 
        ##
        ## See also the MySQL metrics listing: https://docs.datadoghq.com/integrations/mysql/#metrics
        #
        # schema_size_metrics: false

        ## @param table_size_metrics - boolean - optional - default: false
        ## Set to `true` to collect data table size metrics.
        ## Note that only tables and schemas for which the user has been granted SELECT privileges are collected.
        ## This runs a heavy query against your database to compute the relevant metrics
        ## for all your existing tables. Due to the nature of these calls, if you have a
        ## high number of tables and schemas, this may have a negative impact on your database performance.
        ## See also the MySQL metrics listing: https://docs.datadoghq.com/integrations/mysql/#metrics
        #
        # table_size_metrics: false

        ## @param system_table_size_metrics - boolean - optional - default: false
        ## Set to `true` to collect system table size metrics from performance_schema, information_schema and mysql.
        ## Note that only tables and schemas for which the user has been granted SELECT privileges are collected.
        ## This runs a heavy query against your database to compute the relevant metrics
        ## for all your existing tables. Due to the nature of these calls, if you have a
        ## high number of tables and schemas, this may have a negative impact on your database performance.
        ## See also the MySQL metrics listing: https://docs.datadoghq.com/integrations/mysql/#metrics
        #
        # system_table_size_metrics: false

        ## @param table_rows_stats_metrics - boolean - optional - default: false
        ## These metrics are reported if you have a Percona MySQL instance with `userstat` enabled
        ## Set to `true` to collect table stats metrics.
        ## Metrics provided by the options:
        ##   - mysql.info.table.rows.read (total per table)
        ##   - mysql.info.table.rows.changed (total per table)
        ## Note that this runs a heavy query against your database to compute the relevant metrics
        ## for all your existing tables. Due to the nature of these calls, if you have a
        ## high number of tables and schemas, this may have a negative impact on your database performance.
        ## See also the MySQL metrics listing: https://docs.datadoghq.com/integrations/mysql/#metrics
        #
        # table_rows_stats_metrics: false

        ## @param extra_performance_metrics - boolean - optional - default: false
        ## These metrics are reported if `performance_schema` is enabled in the MySQL instance
        ## and if the version for that instance is >= 5.6.0.
        ##
        ## Note that this runs a heavy query against your database to compute the relevant metrics
        ## for all your existing schemas. Due to the nature of these calls, if you have a
        ## high number of tables and schemas, this may have a negative impact on your database performance.
        ##
        ## Metrics provided by the options:
        ##   - mysql.info.schema.size (per schema)
        ##   - mysql.performance.query_run_time.avg (per schema)
        ##   - mysql.performance.digest_95th_percentile.avg_us
        ##
        ## Note that some of these require the `user` defined for this instance
        ## to have PROCESS and SELECT privileges. Take a look at the
        ## MySQL integration tile in the Datadog Web UI for further instructions.
        #
        # extra_performance_metrics: false

    ## @param dbm - boolean - optional - default: false
    ## Set to `true` enable Database Monitoring.
    #
    # dbm: false

    ## Configure collection of query metrics
    #
    # query_metrics:

        ## @param enabled - boolean - optional - default: true
        ## Enable collection of query metrics. Requires `dbm: true`.
        #
        # enabled: true

        ## @param collection_interval - number - optional - default: 10
        ## Set the query metric collection interval (in seconds). Each collection involves a single query to
        ## `pg_stat_statements`. If a non-default value is chosen then that exact same value must be used for *every*
        ## check instance. Running different instances with different collection intervals is not supported.
        #
        # collection_interval: 10

    ## Configure collection of query samples
    #
    # query_samples:

        ## @param enabled - boolean - optional - default: true
        ## Enables collection of query samples. Requires `dbm: true`.
        #
        # enabled: true

        ## @param collection_interval - number - optional - default: 1
        ## Sets the query sample collection interval (in seconds). Each collection involves a single query to one
        ## of the `performance_schema.events_statements_*` tables, followed by at most one `EXPLAIN` query per
        ## unique statement seen.
        #
        # collection_interval: 1

        ## @param explained_queries_per_hour_per_query - integer - optional - default: 60
        ## Sets the rate limit for how many execution plans will be collected per hour per normalized query.
        #
        # explained_queries_per_hour_per_query: 60

        ## @param samples_per_hour_per_query - integer - optional - default: 15
        ## Sets the rate limit for how many statement sample events will be ingested per hour per normalized
        ## execution plan.
        #
        # samples_per_hour_per_query: 15

        ## @param explained_queries_cache_maxsize - integer - optional - default: 5000
        ## Sets the max size of the cache used for the explained_queries_per_hour_per_query rate limit. This
        ## should be increased for databases with a very large number of unique normalized queries which exceed the
        ## cache's limit.
        #
        # explained_queries_cache_maxsize: 5000

        ## @param seen_samples_cache_maxsize - integer - optional - default: 10000
        ## Sets the max size of the cache used for the samples_per_hour_per_query rate limit. This should be
        ## increased for databases with a very large number of unique normalized execution plans which exceed the
        ## cache's limit.
        #
        # seen_samples_cache_maxsize: 10000

        ## @param events_statements_row_limit - integer - optional - default: 5000
        ## Sets the maximum number of rows to read out from a `performance_schema.events_statements_*` table during
        ## a single collection.
        #
        # events_statements_row_limit: 5000

        ## @param explain_procedure - string - optional - default: explain_statement
        ## Overrides the default procedure used for collecting execution plans. The agent will use this
        ## procedure in each schema where it exists: `{schema}.explain_statement({statement})`.
        #
        # explain_procedure: explain_statement

        ## @param fully_qualified_explain_procedure - string - optional - default: datadog.explain_statement
        ## Overrides the default fully qualified explain procedure used for collecting execution plans for
        ## statements sent from connections that do not have a default schema configured.
        #
        # fully_qualified_explain_procedure: datadog.explain_statement

        ## @param events_statements_enable_procedure - string - optional - default: datadog.enable_events_statements_consumers
        ## Overrides the default procedure used for enabling events statements consumers.
        #
        # events_statements_enable_procedure: datadog.enable_events_statements_consumers

        ## @param events_statements_temp_table_name - string - optional - default: datadog.temp_events
        ## Overrides the default fully qualified name for the temp table the agent creates while collecting
        ## samples.
        #
        # events_statements_temp_table_name: datadog.temp_events

        ## @param collection_strategy_cache_maxsize - integer - optional - default: 1000
        ## Sets the max size of the cache used for caching collection strategies. This value should be increased
        ## to be at least as many as the number of unique schemas that are being monitored.
        #
        # collection_strategy_cache_maxsize: 1000

        ## @param collection_strategy_cache_ttl - integer - optional - default: 300
        ## Sets how long to cache collection strategies. This should only be decreased if the set of enabled
        ## `events_statements_*` tables changes frequently enough to cause stale strategies to return empty
        ## results for an extended period of time.
        #
        # collection_strategy_cache_ttl: 300

    ## Configure collection of active sessions monitoring
    #
    # query_activity:

        ## @param enabled - boolean - optional - default: true
        ## Enable collection of active sessions. Requires `dbm: true`.
        #
        # enabled: true

        ## @param collection_interval - number - optional - default: 10
        ## Set the activity collection interval (in seconds).
        #
        # collection_interval: 10

    ## This block defines the configuration for AWS RDS and Aurora instances. 
    ##
    ## Complete this section if you have installed the Datadog AWS Integration 
    ## (https://docs.datadoghq.com/integrations/amazon_web_services) to enrich instances 
    ## with MySQL integration telemetry.
    ##
    ## These values are only applied when `dbm: true` option is set.
    #
    # aws:

        ## @param instance_endpoint - string - optional - default: mydb.cfxgae8cilcf.us-east-1.rds.amazonaws.com
        ## Equal to the Endpoint.Address of the instance the agent is connecting to. 
        ## This value is optional if the value of `host` is already configured to the instance endpoint.
        ##
        ## For more information on instance endpoints, 
        ## see the AWS docs https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_Endpoint.html
        #
        # instance_endpoint: mydb.cfxgae8cilcf.us-east-1.rds.amazonaws.com

    ## This block defines the configuration for Google Cloud SQL instances.
    ##
    ## Complete this section if you have installed the Datadog GCP Integration 
    ## (https://docs.datadoghq.com/integrations/google_cloud_platform) to enrich instances 
    ## with MySQL integration telemetry.
    ##
    ## These values are only applied when `dbm: true` option is set.
    #
    # gcp:

        ## @param project_id - string - optional - default: foo-project
        ## Equal to the GCP resource's project ID.
        ##
        ## For more information on project IDs,
        ## See the GCP docs https://cloud.google.com/resource-manager/docs/creating-managing-projects
        #
        # project_id: foo-project

        ## @param instance_id - string - optional - default: foo-database
        ## Equal to the GCP resource's instance ID.
        ##
        ## For more information on instance IDs,
        ## See the GCP docs https://cloud.google.com/sql/docs/mysql/instance-settings#instance-id-2ndgen
        #
        # instance_id: foo-database

    ## This block defines the configuration for Azure Database for MySQL.
    ##
    ## Complete this section if you have installed the Datadog Azure Integration 
    ## (https://docs.datadoghq.com/integrations/azure) to enrich instances 
    ## with MySQL integration telemetry.
    ##
    ## These values are only applied when `dbm: true` option is set.
    #
    # azure:

        ## @param deployment_type - string - optional - default: flexible_server
        ## Equal to the deployment type for the managed database. 
        ##
        ## Acceptable values are:
        ##   - `flexible_server` 
        ##   - `single_server`
        ##   - `virtual_machine`
        ##
        ## For more information on deployment types, 
        ## see the Azure docs https://docs.microsoft.com/en-us/azure/mysql/select-right-deployment-type
        #
        # deployment_type: flexible_server

        ## @param name - string - optional - default: mysql-database
        ## Equal to the name of the Azure MySQL database instance.
        #
        # name: mysql-database

    ## Configure how the SQL obfuscator behaves.
    ## Note: This option only applies when `dbm` is enabled.
    #
    # obfuscator_options:

        ## @param replace_digits - boolean - optional - default: false
        ## Set to `true` to replace digits in identifiers and table names with question marks in your SQL statements.
        ## Note: This option also applies to extracted tables using `collect_tables`.
        #
        # replace_digits: false

        ## @param collect_metadata - boolean - optional - default: true
        ## Set to `false` to disable the collection of metadata in your SQL statements.
        ## Metadata includes things such as tables, commands, and comments.
        #
        # collect_metadata: true

        ## @param collect_tables - boolean - optional - default: true
        ## Set to `false` to disable the collection of tables in your SQL statements.
        ## Requires `collect_metadata: true`.
        #
        # collect_tables: true

        ## @param collect_commands - boolean - optional - default: true
        ## Set to `false` to disable the collection of commands in your SQL statements.
        ## Requires `collect_metadata: true`.
        ##
        ## Examples: SELECT, UPDATE, DELETE, etc.
        #
        # collect_commands: true

        ## @param collect_comments - boolean - optional - default: true
        ## Set to `false` to disable the collection of comments in your SQL statements.
        ## Requires `collect_metadata: true`.
        #
        # collect_comments: true

        ## @param keep_sql_alias - boolean - optional - default: true
        ## Set to `true` to keep sql aliases in obfuscated SQL statements. Examples of aliases are
        ## `with select 1 as alias`, `select column as other_name`, or `select * from table t`.
        ## When `true` these aliases will not be removed.
        #
        # keep_sql_alias: true

    ## @param tags - list of strings - optional
    ## A list of tags to attach to every metric and service check emitted by this instance.
    ##
    ## Learn more about tagging at https://docs.datadoghq.com/tagging
    #
    # tags:
    #   - <KEY_1>:<VALUE_1>
    #   - <KEY_2>:<VALUE_2>

    ## @param service - string - optional
    ## Attach the tag `service:<SERVICE>` to every metric, event, and service check emitted by this integration.
    ##
    ## Overrides any `service` defined in the `init_config` section.
    #
    # service: <SERVICE>

    ## @param min_collection_interval - number - optional - default: 15
    ## This changes the collection interval of the check. For more information, see:
    ## https://docs.datadoghq.com/developers/write_agent_check/#collection-interval
    #
    # min_collection_interval: 15

    ## @param empty_default_hostname - boolean - optional - default: false
    ## This forces the check to send metrics with no hostname.
    ##
    ## This is useful for cluster-level checks.
    #
    # empty_default_hostname: false

    ## @param disable_generic_tags - boolean - optional - default: false
    ## Replaces generic tag such as `server` with `mysql_server` to avoid getting it mixed with
    ## other integrations tags.
    #
    disable_generic_tags: true

    ## @param metric_patterns - mapping - optional
    ## A mapping of metrics to include or exclude, with each entry being a regular expression.
    ##
    ## Metrics defined in `exclude` will take precedence in case of overlap.
    #
    # metric_patterns:
    #   include:
    #   - <INCLUDE_REGEX>
    #   exclude:
    #   - <EXCLUDE_REGEX>

## Log Section
##
## type - required - Type of log input source (tcp / udp / file / windows_event).
## port / path / channel_path - required - Set port if type is tcp or udp.
##                                         Set path if type is file.
##                                         Set channel_path if type is windows_event.
## source  - required - Attribute that defines which integration sent the logs.
## encoding - optional - For file specifies the file encoding. Default is utf-8. Other
##                       possible values are utf-16-le and utf-16-be.
## service - optional - The name of the service that generates the log.
##                      Overrides any `service` defined in the `init_config` section.
## tags - optional - Add tags to the collected logs.
##
## Discover Datadog log collection: https://docs.datadoghq.com/logs/log_collection/
#
# logs:
#   - type: file
#     path: <GENERAL_LOG_FILE_PATH>
#     source: mysql
#     log_processing_rules:
#     - type: multi_line
#       name: new_log_start_with_date
#       pattern: \d{4}\-(0?[1-9]|1[012])\-(0?[1-9]|[12][0-9]|3[01])
#     - type: multi_line
#       name: new_logs_do_not_always_start_with_timestamp
#       pattern: \t\t\s*\d+\s+|\d{6}\s+\d{,2}:\d{2}:\d{2}\t\s*\d+\s+
#   - type: file
#     path: <ERROR_LOG_FILE_PATH>
#     source: mysql
#   - type: file
#     path: <SLOW_QUERY_LOG_FILE_PATH>
#     source: mysql
#     log_processing_rules:
#     - type: multi_line
#       name: new_slow_query_log_entry
#       pattern: '# Time:'
#     - type: multi_line
#       name: mysqld_log_short_format_new_slow_query_log_entry
#       pattern: '# Query_time:'
