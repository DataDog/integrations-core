init_config:
    ## @param global_custom_queries - list - optional
    ## See `custom_queries` defined below.
    ##
    ## Global custom queries can be applied to all instances using the
    ## `use_global_custom_queries` setting at the instance level.
    #
    # global_custom_queries:
    #   - metric_prefix: clickhouse
    #     query: <QUERY>
    #     columns: <COLUMNS>
    #     tags: <TAGS>

instances:

    ## @param server - string - required
    ## MySQL server to connect to.
    ## NOTE: Even if the server name is "localhost", the agent connects to MySQL using TCP/IP, unless you also
    ## provide a value for the sock key (below).
    #
  - server: localhost

    ## @param user - string - required
    ## Datadog Username created to connect to MySQL.
    #
    user: datadog

    ## @param pass - string - required
    ## Password associated with the datadog user.
    #
    pass: <UNIQUEPASSWORD>

    ## @param port - integer - optional - default: 3306
    ## Port to use when connecting to MySQL.
    #
    # port: 3306

    ## @param sock - string - optional
    ## Set the sock parameter if you want to connect to MySQL using a Unix Socket.
    #
    # sock: <SOCK_PATH>

    ## @param defaults_file - string - optional
    ## Enter the path of an alternate configuration mechanism file.
    #
    # defaults_file: <CONFIGURATION_FILE_PATH>

    ## @param connect_timeout - integer - optional - default: 10
    ## Change the timeout time in second for the Agent queries.
    #
    # connect_timeout: 10

    ## @param tags - list of key:value elements - optional
    ## List of tags to attach to every metric, event and service check emitted by this integration.
    ##
    ## Learn more about tagging: https://docs.datadoghq.com/tagging/
    #
    # tags:
    #   - <KEY_1>:<VALUE_1>
    #   - <KEY_2>:<VALUE_2>

    ## @param ssl - object - optional
    ## Use this parameter to configure a SSL connection between the Agent and MySQL:
    ## `key`: Path to your key file.
    ## `cert`: path to your cert file.
    ## `ca`: path to your ca file.
    #
    # ssl:
    #   key: <KEY_FILE_PATH>
    #   cert: <CERT_FILE_PATH>
    #   ca: <CA_PATH_FILE>

    ## @param use_global_custom_queries - string - optional - default: true
    ## How `global_custom_queries` should be used for this instance. There are 3 options:
    ##
    ## 1. true - `global_custom_queries` override `custom_queries`.
    ## 2. false - `custom_queries` override `global_custom_queries`.
    ## 2. extend - `global_custom_queries` are used in addition to any `custom_queries`.
    #
    # use_global_custom_queries: true

    ## @param custom_queries - list - optional
    ## Each query must have 2 fields:
    ##
    ## 1. query - The SQL to execute. It can be a simple statement or a multi-line script.
    ## 2. columns - The list representing each column, ordered sequentially from left to right.
    ##              The number of columns must equal the number of columns returned in the query.
    ##              There are 2 required pieces of data:
    ##                a. name - The suffix to append to `clickhouse.` to form
    ##                          the full metric name. If `type` is `tag`, this column is
    ##                          considered a tag and applied to every
    ##                          metric collected by this particular query.
    ##                b. type - The submission method (gauge, monotonic_count, etc.).
    ##                          This can also be set to `tag` to tag each metric in the row
    ##                          with the name and value of the item in this column. You can
    ##                          use the `count` type to perform aggregation for queries that
    ##                          return multiple rows with the same or no tags.
    ## 3. tags (optional) - A list of tags to apply to each metric.
    #
    # custom_queries:
    #   - query: |  # Use the pipe if you require a multi-line script.
    #       SELECT foo,
    #              COUNT(*)
    #       FROM table.events
    #       GROUP BY foo
    #     columns:
    #       # Columns without a name are ignored, use this for any column you wish to skip:
    #       # - {}
    #       - name: foo
    #         type: tag
    #       - name: event.total
    #         type: gauge
    #     tags:
    #       - test:mysql

    ## @param max_custom_queries - integer - optional - default: 20
    ## Set the maximum number of custom queries to execute with this integration.
    ##
    ## WARNING: This only applies to the deprecated `queries` option.
    #
    # max_custom_queries: 20

    ## @param queries - object - optional
    ## Define custom queries to collect custom metrics on your MySQL
    ## See https://docs.datadoghq.com/integrations/faq/how-to-collect-metrics-from-custom-mysql-queries to learn more.
    ##
    ## WARNING: This option is deprecated and will be removed in a future release. Use the `custom_queries` option.
    #
    # queries:
    #   - query: <QUERY>
    #     metric: <METRIC_NAME>
    #     tags:
    #       - <METRIC_TAG_KEY>:<METRIC_TAG_VALUE>
    #     type: <METRIC_TYPE>
    #     field: <FIELD_NAME>

    ## @param options - object - optional
    ## Enable options to collect extra metrics from your MySQL integration.
    #
    # options:

      ## @param replication - boolean - optional - default: false
      ## Set replication to true to collect replications metrics.
      #
      # replication: false

      ## @param replication_channel - string - optional
      ## If using multiple sources, set the channel name to monitor.
      #
      # replication_channel: <CHANNEL_NAME>

      ## @param replication_non_blocking_status - boolean - optional - default: false
      ## Set to true to grab slave count in non-blocking manner (req. performance_schema)
      #
      # replication_non_blocking_status: false

      ## @param galera_cluster - boolean - optional - default: false
      ## Set galera_cluster to true in order to collect Galera cluster metrics.
      #
      # galera_cluster: false

      ## @param extra_status_metrics - boolean - optional - default: true
      ## Set extra_status_metrics to false to disable extra status metrics.
      ## See the Datadog-MySQL metrics section to learn more:
      ## https://docs.datadoghq.com/integrations/mysql/#metrics
      #
      # extra_status_metrics: true

      ## @param extra_innodb_metrics - boolean - optional - default: true
      ## Set extra_innodb_metrics to false to disable extra innodb metrics.
      ## See the Datadog-MySQL metrics section to learn more:
      ## https://docs.datadoghq.com/integrations/mysql/#metrics
      #
      # extra_innodb_metrics: true

      ## @param schema_size_metrics - boolean - optional - default: false
      ## Set schema_size_metrics parameter to collect schema size metrics. Note that it runs a heavy query
      ## against your DB to compute the relevant metrics for all your existing schemas.
      ## Due to the nature of these calls, if you have a high number of tables and schemas,
      ## you may be subject to some negative impact in the performance of your DB.
      ## See the Datadog-MySQL metrics section to learn more:
      ## https://docs.datadoghq.com/integrations/mysql/#metrics
      #
      # schema_size_metrics: false

      ## @param disable_innodb_metrics - boolean - optional - default: false
      ## Set disable_innodb_metrics to true only if you are using older (unsupported) versions of
      ## MySQL who do not run/have innodb engine support and may experiment issue otherwise.
      ## Should this flag be enabled you will only receive a small subset of metrics.
      ## See the Datadog-MySQL metrics section to learn more:
      ## https://docs.datadoghq.com/integrations/mysql/#metrics
      #
      # disable_innodb_metrics: false

      ## @param extra_performance_metrics - boolean - optional - default: true
      ## extra_performance_metrics is reported if `performance_schema` is enabled
      ## in the MySQL instance and if the version for that instance is >= 5.6.0
      ##
      ## extra_performance_metrics runs a heavy query against your DB to compute the relevant metrics
      ## for all your existing schemas.
      ## Due to the nature of these calls, if you have a high number of tables and/or schemas,
      ## you may be subject to some negative impact in the performance of your DB.
      ## Please bear that in mind when enabling them.
      ## Metrics provided by the options:
      ##   - mysql.info.schema.size (per schame)
      ##   - mysql.performance.query_run_time.avg (per schema)
      ##   - mysql.performance.digest_95th_percentile.avg_us
      ##
      ## Note that some of these require the user defined for this instance
      ## to have PROCESS and SELECT privileges. Take a look at the
      ## MySQL integration tile in the Datadog WebUI for further instructions.
      #
      # extra_performance_metrics: true

## Log Section (Available for Agent >=6.0)
##
## type - mandatory - Type of log input source (tcp / udp / file / windows_event)
## port / path / channel_path - mandatory - Set port if type is tcp or udp. Set path if type is file. Set channel_path if type is windows_event
## service - mandatory - Name of the service that generated the log
## source  - mandatory - Attribute that defines which Integration sent the logs
## tags: - optional - Add tags to the collected logs
##
## Discover Datadog log collection: https://docs.datadoghq.com/logs/log_collection/
#
# logs:
#   - type: file
#     path: "<ERROR_LOG_FILE_PATH>"
#     source: mysql
#     service: "<SERVICE_NAME>"
#
#   - type: file
#     path: "<SLOW_QUERY_LOG_FILE_PATH>"
#     source: mysql
#     service: "<SERVICE_NAME>"
#     log_processing_rules:
#       - type: multi_line
#         name: new_slow_query_log_entry
#         pattern: "# Time:"
#         # If mysqld was started with `--log-short-format`, use:
#         # pattern: "# Query_time:"
#
#   - type: file
#     path: "<GENERAL_LOG_FILE_PATH>"
#     source: mysql
#     service: "<SERVICE_NAME>"
## For multiline logs, if they start by the date with the format yyyy-mm-dd uncomment the following processing rule
#     log_processing_rules:
#       - type: multi_line
#         name: new_log_start_with_date
#         pattern: \d{4}\-(0?[1-9]|1[012])\-(0?[1-9]|[12][0-9]|3[01])
